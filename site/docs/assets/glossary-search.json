[
  {
    "term": "agentic ai",
    "aliases": [
      "ai agents",
      "autonomous agent"
    ],
    "categories": [
      "Agents & Tooling"
    ],
    "roles": [
      "engineering",
      "product"
    ],
    "short_def": "Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.",
    "nist_rmf_tags": [
      "accountability",
      "risk_management"
    ],
    "status": "draft",
    "slug": "agentic-ai"
  },
  {
    "term": "ai assurance",
    "aliases": [
      "AI assurance program"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "product",
      "engineering",
      "policy",
      "legal",
      "security"
    ],
    "short_def": "Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.",
    "nist_rmf_tags": [
      "accountability",
      "risk_management",
      "transparency"
    ],
    "status": "draft",
    "slug": "ai-assurance"
  },
  {
    "term": "ai incident response",
    "aliases": [
      "model incident response",
      "ai escalation"
    ],
    "categories": [
      "Operations & Monitoring",
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "engineering",
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "draft",
    "slug": "ai-incident-response"
  },
  {
    "term": "algorithmic audit",
    "aliases": [
      "algorithm accountability audit"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "product",
      "engineering",
      "policy",
      "legal",
      "communications"
    ],
    "short_def": "Independent review of an AI system’s data, design, and outcomes to verify compliance, fairness, and risk controls.",
    "nist_rmf_tags": [
      "accountability",
      "fairness",
      "transparency"
    ],
    "status": "draft",
    "slug": "algorithmic-audit"
  },
  {
    "term": "algorithmic bias",
    "aliases": [
      "systemic bias",
      "ai bias"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "legal",
      "product",
      "communications",
      "security"
    ],
    "short_def": "Systematic unfairness in model outputs that disadvantages certain groups or outcomes.",
    "nist_rmf_tags": [
      "fairness",
      "transparency"
    ],
    "status": "draft",
    "slug": "algorithmic-bias"
  },
  {
    "term": "algorithmic impact assessment",
    "aliases": [
      "AIA",
      "AI impact assessment"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "legal",
      "product",
      "communications",
      "engineering"
    ],
    "short_def": "Structured review that documents how an AI system may affect people, processes, and compliance obligations.",
    "nist_rmf_tags": [
      "accountability",
      "risk_management",
      "transparency"
    ],
    "status": "draft",
    "slug": "algorithmic-impact-assessment"
  },
  {
    "term": "alignment",
    "aliases": [
      "AI alignment",
      "value alignment"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Making sure AI systems optimize for human values, policies, and intended outcomes.",
    "nist_rmf_tags": [
      "accountability",
      "transparency",
      "validity"
    ],
    "status": "draft",
    "slug": "alignment"
  },
  {
    "term": "attention",
    "aliases": [
      "attention mechanism",
      "self-attention"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Technique enabling models to weight input tokens differently when producing each output.",
    "nist_rmf_tags": [
      "transparency",
      "robustness"
    ],
    "status": "draft",
    "slug": "attention"
  },
  {
    "term": "beam search",
    "aliases": [
      "beam decoding",
      "multi-path decoding"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "draft",
    "slug": "beam-search"
  },
  {
    "term": "bias-variance tradeoff",
    "aliases": [
      "bias variance trade-off",
      "generalization tradeoff"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "policy"
    ],
    "short_def": "Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "draft",
    "slug": "bias-variance-tradeoff"
  },
  {
    "term": "chunking",
    "aliases": [
      "document chunking",
      "segmentation"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Splitting source documents into manageable pieces before indexing or feeding them to models.",
    "nist_rmf_tags": [
      "data_quality",
      "privacy"
    ],
    "status": "draft",
    "slug": "chunking"
  },
  {
    "term": "clip",
    "aliases": [
      "contrastive language-image pretraining",
      "clip model"
    ],
    "categories": [
      "Foundations",
      "Retrieval & RAG"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product",
      "communications"
    ],
    "short_def": "Multimodal model that embeds images and text into a shared space using contrastive learning.",
    "nist_rmf_tags": [
      "data_quality",
      "transparency"
    ],
    "status": "draft",
    "slug": "clip"
  },
  {
    "term": "confusion matrix",
    "aliases": [
      "contingency table",
      "error matrix"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product",
      "policy"
    ],
    "short_def": "Table that summarizes true/false positives and negatives to diagnose classification performance.",
    "nist_rmf_tags": [
      "transparency",
      "validity"
    ],
    "status": "draft",
    "slug": "confusion-matrix"
  },
  {
    "term": "constitutional ai",
    "aliases": [
      "principle-guided alignment",
      "self-critique alignment"
    ],
    "categories": [
      "Governance & Risk",
      "LLM Core"
    ],
    "roles": [
      "product",
      "engineering",
      "policy"
    ],
    "short_def": "Alignment approach where models critique and revise their own outputs against a written set of principles.",
    "nist_rmf_tags": [
      "accountability",
      "validity",
      "transparency"
    ],
    "status": "draft",
    "slug": "constitutional-ai"
  },
  {
    "term": "content moderation",
    "aliases": [
      "trust and safety",
      "policy enforcement"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "policy",
      "communications",
      "product",
      "security",
      "engineering"
    ],
    "short_def": "Workflows and tools that review, filter, and act on user-generated content to enforce policy.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "draft",
    "slug": "content-moderation"
  },
  {
    "term": "context window",
    "aliases": [
      "context length",
      "sequence length limit"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Maximum number of tokens a model can consider at once during prompting or inference.",
    "nist_rmf_tags": [
      "transparency",
      "privacy"
    ],
    "status": "draft",
    "slug": "context-window"
  },
  {
    "term": "cross-validation",
    "aliases": [
      "k-fold validation",
      "cv"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "draft",
    "slug": "cross-validation"
  },
  {
    "term": "data minimization",
    "aliases": [
      "data minimisation",
      "minimal data collection"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Principle of collecting and retaining only the data necessary for a defined purpose.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management"
    ],
    "status": "draft",
    "slug": "data-minimization"
  },
  {
    "term": "data retention",
    "aliases": [
      "retention policy",
      "data lifecycle"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "legal",
      "policy",
      "security",
      "product"
    ],
    "short_def": "Policies defining how long data is stored, where it lives, and how it is deleted.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management"
    ],
    "status": "draft",
    "slug": "data-retention"
  },
  {
    "term": "decoding",
    "aliases": [
      "text decoding",
      "generation decoding"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Algorithms that turn model probability distributions into output tokens during generation.",
    "nist_rmf_tags": [
      "transparency",
      "robustness"
    ],
    "status": "draft",
    "slug": "decoding"
  },
  {
    "term": "differential privacy",
    "aliases": [
      "DP",
      "epsilon-differential privacy"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "engineering",
      "legal",
      "policy",
      "security"
    ],
    "short_def": "Mathematical framework that limits how much any single record influences published data or model outputs.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management"
    ],
    "status": "draft",
    "slug": "differential-privacy"
  },
  {
    "term": "diffusion model",
    "aliases": [
      "denoising diffusion model",
      "score-based model"
    ],
    "categories": [
      "Foundations",
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product",
      "communications"
    ],
    "short_def": "Generative model that iteratively denoises random noise to synthesize images, audio, or other data.",
    "nist_rmf_tags": [
      "risk_management",
      "transparency"
    ],
    "status": "draft",
    "slug": "diffusion-model"
  },
  {
    "term": "embedding",
    "aliases": [
      "vector embedding",
      "representation vector"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Dense numerical representation that captures semantic meaning of text, images, or other data.",
    "nist_rmf_tags": [
      "data_quality",
      "accountability"
    ],
    "status": "draft",
    "slug": "embedding"
  },
  {
    "term": "evaluation",
    "aliases": [
      "model evaluation",
      "AI evaluation"
    ],
    "categories": [
      "Operations & Monitoring",
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "engineering",
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Systematic measurement of model performance, safety, and reliability using defined tests.",
    "nist_rmf_tags": [
      "validity",
      "accountability"
    ],
    "status": "draft",
    "slug": "evaluation"
  },
  {
    "term": "f1 score",
    "aliases": [
      "f-score",
      "harmonic mean of precision and recall"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Harmonic mean of precision and recall, balancing false positives and false negatives.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "draft",
    "slug": "f1-score"
  },
  {
    "term": "fairness metrics",
    "aliases": [
      "fairness measures",
      "fairness evaluation"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "policy",
      "legal",
      "engineering",
      "product"
    ],
    "short_def": "Quantitative measures that evaluate whether model performance is equitable across groups.",
    "nist_rmf_tags": [
      "fairness",
      "transparency"
    ],
    "status": "draft",
    "slug": "fairness-metrics"
  },
  {
    "term": "fine-tuning",
    "aliases": [
      "model adaptation",
      "supervised fine-tuning"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "data_science",
      "engineering"
    ],
    "short_def": "Additional training that adapts a pretrained model to a specific task or domain.",
    "nist_rmf_tags": [
      "accountability",
      "validity"
    ],
    "status": "draft",
    "slug": "fine-tuning"
  },
  {
    "term": "function calling",
    "aliases": [
      "tool calling",
      "structured output invocation"
    ],
    "categories": [
      "Agents & Tooling",
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "product",
      "data_science"
    ],
    "short_def": "LLM capability that lets prompts invoke predefined functions and return structured arguments.",
    "nist_rmf_tags": [
      "accountability",
      "transparency"
    ],
    "status": "draft",
    "slug": "function-calling"
  },
  {
    "term": "generative ai",
    "aliases": [
      "genai",
      "generative artificial intelligence"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "communications",
      "policy",
      "product"
    ],
    "short_def": "Family of models that produce new content—text, images, code—rather than only making predictions.",
    "nist_rmf_tags": [
      "transparency",
      "risk_management"
    ],
    "status": "draft",
    "slug": "generative-ai"
  },
  {
    "term": "greedy decoding",
    "aliases": [
      "argmax decoding"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Strategy that selects the highest-probability token at each step, producing deterministic outputs.",
    "nist_rmf_tags": [
      "validity",
      "accountability"
    ],
    "status": "draft",
    "slug": "greedy-decoding"
  },
  {
    "term": "guardrails",
    "aliases": [
      "safety guardrails",
      "policy guardrails"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Controls that constrain model behavior to comply with safety, legal, or brand requirements.",
    "nist_rmf_tags": [
      "accountability",
      "privacy",
      "robustness"
    ],
    "status": "draft",
    "slug": "guardrails"
  },
  {
    "term": "hallucination",
    "aliases": [
      "AI hallucination",
      "confabulation"
    ],
    "categories": [
      "LLM Core",
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "data_science",
      "engineering",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "When an AI model presents fabricated or unsupported information as fact.",
    "nist_rmf_tags": [
      "accuracy",
      "transparency",
      "validity"
    ],
    "status": "draft",
    "slug": "hallucination"
  },
  {
    "term": "knowledge distillation",
    "aliases": [
      "distillation",
      "teacher-student training"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "data_science",
      "engineering"
    ],
    "short_def": "Technique that trains a smaller student model to mimic a larger teacher model’s behavior.",
    "nist_rmf_tags": [
      "efficiency",
      "validity"
    ],
    "status": "draft",
    "slug": "knowledge-distillation"
  },
  {
    "term": "kv cache",
    "aliases": [
      "key-value cache",
      "attention cache"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Stored attention keys and values reused across decoding steps to speed sequential generation.",
    "nist_rmf_tags": [
      "efficiency",
      "privacy"
    ],
    "status": "draft",
    "slug": "kv-cache"
  },
  {
    "term": "log probability",
    "aliases": [
      "logprob",
      "token log probability"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Logarithm of a token’s probability, used to inspect model confidence and guide decoding tweaks.",
    "nist_rmf_tags": [
      "transparency",
      "validity"
    ],
    "status": "draft",
    "slug": "log-probability"
  },
  {
    "term": "low-rank adaptation",
    "aliases": [
      "LoRA",
      "low rank fine-tuning"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "data_science",
      "engineering"
    ],
    "short_def": "Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.",
    "nist_rmf_tags": [
      "efficiency",
      "accountability"
    ],
    "status": "draft",
    "slug": "low-rank-adaptation"
  },
  {
    "term": "ml observability",
    "aliases": [
      "model observability",
      "ai observability"
    ],
    "categories": [
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "policy",
      "security"
    ],
    "short_def": "Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.",
    "nist_rmf_tags": [
      "monitoring",
      "accountability"
    ],
    "status": "draft",
    "slug": "ml-observability"
  },
  {
    "term": "ml ops",
    "aliases": [
      "mlops",
      "machine learning operations"
    ],
    "categories": [
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "policy",
      "security"
    ],
    "short_def": "Operational discipline that manages ML models from experimentation through deployment and monitoring.",
    "nist_rmf_tags": [
      "accountability",
      "monitoring"
    ],
    "status": "draft",
    "slug": "ml-ops"
  },
  {
    "term": "model card",
    "aliases": [
      "model documentation",
      "model datasheet"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "policy",
      "legal",
      "product",
      "engineering",
      "communications"
    ],
    "short_def": "Standardized documentation describing a model’s purpose, data, performance, and limitations.",
    "nist_rmf_tags": [
      "documentation",
      "accountability"
    ],
    "status": "draft",
    "slug": "model-card"
  },
  {
    "term": "model drift",
    "aliases": [
      "distribution drift",
      "concept drift"
    ],
    "categories": [
      "Operations & Monitoring",
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "engineering",
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Gradual mismatch between model assumptions and real-world data that degrades performance over time.",
    "nist_rmf_tags": [
      "monitoring",
      "validity"
    ],
    "status": "draft",
    "slug": "model-drift"
  },
  {
    "term": "model governance",
    "aliases": [
      "ai governance",
      "ml governance"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.",
    "nist_rmf_tags": [
      "accountability",
      "risk_management"
    ],
    "status": "draft",
    "slug": "model-governance"
  },
  {
    "term": "model interpretability",
    "aliases": [
      "interpretability",
      "explainability"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "policy",
      "legal",
      "product"
    ],
    "short_def": "Ability to explain how a model arrives at its predictions in ways stakeholders understand.",
    "nist_rmf_tags": [
      "transparency",
      "accountability"
    ],
    "status": "draft",
    "slug": "model-interpretability"
  },
  {
    "term": "overfitting",
    "aliases": [
      "model overfitting",
      "overtraining"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "When a model memorizes training data patterns so closely that it performs poorly on new samples.",
    "nist_rmf_tags": [
      "validity",
      "data_quality"
    ],
    "status": "draft",
    "slug": "overfitting"
  },
  {
    "term": "precision",
    "aliases": [
      "positive predictive value",
      "ppv"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product",
      "policy"
    ],
    "short_def": "Share of predicted positives that are actually correct for a given classifier.",
    "nist_rmf_tags": [
      "validity",
      "accountability"
    ],
    "status": "draft",
    "slug": "precision"
  },
  {
    "term": "privacy impact assessment",
    "aliases": [
      "pia",
      "data protection impact assessment"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Structured review that evaluates how a system collects, uses, and safeguards personal data.",
    "nist_rmf_tags": [
      "privacy",
      "documentation"
    ],
    "status": "draft",
    "slug": "privacy-impact-assessment"
  },
  {
    "term": "privacy",
    "aliases": [
      "data privacy",
      "information privacy"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Principle of limiting data collection, use, and exposure to protect individuals’ information.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management"
    ],
    "status": "draft",
    "slug": "privacy"
  },
  {
    "term": "prompt engineering",
    "aliases": [
      "prompt design",
      "prompt scripting"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Crafting and testing prompts to steer model behavior toward desired outcomes.",
    "nist_rmf_tags": [
      "accountability",
      "transparency"
    ],
    "status": "draft",
    "slug": "prompt-engineering"
  },
  {
    "term": "quantization",
    "aliases": [
      "model quantization",
      "weight quantization"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "data_science",
      "engineering"
    ],
    "short_def": "Technique that compresses model weights into lower-precision formats to shrink size and speed inference.",
    "nist_rmf_tags": [
      "efficiency",
      "robustness",
      "documentation"
    ],
    "status": "draft",
    "slug": "quantization"
  },
  {
    "term": "recall",
    "aliases": [
      "sensitivity",
      "true positive rate"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product",
      "policy"
    ],
    "short_def": "Share of actual positives a model successfully identifies.",
    "nist_rmf_tags": [
      "validity",
      "accountability"
    ],
    "status": "draft",
    "slug": "recall"
  },
  {
    "term": "red teaming",
    "aliases": [
      "ai red teaming",
      "adversarial testing"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "communications",
      "engineering",
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "draft",
    "slug": "red-teaming"
  },
  {
    "term": "repetition penalty",
    "aliases": [
      "anti-repetition penalty",
      "token penalty"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "draft",
    "slug": "repetition-penalty"
  },
  {
    "term": "reranking",
    "aliases": [
      "re-ranking",
      "second-stage ranking"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Step that refines retrieval results using a more precise but slower scoring model.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "draft",
    "slug": "reranking"
  },
  {
    "term": "responsible ai",
    "aliases": [
      "trustworthy ai",
      "ethical ai"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "draft",
    "slug": "responsible-ai"
  },
  {
    "term": "retrieval-augmented generation",
    "aliases": [
      "RAG",
      "retrieval augmented generation"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Workflow that grounds a generative model with retrieved context before producing output.",
    "nist_rmf_tags": [
      "transparency",
      "data_quality",
      "documentation"
    ],
    "status": "draft",
    "slug": "retrieval-augmented-generation"
  },
  {
    "term": "retrieval",
    "aliases": [
      "information retrieval",
      "retriever"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Process of selecting relevant documents or vectors from a corpus in response to a query.",
    "nist_rmf_tags": [
      "data_quality",
      "transparency"
    ],
    "status": "draft",
    "slug": "retrieval"
  },
  {
    "term": "roc auc",
    "aliases": [
      "area under the roc curve",
      "roc area"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Metric summarizing binary classifier performance by measuring area under the ROC curve.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "draft",
    "slug": "roc-auc"
  },
  {
    "term": "safety evaluation",
    "aliases": [
      "safety testing",
      "safety assessment"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "policy",
      "product",
      "communications"
    ],
    "short_def": "Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "draft",
    "slug": "safety-evaluation"
  },
  {
    "term": "safety spec",
    "aliases": [
      "safety specification",
      "model safety policy"
    ],
    "categories": [
      "Governance & Risk",
      "LLM Core"
    ],
    "roles": [
      "product",
      "engineering",
      "policy",
      "security",
      "communications"
    ],
    "short_def": "Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability",
      "transparency"
    ],
    "status": "draft",
    "slug": "safety-spec"
  },
  {
    "term": "synthetic data evaluation",
    "aliases": [
      "synthetic data quality assessment",
      "synthetic validation"
    ],
    "categories": [
      "Operations & Monitoring",
      "Governance & Risk"
    ],
    "roles": [
      "data_science",
      "engineering",
      "policy",
      "product"
    ],
    "short_def": "Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.",
    "nist_rmf_tags": [
      "data_quality",
      "privacy"
    ],
    "status": "draft",
    "slug": "synthetic-data-evaluation"
  },
  {
    "term": "synthetic data",
    "aliases": [
      "generated data",
      "simulated data"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "data_science",
      "engineering",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.",
    "nist_rmf_tags": [
      "privacy",
      "data_quality"
    ],
    "status": "draft",
    "slug": "synthetic-data"
  },
  {
    "term": "system prompt",
    "aliases": [
      "system instruction",
      "base prompt"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.",
    "nist_rmf_tags": [
      "transparency",
      "accountability"
    ],
    "status": "draft",
    "slug": "system-prompt"
  },
  {
    "term": "temperature",
    "aliases": [
      "sampling temperature",
      "softmax temperature"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Decoding parameter that controls how random or deterministic a model’s outputs are.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "draft",
    "slug": "temperature"
  },
  {
    "term": "token",
    "aliases": [
      "subword token",
      "tokenized unit"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Smallest unit of text a model processes after tokenization, such as a word fragment or character.",
    "nist_rmf_tags": [
      "transparency",
      "validity"
    ],
    "status": "draft",
    "slug": "token"
  },
  {
    "term": "tool use",
    "aliases": [
      "function calling",
      "model tool invocation"
    ],
    "categories": [
      "Agents & Tooling"
    ],
    "roles": [
      "engineering",
      "product"
    ],
    "short_def": "Pattern where a model selects external tools or functions to handle parts of a task.",
    "nist_rmf_tags": [
      "accountability",
      "transparency"
    ],
    "status": "draft",
    "slug": "tool-use"
  },
  {
    "term": "top-k sampling",
    "aliases": [
      "k-sampling",
      "truncated sampling"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Decoding method that samples from the k most probable next tokens to balance diversity and control.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "draft",
    "slug": "top-k-sampling"
  },
  {
    "term": "top-p sampling",
    "aliases": [
      "nucleus sampling",
      "p-sampling"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "draft",
    "slug": "top-p-sampling"
  },
  {
    "term": "vector store",
    "aliases": [
      "vector database",
      "embedding index"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Database optimized to store embeddings and execute similarity search over vectors.",
    "nist_rmf_tags": [
      "accountability",
      "data_quality"
    ],
    "status": "draft",
    "slug": "vector-store"
  },
  {
    "term": "voice cloning",
    "aliases": [
      "voice synthesis",
      "speech cloning"
    ],
    "categories": [
      "Foundations",
      "Governance & Risk"
    ],
    "roles": [
      "product",
      "communications",
      "legal",
      "policy",
      "security"
    ],
    "short_def": "Technique that replicates a person’s voice using generative models trained on audio samples.",
    "nist_rmf_tags": [
      "risk_management",
      "transparency"
    ],
    "status": "draft",
    "slug": "voice-cloning"
  }
]