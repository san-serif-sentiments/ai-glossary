<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# model drift

**Aliases:** distribution drift, concept drift
**Categories:** Operations & Monitoring, Governance & Risk
**Roles:** Communications & Enablement, Engineering & Platform, Legal & Compliance, Policy & Risk, Product & Program Managers, Security & Trust
**Part of speech:** `concept`
**Status:** <span class="status-chip status-draft">Draft</span> (Last reviewed: 2024-11-01)

## Role takeaways
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Gradual mismatch between model assumptions and real-world data that degrades performance over time.

## Long definition
Model drift arises when the data encountered in production no longer matches the distribution seen during training or evaluation. Changes in user behavior, regulations, adversarial inputs, or upstream systems can all erode accuracy and trustworthiness. Drift appears in multiple forms: data drift alters input characteristics, concept drift changes the meaning of target labels, and prior drift moves latent relationships within embeddings. Operations teams monitor dashboards for early signals using statistical tests, canary evaluations, and feedback loops. Product managers plan remediation playbooks that include prompt updates, retrieval refreshes, or new fine-tuning cycles. Governance frameworks require drift detection as part of continuous monitoring obligations, ensuring sensitive use cases receive timely reviews. Documenting drift incidents, response timelines, and residual risk assessments supports compliance and informs future model lifecycle decisions.

## Audience perspectives
- **Exec:** Model drift is the slow decay that happens when the world changes but the AI stays static.
- **Engineer:** Shift between training and production distributions; measured via statistical monitoring and addressed with retraining or prompt updates.

## Examples
**Do**
- Set drift thresholds and automated alerts tied to evaluation reruns for critical workflows.

**Don't**
- Ignore user feedback that signals degraded relevance or bias until after incidents occur.

## Governance
- **NIST RMF tags:** monitoring, validity
- **Risk notes:** Undetected drift can lead to policy violations, misaligned outputs, and regulatory non-compliance.

## Relationships
- **Broader:** model governance
- **Related:** evaluation, guardrails, observability

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'model drift'.

## Citations
- [NIST AI RMF Glossary](https://airc.nist.gov/glossary/)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/model-drift.yml`_
