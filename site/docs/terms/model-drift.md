<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# model drift

**Aliases:** distribution drift, concept drift
**Categories:** Operations & Monitoring, Governance & Risk
**Roles:** Communications & Enablement, Engineering & Platform, Legal & Compliance, Policy & Risk, Product & Program Managers, Security & Trust
**Part of speech:** `concept`
**Status:** `draft` (Last reviewed: 2024-11-01)

## Short definition
Gradual mismatch between model assumptions and real-world data that degrades performance over time.

## Long definition
Model drift arises when the data encountered in production no longer matches the distribution seen during training or evaluation. Changes in user behavior, regulations, adversarial inputs, or upstream systems can all erode accuracy and trustworthiness. Drift appears in multiple forms: data drift alters input characteristics, concept drift changes the meaning of target labels, and prior drift moves latent relationships within embeddings. Operations teams monitor dashboards for early signals using statistical tests, canary evaluations, and feedback loops. Product managers plan remediation playbooks that include prompt updates, retrieval refreshes, or new fine-tuning cycles. Governance frameworks require drift detection as part of continuous monitoring obligations, ensuring sensitive use cases receive timely reviews. Documenting drift incidents, response timelines, and residual risk assessments supports compliance and informs future model lifecycle decisions.

## Audience perspectives
- **Exec:** Model drift is the slow decay that happens when the world changes but the AI stays static.
- **Engineer:** Shift between training and production distributions; measured via statistical monitoring and addressed with retraining or prompt updates.

## Examples
**Do**
- Set drift thresholds and automated alerts tied to evaluation reruns for critical workflows.

**Don't**
- Ignore user feedback that signals degraded relevance or bias until after incidents occur.

## Governance
- **NIST RMF tags:** monitoring, validity
- **Risk notes:** Undetected drift can lead to policy violations, misaligned outputs, and regulatory non-compliance.

## Relationships
- **Broader:** model governance
- **Related:** evaluation, guardrails, observability

## Citations
- [NIST AI RMF Glossary](https://airc.nist.gov/glossary/)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/model-drift.yml`_
