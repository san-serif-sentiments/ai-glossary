<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# ai assurance

**Aliases:** AI assurance program
**Categories:** Governance & Risk
**Roles:** Product & Program Managers, Engineering & Platform, Policy & Risk, Legal & Compliance, Security & Trust
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.

## Long definition
AI assurance is the end-to-end discipline that gives stakeholders confidence an AI system behaves as intended across its lifecycle. Teams collect structured evidence—model cards, test results, evaluation dashboards, bias and privacy assessments—and compare them against assurance criteria derived from policy, regulation, and business risk appetite. Assurance programs span pre-launch discovery, red teaming, and documentation, followed by post-launch monitoring and re-certification when data, models, or prompts change. Product and engineering leads own the technical mitigations, while policy, legal, and security partners review controls, disclosure obligations, and escalation paths. Mature organisations use playbooks, independent reviewers, and audit trails so they can demonstrate compliance with frameworks such as the NIST AI RMF or government procurement requirements. Without an assurance function, leaders lack defensible evidence for regulators and customers, and harmful behaviour may slip into production unchecked.

## Audience perspectives
- **Exec:** Confirms our AI launches have evidence, owners, and guardrails that withstand regulatory and customer scrutiny.
- **Engineer:** Defines the artefacts, tests, and controls you must deliver before handover and keep current after updates.

## Examples
**Do**
- Assemble an assurance dossier that bundles eval results, model cards, and mitigation plans before launch approval.
- Schedule periodic re-certification when training data, prompts, or external regulations change.

**Don't**
- Rely on a single accuracy metric as proof of readiness without documenting safety or fairness tests.
- Ship critical updates without notifying assurance reviewers or updating evidence trails.

## Governance
- **NIST RMF tags:** accountability, risk_management, transparency
- **Risk notes:** Missing assurance evidence makes it impossible to prove compliance, increasing regulatory, contractual, and safety exposure.

## Relationships
- **Broader:** responsible ai
- **Related:** model governance, algorithmic audit, evaluation, ai incident response

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'ai assurance'.

## Citations
- [CDEI AI Assurance Guidance](https://www.gov.uk/government/collections/cdei-assurance-guidance)
- [OECD AI Glossary](https://oecd.ai/en/glossary)
- [Partnership on AI Glossary](https://partnershiponai.org/glossary/)

_License: CC BY-SA 4.0_

_Source file: `data/terms/ai-assurance.yml`_
