<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# algorithmic audit

**Aliases:** algorithm accountability audit
**Categories:** Governance & Risk
**Roles:** Product & Program Managers, Engineering & Platform, Policy & Risk, Legal & Compliance, Communications & Enablement
**Part of speech:** `concept`
**Status:** <span class="status-chip status-reviewed">Reviewed</span> (Last reviewed: 2025-09-28)

## Role takeaways
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Independent review of an AI system’s data, design, and outcomes to verify compliance, fairness, and risk controls.

## Long definition
An algorithmic audit is a structured examination of an AI system carried out by internal or external reviewers to assess whether it behaves responsibly and complies with legal, ethical, and contractual obligations. Auditors inspect training data provenance, modeling choices, evaluation methods, documentation, and deployment safeguards. They recreate metrics for accuracy, fairness, robustness, privacy, and security, and look for gaps between intended and observed behaviour. Audits often include stakeholder interviews, policy mapping, bias testing, and reproducibility checks, culminating in findings and remediation plans with accountable owners. Organisations commission audits pre-launch or after material changes, and regulators increasingly require independent assurance for high-risk AI services. Successful audits depend on traceable artefacts—model cards, change logs, governance decisions—and cross-functional collaboration across engineering, product, policy, and communications teams. Skipping or under-scoping audits makes it difficult to prove compliance, potentially exposing users to harm and the organisation to legal or reputational damage.

## Audience perspectives
- **Exec:** Demonstrates to regulators, customers, and partners that our AI undergoes independent scrutiny with corrective action plans.
- **Engineer:** Clarifies the evidence, metrics, and documentation auditors need to reproduce and validate model behaviour.

## Examples
**Do**
- Engage an independent reviewer to replicate fairness, privacy, and robustness tests before public launch.
- Track remediation actions from audit findings through to closure with documented owners and deadlines.

**Don't**
- Limit audits to marketing claims without sharing code, datasets, or evaluation pipelines.
- Ignore communications planning—audits should prepare messaging for customers and regulators.

## Governance
- **NIST RMF tags:** accountability, fairness, transparency
- **Risk notes:** Inadequate audits hide systemic issues, undermining compliance readiness and eroding stakeholder trust.

## Relationships
- **Broader:** ai assurance
- **Related:** model governance, fairness metrics, evaluation, red teaming

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'algorithmic audit'.

## Citations
- [AI Now Institute Lexicon](https://ainowinstitute.org/lexicon.html)
- [OECD AI Glossary](https://oecd.ai/en/glossary)
- [NIST AI RMF Playbook](https://ai.gov/nist-ai-rmf/)

_License: CC BY-SA 4.0_

_Source file: `data/terms/algorithmic-audit.yml`_
