<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# Glossary Terms

Total entries: 58

- [agentic ai](./agentic-ai.md) — Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.
- [ai incident response](./ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic bias](./algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [alignment](./alignment.md) — Making sure AI systems optimize for human values, policies, and intended outcomes.
- [attention](./attention.md) — Technique enabling models to weight input tokens differently when producing each output.
- [beam search](./beam-search.md) — Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.
- [bias-variance tradeoff](./bias-variance-tradeoff.md) — Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.
- [chunking](./chunking.md) — Splitting source documents into manageable pieces before indexing or feeding them to models.
- [clip](./clip.md) — Multimodal model that embeds images and text into a shared space using contrastive learning.
- [confusion matrix](./confusion-matrix.md) — Table that summarizes true/false positives and negatives to diagnose classification performance.
- [content moderation](./content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [context window](./context-window.md) — Maximum number of tokens a model can consider at once during prompting or inference.
- [cross-validation](./cross-validation.md) — Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.
- [data minimization](./data-minimization.md) — Principle of collecting and retaining only the data necessary for a defined purpose.
- [decoding](./decoding.md) — Algorithms that turn model probability distributions into output tokens during generation.
- [differential privacy](./differential-privacy.md) — Mathematical framework that limits how much any single record influences published data or model outputs.
- [diffusion model](./diffusion-model.md) — Generative model that iteratively denoises random noise to synthesize images, audio, or other data.
- [embedding](./embedding.md) — Dense numerical representation that captures semantic meaning of text, images, or other data.
- [evaluation](./evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [f1 score](./f1-score.md) — Harmonic mean of precision and recall, balancing false positives and false negatives.
- [fine-tuning](./fine-tuning.md) — Additional training that adapts a pretrained model to a specific task or domain.
- [generative ai](./generative-ai.md) — Family of models that produce new content—text, images, code—rather than only making predictions.
- [greedy decoding](./greedy-decoding.md) — Strategy that selects the highest-probability token at each step, producing deterministic outputs.
- [guardrails](./guardrails.md) — Controls that constrain model behavior to comply with safety, legal, or brand requirements.
- [hallucination](./hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [knowledge distillation](./knowledge-distillation.md) — Technique that trains a smaller student model to mimic a larger teacher model’s behavior.
- [kv cache](./kv-cache.md) — Stored attention keys and values reused across decoding steps to speed sequential generation.
- [log probability](./log-probability.md) — Logarithm of a token’s probability, used to inspect model confidence and guide decoding tweaks.
- [low-rank adaptation](./low-rank-adaptation.md) — Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.
- [ml observability](./ml-observability.md) — Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.
- [ml ops](./ml-ops.md) — Operational discipline that manages ML models from experimentation through deployment and monitoring.
- [model card](./model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](./model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model governance](./model-governance.md) — Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.
- [overfitting](./overfitting.md) — When a model memorizes training data patterns so closely that it performs poorly on new samples.
- [precision](./precision.md) — Share of predicted positives that are actually correct for a given classifier.
- [privacy](./privacy.md) — Principle of limiting data collection, use, and exposure to protect individuals’ information.
- [privacy impact assessment](./privacy-impact-assessment.md) — Structured review that evaluates how a system collects, uses, and safeguards personal data.
- [prompt engineering](./prompt-engineering.md) — Crafting and testing prompts to steer model behavior toward desired outcomes.
- [quantization](./quantization.md) — Technique that compresses model weights into lower-precision formats to shrink size and speed inference.
- [recall](./recall.md) — Share of actual positives a model successfully identifies.
- [red teaming](./red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [repetition penalty](./repetition-penalty.md) — Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.
- [reranking](./reranking.md) — Step that refines retrieval results using a more precise but slower scoring model.
- [responsible ai](./responsible-ai.md) — Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.
- [retrieval](./retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [retrieval-augmented generation](./retrieval-augmented-generation.md) — Workflow that grounds a generative model with retrieved context before producing output.
- [roc auc](./roc-auc.md) — Metric summarizing binary classifier performance by measuring area under the ROC curve.
- [safety evaluation](./safety-evaluation.md) — Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.
- [synthetic data](./synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [system prompt](./system-prompt.md) — Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.
- [temperature](./temperature.md) — Decoding parameter that controls how random or deterministic a model’s outputs are.
- [token](./token.md) — Smallest unit of text a model processes after tokenization, such as a word fragment or character.
- [tool use](./tool-use.md) — Pattern where a model selects external tools or functions to handle parts of a task.
- [top-k sampling](./top-k-sampling.md) — Decoding method that samples from the k most probable next tokens to balance diversity and control.
- [top-p sampling](./top-p-sampling.md) — Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.
- [vector store](./vector-store.md) — Database optimized to store embeddings and execute similarity search over vectors.
- [voice cloning](./voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.
