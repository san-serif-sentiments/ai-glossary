<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# synthetic data

**Aliases:** generated data, simulated data
**Categories:** Governance & Risk, Operations & Monitoring
**Roles:** Data Science & Research, Engineering & Platform, Policy & Risk, Product & Program Managers, Security & Trust
**Part of speech:** `noun`
**Status:** <span class="status-chip status-draft">Draft</span> (Last reviewed: 2024-11-02)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.

## Long definition
Synthetic data recreates statistical properties of real datasets without exposing exact records. Teams generate it with generative models, simulations, or rule-based scripts to augment scarce examples, balance demographic representation, or share data across boundaries. When used responsibly, synthetic data accelerates experimentation and protects privacy; when mismanaged, it can amplify biases, leak sensitive patterns, or give a false sense of security. Product and data science teams validate that synthetic datasets preserve signal relevant to their tasks, while policy and security partners evaluate whether the generation process meets governance requirements. Documentation should cover source data lineage, generation methods, and evaluation metrics such as fidelity, utility, and privacy leakage. Synthetic data is most effective when paired with differential privacy, guardrails, and robust monitoring that detects drift between simulated and real-world behavior.

## Audience perspectives
- **Exec:** Synthetic data lets teams test and train quickly without always touching sensitive customer records.
- **Engineer:** Generate data via simulations or generative models; validate fidelity, diversity, and privacy leakage before using it in pipelines.

## Examples
**Do**
- Track utility metrics comparing model performance on synthetic versus real validation sets.
- Label synthetic datasets clearly so downstream teams understand provenance.

**Don't**
- Assume synthetic data automatically removes biasâ€”measure subgroup impacts explicitly.
- Share synthetic datasets externally without privacy and legal review.

## Governance
- **NIST RMF tags:** privacy, data_quality
- **Risk notes:** Poorly generated data can encode discriminatory patterns or expose sensitive distributions; pair releases with privacy and bias assessments.

## Relationships
- **Broader:** data preprocessing
- **Related:** differential privacy, model drift, evaluation

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'synthetic data'.

## Citations
- [NIST AI RMF Glossary](https://airc.nist.gov/glossary/)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [UK POST AI Glossary](https://post.parliament.uk/publications/artificial-intelligence/ai-glossary/)

_License: CC BY-SA 4.0_

_Source file: `data/terms/synthetic-data.yml`_
