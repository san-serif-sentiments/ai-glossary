<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# synthetic data evaluation

**Aliases:** synthetic data quality assessment, synthetic validation
**Categories:** Operations & Monitoring, Governance & Risk
**Roles:** Data Science & Research, Engineering & Platform, Policy & Risk, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.

## Long definition
Synthetic data evaluation verifies that generated datasets mirror the statistical properties of real data without leaking sensitive information. Teams measure fidelity (distribution similarity), utility (model performance when trained on synthetic data), privacy leakage (distance or matching tests), and bias (subgroup balance). Evaluations often combine automated metrics—such as Wasserstein distance, nearest-neighbor tests, TSTR/TSNT—and human or domain-expert reviews. Product and policy teams review results to decide whether synthetic data is acceptable for training, testing, or sharing. Documentation should capture evaluation methods, thresholds, and mitigation plans when issues arise.

## Audience perspectives
- **Exec:** Synthetic data evaluation proves the generated data is both useful and safe before we rely on it.
- **Engineer:** Compute fidelity, utility, privacy, and bias metrics; compare against acceptance thresholds and log findings with dataset versions.

## Examples
**Do**
- Use real-world benchmarks (TSTR) to confirm models trained on synthetic data generalize.
- Validate privacy leakage with membership inference or nearest-neighbor tests.

**Don't**
- Assume synthetic data is safe without quantitative evaluation.
- Ignore subgroup representation when assessing utility.

## Governance
- **NIST RMF tags:** data_quality, privacy
- **Risk notes:** Poor evaluation can result in biased or leaky synthetic datasets reaching production or partners.

## Relationships
- **Broader:** synthetic data
- **Related:** differential privacy, fairness metrics, evaluation

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'synthetic data evaluation'.

## Citations
- [NIST AI RMF Glossary](https://airc.nist.gov/glossary/)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/synthetic-data-evaluation.yml`_
