<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# attention

**Aliases:** attention mechanism, self-attention
**Categories:** LLM Core
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun`
**Status:** `draft` (Last reviewed: 2024-11-01)

## Short definition
Technique enabling models to weight input tokens differently when producing each output.

## Long definition
Attention assigns dynamic importance scores to tokens so a model can focus on the most relevant parts of the sequence when generating or interpreting outputs. In transformer architectures, self-attention computes query, key, and value projections that interact through scaled dot products, allowing every token to attend to every other token in the same layer. Multi-head attention repeats this operation across parallel subspaces, capturing nuanced relationships such as syntax, long-range dependencies, and positional context. The mechanism replaced recurrent networks for many language and vision tasks by enabling parallel processing and rich contextual reasoning. Engineers diagnose quality issues by inspecting attention patterns, tuning head counts, or constraining context windows to manage memory. Governance teams monitor attention configurations because they influence explainabilityâ€”saliency maps and attribution methods often rely on attention weights to justify model decisions in regulated settings.

## Audience perspectives
- **Exec:** Attention is how the model decides which words or pixels matter most before answering.
- **Engineer:** QKV projections with softmax-normalized weights that let each token aggregate information from the entire sequence.

## Examples
**Do**
- Profile attention head usage to identify redundant heads before applying pruning or distillation.

**Don't**
- Assume longer context windows automatically improve answers without verifying attention saturation and memory usage.

## Governance
- **NIST RMF tags:** transparency, robustness
- **Risk notes:** Opaque attention patterns can hinder explainability obligations in regulated workflows.

## Relationships
- **Broader:** transformer
- **Narrower:** cross-attention, multi-head attention
- **Related:** context window, kv cache, token

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/attention.yml`_
