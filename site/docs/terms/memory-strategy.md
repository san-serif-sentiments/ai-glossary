<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# memory strategy

**Aliases:** agent memory strategy, memory policy
**Categories:** Agents & Tooling
**Roles:** Engineering & Platform, Product & Program Managers
**Part of speech:** `concept`
**Status:** <span class="status-chip status-reviewed">Reviewed</span> (Last reviewed: 2025-09-28)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Audit exposed tools against the safeguards described and document approval paths.
- Test hand-offs with human reviewers to confirm the safety expectations captured here are met.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Deliberate approach for when an AI agent stores, retrieves, or forgets context across tasks.

## Long definition
A memory strategy defines how an AI agent captures, retrieves, and expires contextual information while executing tasks. It balances cost, latency, privacy, and safety considerations by deciding which events to persist, where to store them, and which signals trigger deletion. Engineers design memory policies to avoid stale or sensitive data leaking into future prompts, while product teams ensure the experience remains transparent to users. Common patterns include stateless execution, short-term scratchpads, vector memory with embeddings, and human-curated summaries. The right strategy depends on regulatory constraints, user consent, and the risk tolerance for hallucinations or runaway tool use. Without clear guardrails, agents can over-collect data, misapply outdated facts, or introduce compliance gaps.

## Audience perspectives
- **Exec:** Align on what information the agent keeps so you can set customer trust and compliance expectations.
- **Engineer:** Codify storage tiers, retention windows, and redaction rules before wiring memory into agent workflows.

## Examples
**Do**
- Expire vector memories that include personally identifiable information within 24 hours unless explicit consent exists.
- Store only structured summaries of previous steps so downstream prompts stay within token and privacy budgets.

**Don't**
- Persist raw chat transcripts indefinitely without auditing for sensitive attributes.
- Reuse memories from one customer tenant inside another tenant's session.

## Governance
- **NIST RMF tags:** governance, privacy, risk_management
- **Risk notes:** Poorly scoped memory increases the blast radius for privacy violations and policy noncompliance.

## Relationships
- **Broader:** agentic ai, tool use
- **Related:** vector store, retrieval, data minimization

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'memory strategy'.

## Citations
- [LangChain Documentation – Memory](https://python.langchain.com/docs/modules/memory/)
- [Anthropic Docs – Building Agentic Workflows](https://docs.anthropic.com/claude/docs/building-agentic-workflows)

_License: CC BY-SA 4.0_

_Source file: `data/terms/memory-strategy.yml`_
