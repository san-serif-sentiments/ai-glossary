<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# f1 score

**Aliases:** f-score, harmonic mean of precision and recall
**Categories:** Foundations
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun`
**Status:** <span class="status-chip status-reviewed">Reviewed</span> (Last reviewed: 2025-09-28)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Add this concept to onboarding materials so teammates share a common baseline.
- Link supporting research or documentation in your internal wiki for deeper study.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Harmonic mean of precision and recall, balancing false positives and false negatives.

## Long definition
The F1 score combines precision and recall into a single metric by calculating their harmonic mean. It is especially useful when positive class distribution is imbalanced and teams need a balanced view of false positives and false negatives. An F1 score close to 1.0 indicates strong performance on both metrics, while low values signal trade-offs that require attention. Product managers use F1 to compare model variants quickly, and engineers track the score across cohorts to detect regressions after retraining or threshold changes. Because F1 weights precision and recall equally, decision-makers should confirm that assumption matches business priorities; otherwise metrics like F-beta or cost-based evaluation may be more appropriate.

## Audience perspectives
- **Exec:** F1 score summarizes how well the AI avoids both false alarms and missed detections.
- **Engineer:** 2 * (precision * recall) / (precision + recall); monitor per-class F1 and consider F-beta when recall or precision matters more.

## Examples
**Do**
- Report F1 alongside precision, recall, and confusion matrices for context.
- Track F1 trends after model updates to catch regressions early.

**Don't**
- Rely on F1 when business costs heavily favor precision or recall.
- Compare F1 across datasets without consistent class distributions.

## Governance
- **NIST RMF tags:** validity, transparency
- **Risk notes:** Misinterpreting F1 can hide risk trade-offs; document why the weight between precision and recall fits the use case.

## Relationships
- **Broader:** evaluation
- **Related:** precision, recall, confusion matrix

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'f1 score'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/f1-score.yml`_
