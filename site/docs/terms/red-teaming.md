<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# red teaming

**Aliases:** ai red teaming, adversarial testing
**Categories:** Governance & Risk, Operations & Monitoring
**Roles:** Communications & Enablement, Engineering & Platform, Legal & Compliance, Policy & Risk, Product & Program Managers, Security & Trust
**Part of speech:** `process`
**Status:** `draft` (Last reviewed: 2024-11-01)

## Short definition
Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.

## Long definition
Red teaming mobilizes interdisciplinary experts to craft adversarial prompts, scenarios, and data inputs that challenge an AI system’s safeguards. The goal is to uncover failure modes—such as unsafe content, confidential data leaks, or jailbreak exploits—before attackers or end users discover them. Exercises blend automated probing, scripted attack playbooks, and human creativity. Findings feed into remediation plans for guardrails, prompts, training data, or escalation policies. Product leaders schedule recurring red-team cycles for high-risk surfaces, while engineers build tooling to log attempts, reproduce issues, and verify fixes. Governance teams treat red teaming as part of risk management, requiring documentation of scope, participants, severity ratings, and follow-up actions. In many jurisdictions, regulators expect evidence that red teaming has been performed for sensitive deployments, making it a core component of responsible AI programs.

## Audience perspectives
- **Exec:** Red teaming is the pre-launch fire drill that exposes how the AI could fail or be abused.
- **Engineer:** Design adversarial prompts and automated probes, capture reproduction artifacts, and track mitigation work in the backlog.

## Examples
**Do**
- Incorporate marginalized community perspectives when designing red-team scenarios.

**Don't**
- Close a red-team finding without documenting remediation owners and timelines.

## Governance
- **NIST RMF tags:** risk_management, accountability
- **Risk notes:** Skipping red teaming leaves blind spots that can result in public incidents or regulatory enforcement.

## Relationships
- **Broader:** evaluation
- **Related:** guardrails, alignment, incident response

## Citations
- [NIST AI RMF Glossary](https://airc.nist.gov/glossary/)
- [UK POST AI Glossary](https://post.parliament.uk/publications/artificial-intelligence/ai-glossary/)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/red-teaming.yml`_
