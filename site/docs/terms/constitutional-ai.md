<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# constitutional ai

**Aliases:** principle-guided alignment, self-critique alignment
**Categories:** Governance & Risk, LLM Core
**Roles:** Product & Program Managers, Engineering & Platform, Policy & Risk
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Alignment approach where models critique and revise their own outputs against a written set of principles.

## Long definition
Constitutional AI trains or steers language models with a "constitution"â€”explicit guidelines that describe desired and disallowed behaviors. During training or inference the model generates an answer, critiques it against the constitution, and revises the response to better satisfy those rules. Teams can combine human red teaming with model self-critique to iterate on the constitution, expanding coverage for safety, tone, or compliance requirements. Because principles are documented, product and policy partners can review, update, and audit them as new regulations emerge. Engineering groups wire the constitution into reinforcement learning loops, guardrail services, or inference-time post-processing. The technique complements human feedback and provides a transparent way to encode organizational values directly into model behavior.

## Audience perspectives
- **Exec:** Constitutional AI makes our safety playbook explicit so models can self-correct toward company values.
- **Engineer:** Use principle prompts to drive self-critique loops or policy filters that nudge generations toward compliant behavior.

## Examples
**Do**
- Version control the constitution and sync revisions with evaluation runs before deployment.

**Don't**
- Ship new principles without testing how they affect harmless user journeys.

## Governance
- **NIST RMF tags:** accountability, validity, transparency
- **Risk notes:** Opaque or outdated constitutions create false confidence that safety requirements are being enforced.

## Relationships
- **Broader:** alignment
- **Related:** guardrails, red teaming, fine-tuning, temperature

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'constitutional ai'.

## Citations
- [Anthropic / Claude Glossary](https://www.anthropic.com/news/model-spec-for-ai-safety)
- [DeepLearning.AI AI Glossary](https://www.deeplearning.ai/glossary/)
- [MIT Technology Review AI Dictionary](https://builtin.com/artificial-intelligence/glossary-artificial-intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/constitutional-ai.yml`_
