<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# context window

**Aliases:** context length, sequence length limit
**Categories:** LLM Core
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun_phrase`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Maximum number of tokens a model can consider at once during prompting or inference.

## Long definition
The context window defines how many tokens a model can ingest in a single forward pass, combining both prompt input and generated output. Because transformer attention scales quadratically with sequence length, vendors ship models with fixed limits that balance cost, latency, and accuracy. Exceeding the window forces truncation or sliding-window strategies that can drop critical instructions. Product teams map context size to user scenarios—for example, how long a document can be summarized or how many chat messages remain in memory. Engineers manage context budgets through prompt compression, retrieval chunking, and caching mechanisms like recurrent attention or stateful decoding. Governance stakeholders examine context policies to ensure sensitive data is not unintentionally persisted across conversations or logged beyond retention periods, especially when long windows capture personally identifiable information.

## Audience perspectives
- **Exec:** A context window is the attention span of the model—go past it and instructions fall off.
- **Engineer:** Token budget for input plus output per request; drives attention memory use and dictates truncation strategies.

## Examples
**Do**
- Document context requirements by user journey so prompts stay within safe token limits.

**Don't**
- Assume extending the context window eliminates the need for retrieval or prompt optimization.

## Governance
- **NIST RMF tags:** transparency, privacy
- **Risk notes:** Oversized contexts can capture unnecessary personal data and complicate retention or deletion obligations.

## Relationships
- **Broader:** prompt engineering
- **Narrower:** long-context models
- **Related:** token, attention, kv cache

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'context window'.

## Citations
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [NIST AI RMF Glossary](https://www.nist.gov/itl/ai-risk-management-framework)

_License: CC BY-SA 4.0_

_Source file: `data/terms/context-window.yml`_
