<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# mixture of experts

**Aliases:** moe, expert gating
**Categories:** Optimization & Efficiency
**Roles:** Engineering & Platform, Data Science & Research, Product & Program Managers
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Record before-and-after performance metrics when applying this optimisation technique.
- Document trade-offs for product and policy partners using the glossary's language.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.

## Long definition
Mixture-of-experts (MoE) models split a network into many expert submodels and use a gating function to decide which experts process each token. This allows models to increase parameter counts without running every computation for every input, improving efficiency and enabling specialization (e.g., code vs. dialogue experts). Engineers must manage load balancing, routing stability, and inference infrastructure that can handle sparse activation. Product teams care about how specialization affects consistency across use cases, and governance teams assess whether routing introduces bias or explainability challenges. Operationally, MoE systems demand robust logging to trace which experts contributed to an output for debugging and accountability.

## Audience perspectives
- **Exec:** MoE architectures stretch compute budgets while boosting quality, but require investment in routing controls.
- **Engineer:** Monitor expert utilization, balance routing, and record which experts fired for auditability.

## Examples
**Do**
- Set guardrails for experts that handle sensitive domains like legal or medical content.
- Instrument per-expert performance metrics to catch degradation early.

**Don't**
- Assume routing is fair without analyzing demographic or domain skew.
- Deploy MoE models without redundancy plans for underperforming experts.

## Governance
- **NIST RMF tags:** monitoring, risk_management, transparency
- **Risk notes:** Opaque routing can hide failures or bias; sparse activation complicates reproducibility.

## Relationships
- **Broader:** fine-tuning
- **Related:** knowledge distillation, model interpretability, evaluation

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'mixture of experts'.

## Citations
- [Google Research – Switch Transformers](https://arxiv.org/abs/2101.03961)
- [Google Brain – Outrageously Large Neural Networks](https://arxiv.org/abs/1701.06538)

_License: CC BY-SA 4.0_

_Source file: `data/terms/mixture-of-experts.yml`_
