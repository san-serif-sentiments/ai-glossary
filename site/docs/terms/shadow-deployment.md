<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# shadow deployment

**Aliases:** shadow mode, silent launch
**Categories:** Operations & Monitoring
**Roles:** Engineering & Platform, Product & Program Managers, Policy & Risk, Security & Trust
**Part of speech:** `noun_phrase`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Deploying an AI system alongside the existing workflow without user impact to collect telemetry.

## Long definition
In a shadow deployment, an AI model runs in production but its outputs are not shown to end users or do not control the primary workflow. Teams compare shadow results against human decisions, measure safety metrics, and stress-test guardrails before full launch. Engineering sets up parallel pipelines, product owners define success criteria, and policy leaders review logs for compliance or bias. Shadow mode provides real-world data without exposing customers to unverified behavior, but only if teams actively review the telemetry and follow up on issues.

## Audience perspectives
- **Exec:** Use shadow deployments to de-risk launches and inform go/no-go decisions with real data.
- **Engineer:** Instrument drift, safety, and latency metrics during shadow mode and share findings broadly.

## Examples
**Do**
- Run shadow mode until evaluation thresholds and mitigation backlog items are cleared.
- Log disagreements between the AI and human decisions for analysis.

**Don't**
- Let shadow deployments run without active monitoring or ownership.
- Ignore privacy commitments when storing shadow outputs.

## Governance
- **NIST RMF tags:** monitoring, risk_management, governance
- **Risk notes:** Shadow mode without review creates false confidence—issues surface only after public launch.

## Relationships
- **Broader:** ml ops
- **Related:** evaluation harness, impact mitigation plan, risk register

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'shadow deployment'.

## Citations
- [Google SRE – Canary Releases and Shadowing](https://sre.google/sre-book/release-engineering/)
- [AWS – Shadow Testing Machine Learning Models](https://aws.amazon.com/blogs/machine-learning/how-to-shadow-test-machine-learning-models/)

_License: CC BY-SA 4.0_

_Source file: `data/terms/shadow-deployment.yml`_
