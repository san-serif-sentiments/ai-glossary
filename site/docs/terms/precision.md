<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# precision

**Aliases:** positive predictive value, ppv
**Categories:** Foundations
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers, Policy & Risk
**Part of speech:** `noun`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Add this concept to onboarding materials so teammates share a common baseline.
- Link supporting research or documentation in your internal wiki for deeper study.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Share of predicted positives that are actually correct for a given classifier.

## Long definition
Precision measures how often a model’s positive predictions are right. It is calculated as true positives divided by the sum of true positives and false positives. Precision is critical when false positives carry high costs—such as flagging legitimate transactions as fraud or routing harmless content to safety reviewers. Product managers examine precision alongside recall to balance user trust and coverage, while engineers monitor precision across subgroups to detect drift and bias. Governance teams require precision reporting in regulated workflows where incorrect flags can create legal exposure. Precision is best interpreted in context with other metrics and operational goals, since maximizing it alone may reduce recall and harm users the model fails to detect.

## Audience perspectives
- **Exec:** Precision tells you how many of the alerts the AI raises are actually right.
- **Engineer:** TP / (TP + FP); inspect per-class and per-segment precision, and monitor trade-offs with recall and business cost.

## Examples
**Do**
- Track precision at the thresholds used in production, not just at 0.5.
- Compare precision across demographics to uncover unfair false-positive rates.

**Don't**
- Optimize precision without checking the drop in recall.
- Report a single precision value when different user segments behave differently.

## Governance
- **NIST RMF tags:** validity, accountability
- **Risk notes:** Low precision increases false alarms, eroding trust and causing unnecessary interventions.

## Relationships
- **Broader:** evaluation
- **Related:** recall, f1 score, confusion matrix

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'precision'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/precision.yml`_
