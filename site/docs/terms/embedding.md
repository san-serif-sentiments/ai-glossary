<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# embedding

**Aliases:** vector embedding, representation vector
**Categories:** Retrieval & RAG
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Validate retrieval quality using the evaluation guidance referenced in this entry.
- Ensure knowledge sources named here appear in your data governance inventory.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Dense numerical representation that captures semantic meaning of text, images, or other data.

## Long definition
Embeddings map pieces of information—such as words, sentences, images, or audio—into dense vectors so similar concepts cluster in vector space. Modern foundation models learn embeddings during pretraining, optimizing them to capture contextual meaning, syntactic roles, and domain signals that make downstream tasks like retrieval, classification, and recommendation more effective. When teams build retrieval-augmented generation systems, they compute embeddings for documents and queries, then compare them with similarity metrics like cosine distance or dot product. The quality of those vectors influences whether the right context is retrieved, how well clustering and anomaly detection work, and how fast nearest-neighbor searches execute. Governance programs scrutinize embedding pipelines because biased or outdated vectors can encode harmful associations and propagate them into products. Maintaining embeddings thus requires dataset curation, versioning, and monitoring to detect drift as language or policies change.

## Audience perspectives
- **Exec:** Embeddings are a way to translate concepts into numbers so the system can find and compare ideas efficiently.
- **Engineer:** Model-generated vectors where distance metrics approximate semantic similarity; used for retrieval, clustering, ranking, and transfer learning.

## Examples
**Do**
- Version embedding models and store associated tokenizer metadata to support reproducibility.

**Don't**
- Mix embeddings from different models without recalculating similarity thresholds or rescoring.

## Governance
- **NIST RMF tags:** data_quality, accountability
- **Risk notes:** Embedding drift or biased training data can surface discriminatory outputs when used in search or ranking.

## Relationships
- **Broader:** representation learning
- **Narrower:** sentence embedding, image embedding
- **Related:** retrieval, vector store, chunking

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'embedding'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [NIST AI RMF Glossary](https://airc.nist.gov/glossary/)

_License: CC BY-SA 4.0_

_Source file: `data/terms/embedding.yml`_
