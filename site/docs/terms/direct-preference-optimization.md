<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# direct preference optimization

**Aliases:** dpo, preference optimization without rlhf
**Categories:** LLM Core
**Roles:** Engineering & Platform, Data Science & Research, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Alignment technique that fine-tunes models directly on preference data without training a separate reward model.

## Long definition
Direct preference optimization (DPO) replaces the reward-model-and-RL step in traditional RLHF with a closed-form objective that pushes the model toward preferred responses. Using paired comparisons, it optimizes log-likelihood ratios so preferred outputs receive higher probability while disfavored ones decrease. DPO simplifies infrastructure and reduces instability caused by reinforcement learning. Engineers still need high-quality preference datasets and evaluation benchmarks, while policy teams confirm that annotator guidance reflects safety requirements. Poor datasets or hyperparameter choices can lead to overfitting, regressions, or policy drift, so monitoring remains critical.

## Audience perspectives
- **Exec:** DPO lowers alignment overhead but still requires tight governance on data quality and evaluations.
- **Engineer:** Track win rates and safety metrics before and after DPO to ensure improvements hold.

## Examples
**Do**
- Re-run safety and quality benchmarks after each optimization pass.
- Blend DPO with instruction tuning refreshes to keep behavior current.

**Don't**
- Assume DPO eliminates the need for human review or reward analyses.
- Train on preference data without validating annotator consistency.

## Governance
- **NIST RMF tags:** measurement, risk_management
- **Risk notes:** Misaligned preference data can push the model toward unsafe behavior even without RL instabilities.

## Relationships
- **Broader:** reinforcement learning from human feedback
- **Related:** preference dataset, instruction tuning, reward model

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'direct preference optimization'.

## Citations
- [Stanford – Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
- [Anthropic – Lessons from Preference Optimization](https://www.anthropic.com/research/preference-optimization)

_License: CC BY-SA 4.0_

_Source file: `data/terms/direct-preference-optimization.yml`_
