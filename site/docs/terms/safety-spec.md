<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# safety spec

**Aliases:** safety specification, model safety policy
**Categories:** Governance & Risk, LLM Core
**Roles:** Product & Program Managers, Engineering & Platform, Policy & Risk, Security & Trust, Communications & Enablement
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.

## Long definition
A safety spec translates high-level AI principles into actionable guidance for a specific system. It enumerates desired behaviours, prohibited outputs, escalation paths, and fallback actions across scenarios such as self-harm, misinformation, sensitive topics, or abuse. Product and policy teams draft the intent; engineering and safety specialists convert it into prompts, classifiers, guardrails, and monitoring hooks; communications teams prepare messaging for when guardrails trigger. A good safety spec links each requirement to reference examples, evaluation suites, and human review steps, so updates can be tested before deployment. The document is version-controlled alongside prompts and model configs, and is revisited after incidents, red teaming, or regulatory changes. Without a safety spec, expectations stay tribal, handoffs break down, and teams cannot prove alignment with industry or statutory obligations.

## Audience perspectives
- **Exec:** Shows how we operationalise safety promises with concrete rules, owners, and escalation paths.
- **Engineer:** Clarifies what the model must block or escalate and how to wire guardrails, logging, and overrides.

## Examples
**Do**
- Version-control the safety spec and require sign-off from policy, legal, and engineering before shipping changes.
- Attach evaluation prompts and datasets that must pass before releasing a new prompt or model checkpoint.

**Don't**
- Copy a generic policy without adding concrete prompts, thresholds, or handoff procedures.
- Let the safety spec drift after incidents—update the rules and tests while lessons are fresh.

## Governance
- **NIST RMF tags:** risk_management, accountability, transparency
- **Risk notes:** Outdated or missing safety specs leave teams unable to enforce guardrails or defend decisions during audits.

## Relationships
- **Broader:** alignment, ai assurance
- **Related:** guardrails, red teaming, prompt engineering, ai incident response

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'safety spec'.

## Citations
- [Anthropic Safety Spec](https://www.anthropic.com/news/model-spec-for-ai-safety)
- [OpenAI – Usage Policies](https://openai.com/policies/usage-policies)
- [Anthropic – Model Spec for AI Safety](https://www.anthropic.com/news/model-spec-for-ai-safety)

_License: CC BY-SA 4.0_

_Source file: `data/terms/safety-spec.yml`_
