<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# knowledge distillation

**Aliases:** distillation, teacher-student training
**Categories:** Optimization & Efficiency
**Roles:** Data Science & Research, Engineering & Platform
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.

## Practice & apply
- Record before-and-after performance metrics when applying this optimisation technique.
- Document trade-offs for product and policy partners using the glossary's language.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Technique that trains a smaller student model to mimic a larger teacher model’s behavior.

## Long definition
Knowledge distillation transfers capabilities from a high-capacity teacher model to a more efficient student model by training the student on the teacher’s softened output distributions or generated examples. The approach preserves much of the teacher’s accuracy while reducing parameter count, latency, and cost—making it popular for edge deployments and inference scaling. Distillation complements other efficiency strategies such as quantization or pruning, enabling organizations to meet budget constraints without abandoning model quality. Engineers configure temperature, loss weighting, and dataset selection to ensure the student captures critical behaviors, sometimes blending hard labels with soft targets. Governance teams review distillation pipelines to confirm the teacher and student share licensing compatibility and that synthetic data generation respects privacy obligations. Because distillation can copy undesirable biases along with strengths, evaluations must confirm that risk mitigations remain effective in the distilled model.

## Audience perspectives
- **Exec:** Distillation keeps model quality high while shrinking the model to run faster and cheaper.
- **Engineer:** Train a student network using teacher logits or generated traces so it approximates the teacher’s function with fewer parameters.

## Examples
**Do**
- Document which teacher checkpoints and datasets produced each distilled release.

**Don't**
- Distill sensitive capabilities without reassessing safety performance on the student model.

## Governance
- **NIST RMF tags:** efficiency, validity
- **Risk notes:** Students inherit teacher biases; missing evaluations can hide regressions introduced during compression.

## Relationships
- **Broader:** model optimization
- **Related:** quantization, low-rank adaptation, evaluation

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'knowledge distillation'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/knowledge-distillation.yml`_
