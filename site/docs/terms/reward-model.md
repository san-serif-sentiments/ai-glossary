<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# reward model

**Aliases:** preference model, policy reward model
**Categories:** LLM Core
**Roles:** Engineering & Platform, Data Science & Research, Policy & Risk
**Part of speech:** `noun_phrase`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Model trained on human preferences that scores AI responses for alignment or quality.

## Long definition
A reward model predicts how well a generated response aligns with human preferences or policy guidelines. Teams train it on comparison data where annotators choose the better of two or more outputs. The reward model then guides reinforcement learning, ranking, or rejection sampling to steer the underlying AI system. Because it encodes policy judgments, the model must be audited for bias, saturation, and overfitting. Engineers monitor reward drift, policy teams review labeling instructions, and data scientists retrain the model as new behaviors emerge. If the reward model is misaligned or gamed, the downstream system can optimize for the wrong incentives.

## Audience perspectives
- **Exec:** Treat reward models as governance artifacts—they translate policy into scalable automation.
- **Engineer:** Version datasets, metrics, and calibration checks so the reward model stays trustworthy.

## Examples
**Do**
- Validate reward scores against a holdout of human-reviewed examples.
- Document known blind spots and include them in launch gates.

**Don't**
- Deploy reward models without monitoring for reward hacking.
- Train on unverified or biased annotations.

## Governance
- **NIST RMF tags:** measurement, risk_management, accountability
- **Risk notes:** Reward models encode subjective policy choices; poor oversight leads to misaligned optimization.

## Relationships
- **Broader:** reinforcement learning from human feedback
- **Related:** alignment, evaluation harness, safety spec

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'reward model'.

## Citations
- [OpenAI – InstructGPT Paper](https://arxiv.org/abs/2203.02155)
- [DeepMind – RLHF and Reward Modelling](https://www.deepmind.com/blog/learning-through-human-feedback)

_License: CC BY-SA 4.0_

_Source file: `data/terms/reward-model.yml`_
