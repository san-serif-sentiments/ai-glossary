<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# content moderation

**Aliases:** trust and safety, policy enforcement
**Categories:** Governance & Risk, Operations & Monitoring
**Roles:** Policy & Risk, Communications & Enablement, Product & Program Managers, Security & Trust, Engineering & Platform
**Part of speech:** `process`
**Status:** `draft` (Last reviewed: 2024-11-03)

## Short definition
Workflows and tools that review, filter, and act on user-generated content to enforce policy.

## Long definition
Content moderation combines automation, human review, and escalation procedures to detect policy violations such as hate speech, harassment, misinformation, or disallowed imagery. AI systems often provide the first layer of moderation by classifying or scoring content for human queues, requiring careful tuning of precision/recall trade-offs. Policy teams define enforcement rules, while communications and legal stakeholders handle appeals and transparency reports. Engineering teams maintain moderation pipelines, logging, and guardrails; security teams ensure abuse detection remains resilient. Effective moderation programs rely on measurement, red-teaming, and incident response to adapt to adversarial users and evolving regulations.

## Audience perspectives
- **Exec:** Content moderation protects users and the brand by keeping AI outputs and user posts within policy.
- **Engineer:** Blend classifiers, heuristic filters, and human review; monitor performance, appeals, and adversarial attempts.

## Examples
**Do**
- Audit moderation models for bias against protected groups.
- Publish user-facing guidelines and escalation paths.

**Don't**
- Rely solely on automation without human oversight for edge cases.
- Ignore feedback loops when policies change.

## Governance
- **NIST RMF tags:** risk_management, accountability
- **Risk notes:** Weak moderation exposes users to harm, invites regulatory fines, and erodes trust.

## Relationships
- **Broader:** guardrails
- **Related:** safety evaluation, incident response, algorithmic bias

## Citations
- [UK POST AI Glossary](https://post.parliament.uk/publications/artificial-intelligence/ai-glossary/)
- [NIST AI RMF Glossary](https://airc.nist.gov/glossary/)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/content-moderation.yml`_
