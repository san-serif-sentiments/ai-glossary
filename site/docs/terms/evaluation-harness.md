<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# evaluation harness

**Aliases:** eval harness, agent evaluation pipeline
**Categories:** Operations & Monitoring
**Roles:** Engineering & Platform, Data Science & Research, Product & Program Managers
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Instrument dashboards or alerts that reflect the metrics highlighted in this definition.
- Update incident response or on-call runbooks with the glossary's do/don't scenarios.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.

## Long definition
An evaluation harness packages datasets, prompts, grading logic, and reporting into a repeatable testing loop for AI systems. Teams use it to replay golden tasks, compare new model checkpoints against baselines, and surface regressions before releases. Mature harnesses capture qualitative rubrics, automated metrics, and policy checks in the same run so results are auditable. Product managers wire the harness into launch gates, engineers integrate it with CI/CD, and data scientists own the benchmarks and scoring logic. When paired with incident telemetry, the harness becomes the source of truth for whether mitigations are holding. Without one, evaluation drifts to ad-hoc notebook experiments, making safety, equity, and quality decisions opaque.

## Audience perspectives
- **Exec:** Tie model releases to a standard testing loop so you can watch risk and quality trends across launches.
- **Engineer:** Embed the harness into CI so every model or prompt change ships with quantitative and policy signals.

## Examples
**Do**
- Schedule nightly harness runs against red-team prompts and archive reports in the governance dashboard.
- Track score deltas by dataset slice so regressions in protected classes trigger blocking alerts.

**Don't**
- Rely on one-off notebooks or manual spot checks to validate safety-critical behaviors.
- Ignore harness failures when product metrics trend up; that hides governance or fairness regressions.

## Governance
- **NIST RMF tags:** measurement, monitoring, risk_management
- **Risk notes:** Skipping automation makes it impossible to prove controls work or to show due diligence during audits.

## Relationships
- **Broader:** evaluation, ml ops
- **Narrower:** robust prompting, synthetic data evaluation
- **Related:** safety evaluation, red teaming, responsible ai

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'evaluation harness'.

## Citations
- [LangChain Documentation – Evaluation](https://python.langchain.com/docs/guides/evaluation/)
- [OpenAI Blog – Introducing OpenAI Evals](https://openai.com/blog/evals)

_License: CC BY-SA 4.0_

_Source file: `data/terms/evaluation-harness.yml`_
