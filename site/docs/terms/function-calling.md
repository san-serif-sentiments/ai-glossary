<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# function calling

**Aliases:** tool calling, structured output invocation
**Categories:** Agents & Tooling, LLM Core
**Roles:** Engineering & Platform, Product & Program Managers, Data Science & Research
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.

## Practice & apply
- Audit exposed tools against the safeguards described and document approval paths.
- Test hand-offs with human reviewers to confirm the safety expectations captured here are met.
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
LLM capability that lets prompts invoke predefined functions and return structured arguments.

## Long definition
Function calling bridges natural language prompts and executable tools. Developers describe approved functions—such as retrieval queries, database lookups, or workflow automations—in a schema the model can reason about. During inference the model decides whether to call a function, fills the arguments with structured JSON, and uses the tool's response to continue the conversation. Product teams rely on function calling to chain RAG pipelines, trigger actions, or enforce form validation while keeping humans in the loop for sensitive steps. Robust implementations include guardrails on which functions are exposed, argument validation, and logging so security and compliance teams can audit usage. When paired with agent frameworks, function calling becomes the backbone for orchestrating multi-step tasks.

## Audience perspectives
- **Exec:** Function calling lets assistants take reliable actions by routing model suggestions into vetted business APIs.
- **Engineer:** Expose idempotent, well-typed functions, validate arguments, and loop tool responses back into the conversation state.

## Examples
**Do**
- Start with read-only functions and add write access only after logging and approval workflows exist.

**Don't**
- Expose unrestricted shell commands or production secrets as callable functions.

## Governance
- **NIST RMF tags:** accountability, transparency
- **Risk notes:** Unrestricted function catalogs can let models trigger unsafe actions without audit trails.

## Relationships
- **Broader:** tool-use
- **Related:** agentic ai, system prompt, retrieval-augmented generation, guardrails

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'function calling'.

## Citations
- [OpenAI Glossary](https://help.openai.com/en/articles/6825453-openai-api-glossary)
- [LangChain Documentation Glossary](https://python.langchain.com/v0.2/docs/concepts)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)

_License: CC BY-SA 4.0_

_Source file: `data/terms/function-calling.yml`_
