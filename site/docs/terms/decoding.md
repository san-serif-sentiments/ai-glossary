<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# decoding

**Aliases:** text decoding, generation decoding
**Categories:** LLM Core
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-draft">Draft</span> (Last reviewed: 2024-11-01)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Algorithms that turn model probability distributions into output tokens during generation.

## Long definition
Decoding encompasses the strategies used to convert a model’s token probabilities into concrete outputs. Common methods include greedy decoding, beam search, top-k sampling, and nucleus (top-p) sampling, each trading off determinism, diversity, latency, and risk. Selection of a decoding algorithm affects user experience, safety posture, and evaluation results far more than many teams realize. Product managers configure decoding policies by use case—for example, deterministic responses for support scenarios and higher-variance sampling for ideation tools. Engineers implement controls such as temperature scaling, repetition penalties, and logprob thresholds to manage failure modes like hallucinations or loops. Governance programs capture decoding settings in change logs because updates can materially alter risk assessments. Understanding decoding is foundational to building reliable guardrails, interpreting evaluation results, and communicating the behavior of generative systems to stakeholders.

## Audience perspectives
- **Exec:** Decoding is the decision logic that turns the model’s probabilities into the words customers see.
- **Engineer:** Apply algorithms like greedy, top-k, or top-p to sample from the softmax distribution while enforcing constraints and penalties.

## Examples
**Do**
- Document decoding parameters alongside each release note for regulated experiences.

**Don't**
- Mix decoding strategies across channels without coordinating evaluation coverage.

## Governance
- **NIST RMF tags:** transparency, robustness
- **Risk notes:** Untracked decoding changes can invalidate safety testing and confuse incident investigations.

## Relationships
- **Broader:** generative ai
- **Narrower:** greedy decoding, top-k sampling, top-p sampling
- **Related:** temperature, log probability, guardrails

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'decoding'.

## Citations
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/decoding.yml`_
