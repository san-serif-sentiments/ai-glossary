<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# recall

**Aliases:** sensitivity, true positive rate
**Categories:** Foundations
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers, Policy & Risk
**Part of speech:** `noun`
**Status:** <span class="status-chip status-draft">Draft</span> (Last reviewed: 2024-11-03)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Add this concept to onboarding materials so teammates share a common baseline.
- Link supporting research or documentation in your internal wiki for deeper study.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Share of actual positives a model successfully identifies.

## Long definition
Recall measures how well a model finds all relevant cases by dividing true positives by the sum of true positives and false negatives. High recall is essential when missing a positive case is costly or dangerous, such as failing to flag offensive content or overlooking a critical defect. When teams adjust thresholds, recall often moves in the opposite direction of precision, so decision-makers must weigh acceptable trade-offs. Engineers analyze recall per subgroup to ensure equitable performance, while policy and product teams set recall targets aligned with risk appetite and regulatory requirements. Recall is commonly monitored in tandem with precision, F1 score, and confusion matrices to understand overall effectiveness.

## Audience perspectives
- **Exec:** Recall is how many of the real problems the AI actually catches.
- **Engineer:** TP / (TP + FN); evaluate recall across classes and user cohorts, and tune thresholds or sampling to meet coverage goals.

## Examples
**Do**
- Set minimum recall thresholds for safety-critical detections.
- Monitor recall daily to catch regressions caused by data drift.

**Don't**
- Maximize recall without reviewing the spike in false positives.
- Report aggregate recall without subgroup breakdowns.

## Governance
- **NIST RMF tags:** validity, accountability
- **Risk notes:** Poor recall leaves high-risk events undetected, undermining safety promises and compliance obligations.

## Relationships
- **Broader:** evaluation
- **Related:** precision, f1 score, confusion matrix

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'recall'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/recall.yml`_
