<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# reinforcement learning from human feedback

**Aliases:** rlhf, preference optimization
**Categories:** LLM Core
**Roles:** Engineering & Platform, Data Science & Research, Product & Program Managers, Policy & Risk
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Training approach that tunes a model using reward signals learned from human preference data.

## Long definition
Reinforcement learning from human feedback (RLHF) trains a reward model on human preference comparisons, then uses that reward model to fine-tune a base language model with reinforcement learning. The process typically involves collecting prompts and multiple responses, asking human labelers which response better follows policy, training a reward model on those rankings, and running a reinforcement learning algorithm (often PPO) to optimize the model’s behavior. RLHF aligns outputs with human expectations when explicit rules are difficult to encode. Product teams define the guidelines labelers follow, policy teams ensure safety requirements are reflected, and engineers monitor for reward hacking or regressions. RLHF requires continuous refreshes as policies evolve; stale preference data can drift from current standards.

## Audience perspectives
- **Exec:** Budget for ongoing RLHF cycles so the model keeps pace with policy and product shifts.
- **Engineer:** Instrument reward signals, evaluation sets, and safety metrics to catch regressions early.

## Examples
**Do**
- Collect diverse preference data that captures edge cases and sensitive topics.
- Audit reward model calibration to prevent exploiting annotation quirks.

**Don't**
- Assume one RLHF pass will keep a model aligned indefinitely.
- Let annotator instructions diverge from published safety or product policies.

## Governance
- **NIST RMF tags:** risk_management, monitoring, transparency
- **Risk notes:** Poorly curated feedback can encode bias or create reward hacking that violates policy.

## Relationships
- **Broader:** fine-tuning
- **Related:** reward model, alignment, robust prompting

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'reinforcement learning from human feedback'.

## Citations
- [OpenAI – Learning from Human Preferences](https://openai.com/research/learning-from-human-preferences)
- [InstructGPT: Training Language Models to Follow Instructions](https://arxiv.org/abs/2203.02155)
- [Hugging Face – RLHF Guide](https://huggingface.co/blog/rlhf)

_License: CC BY-SA 4.0_

_Source file: `data/terms/reinforcement-learning-from-human-feedback.yml`_
