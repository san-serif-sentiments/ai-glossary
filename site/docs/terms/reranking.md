<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# reranking

**Aliases:** re-ranking, second-stage ranking
**Categories:** Retrieval & RAG
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-draft">Draft</span> (Last reviewed: 2024-11-01)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Validate retrieval quality using the evaluation guidance referenced in this entry.
- Ensure knowledge sources named here appear in your data governance inventory.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Step that refines retrieval results using a more precise but slower scoring model.

## Long definition
Reranking applies a secondary model to an initial set of retrieved documents to reorder them according to task-specific relevance. The first retrieval pass prioritizes speed, often using sparse or dense similarity search to gather dozens of candidates. A reranker—frequently a cross-encoder or lightweight generative model—then evaluates each candidate alongside the query to produce fine-grained scores. This approach improves precision, supports citation quality in RAG, and helps guardrails by pushing irrelevant or risky passages down the list. Engineers tune rerankers by limiting candidate set sizes, caching scores, and measuring latency budgets. Governance considerations include documenting model provenance, ensuring the reranker does not encode discriminatory bias, and verifying that safety filters still apply after reordering. When reranking is well managed, it improves both factual accuracy and user trust without incurring the cost of fully generative evaluation for every document in the corpus.

## Audience perspectives
- **Exec:** Reranking lets us double-check the top answers so users see the most reliable sources first.
- **Engineer:** Second-stage scorer—often a cross-encoder—reorders retrieved candidates to boost precision while respecting latency limits.

## Examples
**Do**
- Measure precision@k with and without reranking to justify added latency.

**Don't**
- Deploy rerankers trained on outdated data without monitoring for regressions in safety filters.

## Governance
- **NIST RMF tags:** validity, transparency
- **Risk notes:** Biased rerankers can systematically down-rank protected viewpoints or surface outdated information.

## Relationships
- **Broader:** retrieval
- **Narrower:** cross-encoder reranking
- **Related:** retrieval-augmented generation, chunking, guardrails

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'reranking'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/reranking.yml`_
