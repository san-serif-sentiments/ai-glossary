<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# retrieval-augmented generation

**Aliases:** RAG, retrieval augmented generation
**Categories:** Retrieval & RAG
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-draft">Draft</span> (Last reviewed: 2024-09-18)

!!! tip "Put it into practice"
    Consult the [Category Explorer](../categories.md#retrieval--rag) for end-to-end grounding guidance.

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Validate retrieval quality using the evaluation guidance referenced in this entry.
- Ensure knowledge sources named here appear in your data governance inventory.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Workflow that grounds a generative model with retrieved context before producing output.

## Long definition
Retrieval-augmented generation (RAG) combines information retrieval and text generation so that a model answers questions using relevant, up-to-date context instead of only relying on patterns memorized during pretraining. A retrieval component first decomposes the user query and searches a corpus—via sparse, dense, or hybrid methods—to surface the most relevant passages. Those passages are then packaged, often alongside metadata like source citations, and injected into the model prompt or context window prior to decoding. The approach reduces hallucinations, improves factual accuracy, and enables small models to compete with larger ones when grounded knowledge is more important than general fluency. Successful RAG systems invest in document chunking, embedding quality, reranking, and evaluation pipelines that monitor both retrieval recall and answer precision. Governance teams value RAG because it enables transparent sourcing and can encode organizational policies in the retrieval layer, but they also monitor for drift, stale content, and privacy leaks when sensitive documents are indexed.

## Audience perspectives
- **Exec:** Treat it as a guardrail: the system fetches trusted documents before the model speaks, so answers stay on-policy.
- **Engineer:** Pipeline = query rewriting, retriever (BM25, DPR, hybrid), reranker, and generator with retrieved context appended to the prompt.

## Examples
**Do**
- Version and monitor the document index so you can trace outputs back to specific sources.
- Evaluate retrieval recall and answer accuracy separately to isolate failure modes.

**Don't**
- Assume RAG removes the need for model fine-tuning or ongoing evaluation.
- Index sensitive records without access controls or data minimization.

## Governance
- **NIST RMF tags:** transparency, data_quality, documentation
- **Risk notes:** Requires controls for stale data, source governance, and privacy when indexing internal corpora.

## Relationships
- **Broader:** grounded generation
- **Narrower:** hybrid RAG, agentic RAG
- **Related:** retrieval, chunking, reranking, hallucination

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'retrieval-augmented generation'.

## Citations
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/retrieval-augmented-generation.yml`_
