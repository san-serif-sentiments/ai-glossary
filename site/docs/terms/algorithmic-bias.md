<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# algorithmic bias

**Aliases:** systemic bias, ai bias
**Categories:** Governance & Risk
**Roles:** Policy & Risk, Legal & Compliance, Product & Program Managers, Communications & Enablement, Security & Trust
**Part of speech:** `concept`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

!!! tip "Put it into practice"
    Run the [Governance Dashboard](../governance-dashboard.md) checklist before launch.

## Role takeaways
- **Policy & Risk:** Map the definition to governance controls and review checklists.
- **Legal & Compliance:** Assess contractual and regulatory obligations tied to this term.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.
- **Security & Trust:** Plan monitoring and abuse prevention scenarios influenced by this term.

## Practice & apply
- Map this term to the governance dashboard and record accountable owners in the backlog.
- Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Systematic unfairness in model outputs that disadvantages certain groups or outcomes.

## Long definition
Algorithmic bias arises when models produce systematically skewed results that disadvantage individuals or groups, often reflecting historical inequities in training data, feature selection, or objective functions. Bias can manifest through discriminatory false positives, unequal error rates, or exclusionary recommendations. Organizations must examine the full pipeline—data collection, labeling, modeling, evaluation, and deployment—to identify root causes. Product teams collaborate with policy, legal, and communications partners to define fairness objectives, while engineers implement bias detection metrics, reweighting, and post-processing adjustments. Governance frameworks (NIST AI RMF, EU AI Act) require documentation of bias assessments, stakeholder engagement, and remediation plans. Failing to manage algorithmic bias can create legal liability, reputational damage, and harm to marginalized communities. Continuous monitoring, diverse evaluation cohorts, and community feedback loops are essential for sustained mitigation.

## Audience perspectives
- **Exec:** Algorithmic bias is when the AI treats groups unfairly, risking customer trust and regulatory violations.
- **Engineer:** Quantify and mitigate disparities across subpopulations using metrics like equalized odds, demographic parity, and subgroup ROC analysis.

## Examples
**Do**
- Include fairness metrics in evaluation pipelines and report them alongside accuracy.
- Engage impacted stakeholders when designing mitigation strategies.

**Don't**
- Launch features without testing performance across sensitive attributes.
- Assume bias is solved after one mitigation; monitor continuously.

## Governance
- **NIST RMF tags:** fairness, transparency
- **Risk notes:** Unmitigated bias can violate anti-discrimination laws, trigger regulatory action, and erode brand trust.

## Relationships
- **Broader:** responsible ai
- **Related:** safety evaluation, red teaming, privacy impact assessment

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'algorithmic bias'.

## Citations
- [Wikipedia – Algorithmic Bias](https://en.wikipedia.org/wiki/Algorithmic_bias)
- [Nature – The Fight Against Algorithmic Bias](https://www.nature.com/articles/d41586-020-03186-4)
- [Brookings – Understanding Algorithmic Bias](https://www.brookings.edu/articles/understanding-algorithmic-bias/)

_License: CC BY-SA 4.0_

_Source file: `data/terms/algorithmic-bias.yml`_
