<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# clip

**Aliases:** contrastive language-image pretraining, clip model
**Categories:** Foundations, Retrieval & RAG
**Roles:** Engineering & Platform, Data Science & Research, Product & Program Managers, Communications & Enablement
**Part of speech:** `noun`
**Status:** <span class="status-chip status-reviewed">Reviewed</span> (Last reviewed: 2025-09-28)

## Role takeaways
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Communications & Enablement:** Align messaging, FAQs, and enablement materials using this definition.

## Practice & apply
- Add this concept to onboarding materials so teammates share a common baseline.
- Link supporting research or documentation in your internal wiki for deeper study.
- Validate retrieval quality using the evaluation guidance referenced in this entry.
- Ensure knowledge sources named here appear in your data governance inventory.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Multimodal model that embeds images and text into a shared space using contrastive learning.

## Long definition
CLIP (Contrastive Language-Image Pretraining) jointly trains an image encoder and a text encoder so that semantically related images and captions map to nearby vectors. The approach uses large-scale image-text pairs scraped from the web and optimizes a contrastive loss that pushes matching pairs together while separating non-matching ones. Once trained, CLIP can perform zero-shot classification, retrieval, and multimodal search by comparing similarity between embeddings. Product teams leverage CLIP to improve content moderation, recommendation, and creative tooling without task-specific labels. Engineers integrate CLIP embeddings into vector stores or downstream fine-tuning pipelines, paying attention to bias, licensing, and safety constraints inherited from web-scale training data. Communications and policy teams monitor CLIP use because it can expose cultural biases and sensitive associations unless mitigated through filtering and evaluation.

## Audience perspectives
- **Exec:** CLIP understands images and text together, letting you search or classify visuals using natural language.
- **Engineer:** Encode images and text with separate transformers trained via contrastive loss; use cosine similarity for retrieval, zero-shot classification, or RAG pipelines.

## Examples
**Do**
- Audit embeddings for demographic bias before deploying search or moderation features.
- Cache CLIP embeddings and align them with domain-specific prompts to improve precision.

**Don't**
- Assume CLIP is license-clean; review dataset provenance and usage restrictions.
- Ignore safety filters when exposing CLIP-powered features to end users.

## Governance
- **NIST RMF tags:** data_quality, transparency
- **Risk notes:** CLIP inherits web-scale bias and copyright concerns; record how outputs are filtered and evaluated.

## Relationships
- **Broader:** embedding
- **Related:** retrieval, synthetic data, guardrails

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'clip'.

## Citations
- [Hugging Face Glossary](https://huggingface.co/docs/transformers/en/glossary)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/clip.yml`_
