<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# confusion matrix

**Aliases:** contingency table, error matrix
**Categories:** Foundations
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers, Policy & Risk
**Part of speech:** `noun`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.
- **Policy & Risk:** Map the definition to governance controls and review checklists.

## Practice & apply
- Add this concept to onboarding materials so teammates share a common baseline.
- Link supporting research or documentation in your internal wiki for deeper study.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Table that summarizes true/false positives and negatives to diagnose classification performance.

## Long definition
A confusion matrix breaks down classification results into true positives, true negatives, false positives, and false negatives. Visualizing performance in this way helps teams understand error patterns, detect class imbalance issues, and compute derived metrics such as precision, recall, specificity, and accuracy. Product managers use confusion matrices to explain trade-offs to stakeholders, while engineers leverage them to tune thresholds, resampling strategies, or loss functions. Policy and compliance reviewers rely on confusion matrix evidence to confirm that sensitive cohorts are not disproportionately affected. Confusion matrices are most informative when compared across slices—by time, geography, or demographic attributes—to catch drift and fairness concerns.

## Audience perspectives
- **Exec:** A confusion matrix shows where the AI is right or wrong so you can see which mistakes matter most.
- **Engineer:** Tabulate TP, TN, FP, FN counts; analyze row/column distributions and normalize per class to diagnose bias and guide mitigation.

## Examples
**Do**
- Normalize confusion matrices to compare error rates across classes of different sizes.
- Track confusion matrices over time to spot drift and guardrail regressions.

**Don't**
- Rely on aggregate accuracy without inspecting confusion patterns.
- Hide confusion matrices from stakeholders—they clarify trade-offs.

## Governance
- **NIST RMF tags:** transparency, validity
- **Risk notes:** Ignoring confusion matrix signals can allow harmful error patterns to persist unnoticed.

## Relationships
- **Broader:** evaluation
- **Related:** precision, recall, roc auc

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'confusion matrix'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/confusion-matrix.yml`_
