<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# token

**Aliases:** subword token, tokenized unit
**Categories:** LLM Core
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `noun`
**Status:** <span class="status-chip status-draft">Draft</span> (Last reviewed: 2024-11-01)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.
- Share findings with enablement so downstream teams understand model implications.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Smallest unit of text a model processes after tokenization, such as a word fragment or character.

## Long definition
In language models a token is the discrete unit produced by a tokenizer before feeding data into the network. Depending on the tokenizer, a token might represent a whole word, a meaningful subword chunk, or even individual characters and punctuation. Models operate on token sequences rather than raw text, which makes token boundaries central to context window sizing, cost estimation, and latency planning. Tokens also drive how prompts and completions are billed in commercial APIs, where limits are expressed in tokens instead of characters. Engineers track token counts to avoid truncating prompts or spilling past context limits, while product teams translate token budgets into supported use cases and pricing. Understanding tokenization quirks helps explain why uncommon spellings, multilingual inputs, or code snippets can explode in length compared with natural language, affecting both accuracy and economics. Governance stakeholders rely on the deterministic nature of tokenization when auditing prompts and ensuring reproducibility across deployments.

## Audience perspectives
- **Exec:** Think of tokens as the LEGO bricks that determine how long prompts can be and what they cost.
- **Engineer:** Tokenizer output units that set sequence length, influence embedding lookup, and bound context windows for inference/training.

## Examples
**Do**
- Estimate prompt costs by counting tokens instead of characters before rolling out a pricing plan.

**Don't**
- Assume character length equals token length when enforcing guardrails or cost controls.

## Governance
- **NIST RMF tags:** transparency, validity
- **Risk notes:** Incorrect token accounting can break context constraints, leading to truncated outputs and compliance failures.

## Relationships
- **Broader:** tokenization
- **Related:** context window, embedding, prompt engineering

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'token'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/python/latest/)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/token.yml`_
