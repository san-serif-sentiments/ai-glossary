<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# cross-validation

**Aliases:** k-fold validation, cv
**Categories:** Foundations
**Roles:** Data Science & Research, Engineering & Platform, Product & Program Managers
**Part of speech:** `process`
**Status:** <span class="status-chip status-approved">Approved</span> (Last reviewed: 2025-09-29)

## Role takeaways
- **Data Science & Research:** Incorporate the metric or method into evaluation pipelines.
- **Engineering & Platform:** Document implementation requirements and operational caveats.
- **Product & Program Managers:** Translate this concept into user impact and rollout plans.

## Practice & apply
- Add this concept to onboarding materials so teammates share a common baseline.
- Link supporting research or documentation in your internal wiki for deeper study.
- Share takeaways with the accountable roles listed above so actions land with the right owners.

## Short definition
Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.

## Long definition
Cross-validation repeatedly partitions a labeled dataset into complementary subsets to assess how a model generalizes. In k-fold cross-validation, the dataset is divided into k folds; the model trains on k-1 folds and validates on the remaining fold, cycling until every fold has served as the validation set. Aggregating results reduces variance compared with a single train-test split and surfaces instability caused by small datasets or class imbalance. Variants such as stratified, time-series, and leave-one-out cross-validation address specific domains. Product managers rely on cross-validation when comparing model candidates, while engineers use fold-level diagnostics to catch overfitting and data leakage before production. Governance teams view cross-validation artifacts as evidence that evaluation processes are robust and reproducible, particularly for regulated scenarios where a single split could mask risk.

## Audience perspectives
- **Exec:** Cross-validation is the rehearsal that shows how a model behaves across different slices before real customers see it.
- **Engineer:** Partition data into k folds, train/validate across permutations, and average metrics to estimate generalization; track per-fold variance for risk analysis.

## Examples
**Do**
- Use stratified folds when label imbalance could skew results.
- Store per-fold metrics and random seeds for reproducibility.

**Don't**
- Leak validation data by reusing preprocessing steps fitted on the full dataset.
- Rely on a single train/test split for high-stakes decisions.

## Governance
- **NIST RMF tags:** validity, transparency
- **Risk notes:** Skipping cross-validation invites optimistic bias and limits evidence required for audits or legal reviews.

## Relationships
- **Broader:** evaluation
- **Related:** overfitting, bias-variance tradeoff, model drift

!!! info "Something missing?"
    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention 'cross-validation'.

## Citations
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)
- [Stanford HAI Brief Definitions](https://hai.stanford.edu/news/brief-definitions)

_License: CC BY-SA 4.0_

_Source file: `data/terms/cross-validation.yml`_
