<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# differential privacy

**Aliases:** DP, epsilon-differential privacy
**Categories:** Governance & Risk
**Roles:** Engineering & Platform, Legal & Compliance, Policy & Risk, Security & Trust
**Part of speech:** `concept`
**Status:** `draft` (Last reviewed: 2024-11-02)

## Short definition
Mathematical framework that limits how much any single record influences published data or model outputs.

## Long definition
Differential privacy protects individuals in a dataset by adding calibrated noise to statistics or training procedures so the presence or absence of any one person becomes indistinguishable. The framework is governed by privacy budgets—epsilon and delta—which quantify acceptable leakage. In AI systems, teams apply differential privacy when releasing analytics, training embeddings, or sharing evaluation datasets. Engineers integrate mechanisms like DP-SGD, Laplace noise, or randomized response, tracking accumulated budgets across queries. Legal and policy partners evaluate whether privacy guarantees meet regulatory requirements, especially when models ingest sensitive or regulated data. Security teams monitor for side-channel attacks that could combine multiple noisy outputs to infer personal information. Differential privacy is not a silver bullet; product and research groups balance utility loss against risk mitigation, document assumptions, and communicate residual exposure to stakeholders.

## Audience perspectives
- **Exec:** Differential privacy lets you learn from user data while keeping any single person unidentifiable.
- **Engineer:** Bound information leakage by injecting calibrated noise; manage cumulative epsilon/delta budgets across analytics or training steps.

## Examples
**Do**
- Track privacy budgets in dashboards so analysts know when to stop issuing queries.
- Explain residual risk and utility trade-offs in launch documentation.

**Don't**
- Assume a single noisy release protects against repeated queries without monitoring budget depletion.
- Mix differentially private and non-private data exports without clear labeling.

## Governance
- **NIST RMF tags:** privacy, risk_management
- **Risk notes:** Incorrect epsilon or budget accounting can create a false sense of protection and trigger regulatory exposure.

## Relationships
- **Broader:** privacy
- **Narrower:** differentially private SGD
- **Related:** synthetic data, guardrails, model governance

## Citations
- [NIST AI RMF Glossary](https://airc.nist.gov/glossary/)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)
- [Wikipedia AI Glossary](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence)

_License: CC BY-SA 4.0_

_Source file: `data/terms/differential-privacy.yml`_
