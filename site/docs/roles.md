<!--
  This file is auto-generated by scripts/render_docs.py. Do not edit manually.
-->

# Role Starter Packs

Guidance for common stakeholder groups. Each section links to the most relevant terms based on category and role tags.

## Product & Program Managers
Focus on user outcomes, feature scope, and launch readiness.

### Focus areas
- Governance & Risk (18 terms)
- LLM Core (16 terms)
- Foundations (11 terms)
- Operations & Monitoring (8 terms)
- Retrieval & RAG (7 terms)
- Agents & Tooling (2 terms)

### Recommended terms
- [agentic ai](terms/agentic-ai.md) — Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [alignment](terms/alignment.md) — Making sure AI systems optimize for human values, policies, and intended outcomes.
- [attention](terms/attention.md) — Technique enabling models to weight input tokens differently when producing each output.
- [beam search](terms/beam-search.md) — Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.
- [chunking](terms/chunking.md) — Splitting source documents into manageable pieces before indexing or feeding them to models.
- [clip](terms/clip.md) — Multimodal model that embeds images and text into a shared space using contrastive learning.
- [confusion matrix](terms/confusion-matrix.md) — Table that summarizes true/false positives and negatives to diagnose classification performance.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [context window](terms/context-window.md) — Maximum number of tokens a model can consider at once during prompting or inference.
- [cross-validation](terms/cross-validation.md) — Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.
- [data minimization](terms/data-minimization.md) — Principle of collecting and retaining only the data necessary for a defined purpose.
- [decoding](terms/decoding.md) — Algorithms that turn model probability distributions into output tokens during generation.
- [diffusion model](terms/diffusion-model.md) — Generative model that iteratively denoises random noise to synthesize images, audio, or other data.
- [embedding](terms/embedding.md) — Dense numerical representation that captures semantic meaning of text, images, or other data.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [f1 score](terms/f1-score.md) — Harmonic mean of precision and recall, balancing false positives and false negatives.
- [generative ai](terms/generative-ai.md) — Family of models that produce new content—text, images, code—rather than only making predictions.
- [greedy decoding](terms/greedy-decoding.md) — Strategy that selects the highest-probability token at each step, producing deterministic outputs.
- [guardrails](terms/guardrails.md) — Controls that constrain model behavior to comply with safety, legal, or brand requirements.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [kv cache](terms/kv-cache.md) — Stored attention keys and values reused across decoding steps to speed sequential generation.
- [log probability](terms/log-probability.md) — Logarithm of a token’s probability, used to inspect model confidence and guide decoding tweaks.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model governance](terms/model-governance.md) — Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.
- [overfitting](terms/overfitting.md) — When a model memorizes training data patterns so closely that it performs poorly on new samples.
- [precision](terms/precision.md) — Share of predicted positives that are actually correct for a given classifier.
- [privacy](terms/privacy.md) — Principle of limiting data collection, use, and exposure to protect individuals’ information.
- [privacy impact assessment](terms/privacy-impact-assessment.md) — Structured review that evaluates how a system collects, uses, and safeguards personal data.
- [prompt engineering](terms/prompt-engineering.md) — Crafting and testing prompts to steer model behavior toward desired outcomes.
- [recall](terms/recall.md) — Share of actual positives a model successfully identifies.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [repetition penalty](terms/repetition-penalty.md) — Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.
- [reranking](terms/reranking.md) — Step that refines retrieval results using a more precise but slower scoring model.
- [responsible ai](terms/responsible-ai.md) — Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.
- [retrieval](terms/retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [retrieval-augmented generation](terms/retrieval-augmented-generation.md) — Workflow that grounds a generative model with retrieved context before producing output.
- [roc auc](terms/roc-auc.md) — Metric summarizing binary classifier performance by measuring area under the ROC curve.
- [safety evaluation](terms/safety-evaluation.md) — Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [system prompt](terms/system-prompt.md) — Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.
- [temperature](terms/temperature.md) — Decoding parameter that controls how random or deterministic a model’s outputs are.
- [token](terms/token.md) — Smallest unit of text a model processes after tokenization, such as a word fragment or character.
- [tool use](terms/tool-use.md) — Pattern where a model selects external tools or functions to handle parts of a task.
- [top-k sampling](terms/top-k-sampling.md) — Decoding method that samples from the k most probable next tokens to balance diversity and control.
- [top-p sampling](terms/top-p-sampling.md) — Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.
- [vector store](terms/vector-store.md) — Database optimized to store embeddings and execute similarity search over vectors.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.

## Engineering & Platform
Own model integration, infra, and technical debt.

### Focus areas
- LLM Core (16 terms)
- Foundations (10 terms)
- Governance & Risk (10 terms)
- Operations & Monitoring (10 terms)
- Retrieval & RAG (7 terms)
- Optimization & Efficiency (4 terms)
- Agents & Tooling (2 terms)

### Recommended terms
- [agentic ai](terms/agentic-ai.md) — Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [attention](terms/attention.md) — Technique enabling models to weight input tokens differently when producing each output.
- [beam search](terms/beam-search.md) — Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.
- [bias-variance tradeoff](terms/bias-variance-tradeoff.md) — Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.
- [chunking](terms/chunking.md) — Splitting source documents into manageable pieces before indexing or feeding them to models.
- [clip](terms/clip.md) — Multimodal model that embeds images and text into a shared space using contrastive learning.
- [confusion matrix](terms/confusion-matrix.md) — Table that summarizes true/false positives and negatives to diagnose classification performance.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [context window](terms/context-window.md) — Maximum number of tokens a model can consider at once during prompting or inference.
- [cross-validation](terms/cross-validation.md) — Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.
- [decoding](terms/decoding.md) — Algorithms that turn model probability distributions into output tokens during generation.
- [differential privacy](terms/differential-privacy.md) — Mathematical framework that limits how much any single record influences published data or model outputs.
- [diffusion model](terms/diffusion-model.md) — Generative model that iteratively denoises random noise to synthesize images, audio, or other data.
- [embedding](terms/embedding.md) — Dense numerical representation that captures semantic meaning of text, images, or other data.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [f1 score](terms/f1-score.md) — Harmonic mean of precision and recall, balancing false positives and false negatives.
- [fine-tuning](terms/fine-tuning.md) — Additional training that adapts a pretrained model to a specific task or domain.
- [greedy decoding](terms/greedy-decoding.md) — Strategy that selects the highest-probability token at each step, producing deterministic outputs.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [knowledge distillation](terms/knowledge-distillation.md) — Technique that trains a smaller student model to mimic a larger teacher model’s behavior.
- [kv cache](terms/kv-cache.md) — Stored attention keys and values reused across decoding steps to speed sequential generation.
- [log probability](terms/log-probability.md) — Logarithm of a token’s probability, used to inspect model confidence and guide decoding tweaks.
- [low-rank adaptation](terms/low-rank-adaptation.md) — Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.
- [ml observability](terms/ml-observability.md) — Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.
- [ml ops](terms/ml-ops.md) — Operational discipline that manages ML models from experimentation through deployment and monitoring.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [overfitting](terms/overfitting.md) — When a model memorizes training data patterns so closely that it performs poorly on new samples.
- [precision](terms/precision.md) — Share of predicted positives that are actually correct for a given classifier.
- [prompt engineering](terms/prompt-engineering.md) — Crafting and testing prompts to steer model behavior toward desired outcomes.
- [quantization](terms/quantization.md) — Technique that compresses model weights into lower-precision formats to shrink size and speed inference.
- [recall](terms/recall.md) — Share of actual positives a model successfully identifies.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [repetition penalty](terms/repetition-penalty.md) — Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.
- [reranking](terms/reranking.md) — Step that refines retrieval results using a more precise but slower scoring model.
- [retrieval](terms/retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [retrieval-augmented generation](terms/retrieval-augmented-generation.md) — Workflow that grounds a generative model with retrieved context before producing output.
- [roc auc](terms/roc-auc.md) — Metric summarizing binary classifier performance by measuring area under the ROC curve.
- [safety evaluation](terms/safety-evaluation.md) — Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [system prompt](terms/system-prompt.md) — Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.
- [temperature](terms/temperature.md) — Decoding parameter that controls how random or deterministic a model’s outputs are.
- [token](terms/token.md) — Smallest unit of text a model processes after tokenization, such as a word fragment or character.
- [tool use](terms/tool-use.md) — Pattern where a model selects external tools or functions to handle parts of a task.
- [top-k sampling](terms/top-k-sampling.md) — Decoding method that samples from the k most probable next tokens to balance diversity and control.
- [top-p sampling](terms/top-p-sampling.md) — Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.
- [vector store](terms/vector-store.md) — Database optimized to store embeddings and execute similarity search over vectors.

## Data Science & Research
Drive experimentation, measurement, and model improvement.

### Focus areas
- LLM Core (16 terms)
- Foundations (10 terms)
- Retrieval & RAG (7 terms)
- Optimization & Efficiency (4 terms)
- Governance & Risk (2 terms)
- Operations & Monitoring (1 term)

### Recommended terms
- [attention](terms/attention.md) — Technique enabling models to weight input tokens differently when producing each output.
- [beam search](terms/beam-search.md) — Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.
- [bias-variance tradeoff](terms/bias-variance-tradeoff.md) — Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.
- [chunking](terms/chunking.md) — Splitting source documents into manageable pieces before indexing or feeding them to models.
- [clip](terms/clip.md) — Multimodal model that embeds images and text into a shared space using contrastive learning.
- [confusion matrix](terms/confusion-matrix.md) — Table that summarizes true/false positives and negatives to diagnose classification performance.
- [context window](terms/context-window.md) — Maximum number of tokens a model can consider at once during prompting or inference.
- [cross-validation](terms/cross-validation.md) — Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.
- [decoding](terms/decoding.md) — Algorithms that turn model probability distributions into output tokens during generation.
- [diffusion model](terms/diffusion-model.md) — Generative model that iteratively denoises random noise to synthesize images, audio, or other data.
- [embedding](terms/embedding.md) — Dense numerical representation that captures semantic meaning of text, images, or other data.
- [f1 score](terms/f1-score.md) — Harmonic mean of precision and recall, balancing false positives and false negatives.
- [fine-tuning](terms/fine-tuning.md) — Additional training that adapts a pretrained model to a specific task or domain.
- [greedy decoding](terms/greedy-decoding.md) — Strategy that selects the highest-probability token at each step, producing deterministic outputs.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [knowledge distillation](terms/knowledge-distillation.md) — Technique that trains a smaller student model to mimic a larger teacher model’s behavior.
- [kv cache](terms/kv-cache.md) — Stored attention keys and values reused across decoding steps to speed sequential generation.
- [log probability](terms/log-probability.md) — Logarithm of a token’s probability, used to inspect model confidence and guide decoding tweaks.
- [low-rank adaptation](terms/low-rank-adaptation.md) — Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.
- [overfitting](terms/overfitting.md) — When a model memorizes training data patterns so closely that it performs poorly on new samples.
- [precision](terms/precision.md) — Share of predicted positives that are actually correct for a given classifier.
- [prompt engineering](terms/prompt-engineering.md) — Crafting and testing prompts to steer model behavior toward desired outcomes.
- [quantization](terms/quantization.md) — Technique that compresses model weights into lower-precision formats to shrink size and speed inference.
- [recall](terms/recall.md) — Share of actual positives a model successfully identifies.
- [repetition penalty](terms/repetition-penalty.md) — Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.
- [reranking](terms/reranking.md) — Step that refines retrieval results using a more precise but slower scoring model.
- [retrieval](terms/retrieval.md) — Process of selecting relevant documents or vectors from a corpus in response to a query.
- [retrieval-augmented generation](terms/retrieval-augmented-generation.md) — Workflow that grounds a generative model with retrieved context before producing output.
- [roc auc](terms/roc-auc.md) — Metric summarizing binary classifier performance by measuring area under the ROC curve.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [system prompt](terms/system-prompt.md) — Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.
- [temperature](terms/temperature.md) — Decoding parameter that controls how random or deterministic a model’s outputs are.
- [token](terms/token.md) — Smallest unit of text a model processes after tokenization, such as a word fragment or character.
- [top-k sampling](terms/top-k-sampling.md) — Decoding method that samples from the k most probable next tokens to balance diversity and control.
- [top-p sampling](terms/top-p-sampling.md) — Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.
- [vector store](terms/vector-store.md) — Database optimized to store embeddings and execute similarity search over vectors.

## Policy & Risk
Ensure responsible AI controls align with governance frameworks.

### Focus areas
- Governance & Risk (19 terms)
- Operations & Monitoring (10 terms)
- Foundations (6 terms)
- LLM Core (1 term)

### Recommended terms
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [alignment](terms/alignment.md) — Making sure AI systems optimize for human values, policies, and intended outcomes.
- [bias-variance tradeoff](terms/bias-variance-tradeoff.md) — Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.
- [confusion matrix](terms/confusion-matrix.md) — Table that summarizes true/false positives and negatives to diagnose classification performance.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [data minimization](terms/data-minimization.md) — Principle of collecting and retaining only the data necessary for a defined purpose.
- [differential privacy](terms/differential-privacy.md) — Mathematical framework that limits how much any single record influences published data or model outputs.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [generative ai](terms/generative-ai.md) — Family of models that produce new content—text, images, code—rather than only making predictions.
- [guardrails](terms/guardrails.md) — Controls that constrain model behavior to comply with safety, legal, or brand requirements.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [ml observability](terms/ml-observability.md) — Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.
- [ml ops](terms/ml-ops.md) — Operational discipline that manages ML models from experimentation through deployment and monitoring.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model governance](terms/model-governance.md) — Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.
- [precision](terms/precision.md) — Share of predicted positives that are actually correct for a given classifier.
- [privacy](terms/privacy.md) — Principle of limiting data collection, use, and exposure to protect individuals’ information.
- [privacy impact assessment](terms/privacy-impact-assessment.md) — Structured review that evaluates how a system collects, uses, and safeguards personal data.
- [recall](terms/recall.md) — Share of actual positives a model successfully identifies.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [responsible ai](terms/responsible-ai.md) — Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.
- [safety evaluation](terms/safety-evaluation.md) — Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.

## Legal & Compliance
Evaluate regulatory exposure, contracts, and IP concerns.

### Focus areas
- Governance & Risk (16 terms)
- Operations & Monitoring (5 terms)
- Foundations (1 term)
- LLM Core (1 term)

### Recommended terms
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [alignment](terms/alignment.md) — Making sure AI systems optimize for human values, policies, and intended outcomes.
- [data minimization](terms/data-minimization.md) — Principle of collecting and retaining only the data necessary for a defined purpose.
- [differential privacy](terms/differential-privacy.md) — Mathematical framework that limits how much any single record influences published data or model outputs.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [guardrails](terms/guardrails.md) — Controls that constrain model behavior to comply with safety, legal, or brand requirements.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model governance](terms/model-governance.md) — Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.
- [privacy](terms/privacy.md) — Principle of limiting data collection, use, and exposure to protect individuals’ information.
- [privacy impact assessment](terms/privacy-impact-assessment.md) — Structured review that evaluates how a system collects, uses, and safeguards personal data.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [responsible ai](terms/responsible-ai.md) — Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.

## Security & Trust
Safeguard data, access, and abuse prevention.

### Focus areas
- Governance & Risk (11 terms)
- Operations & Monitoring (8 terms)
- Foundations (1 term)

### Recommended terms
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [data minimization](terms/data-minimization.md) — Principle of collecting and retaining only the data necessary for a defined purpose.
- [differential privacy](terms/differential-privacy.md) — Mathematical framework that limits how much any single record influences published data or model outputs.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [ml observability](terms/ml-observability.md) — Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.
- [ml ops](terms/ml-ops.md) — Operational discipline that manages ML models from experimentation through deployment and monitoring.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [privacy impact assessment](terms/privacy-impact-assessment.md) — Structured review that evaluates how a system collects, uses, and safeguards personal data.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [synthetic data](terms/synthetic-data.md) — Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.

## Communications & Enablement
Craft messaging, disclosure, and stakeholder education.

### Focus areas
- Governance & Risk (15 terms)
- Operations & Monitoring (7 terms)
- Foundations (4 terms)
- LLM Core (2 terms)
- Retrieval & RAG (1 term)

### Recommended terms
- [ai incident response](terms/ai-incident-response.md) — Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.
- [algorithmic bias](terms/algorithmic-bias.md) — Systematic unfairness in model outputs that disadvantages certain groups or outcomes.
- [alignment](terms/alignment.md) — Making sure AI systems optimize for human values, policies, and intended outcomes.
- [clip](terms/clip.md) — Multimodal model that embeds images and text into a shared space using contrastive learning.
- [content moderation](terms/content-moderation.md) — Workflows and tools that review, filter, and act on user-generated content to enforce policy.
- [diffusion model](terms/diffusion-model.md) — Generative model that iteratively denoises random noise to synthesize images, audio, or other data.
- [evaluation](terms/evaluation.md) — Systematic measurement of model performance, safety, and reliability using defined tests.
- [generative ai](terms/generative-ai.md) — Family of models that produce new content—text, images, code—rather than only making predictions.
- [guardrails](terms/guardrails.md) — Controls that constrain model behavior to comply with safety, legal, or brand requirements.
- [hallucination](terms/hallucination.md) — When an AI model presents fabricated or unsupported information as fact.
- [model card](terms/model-card.md) — Standardized documentation describing a model’s purpose, data, performance, and limitations.
- [model drift](terms/model-drift.md) — Gradual mismatch between model assumptions and real-world data that degrades performance over time.
- [model governance](terms/model-governance.md) — Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.
- [privacy](terms/privacy.md) — Principle of limiting data collection, use, and exposure to protect individuals’ information.
- [red teaming](terms/red-teaming.md) — Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.
- [responsible ai](terms/responsible-ai.md) — Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.
- [safety evaluation](terms/safety-evaluation.md) — Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.
- [voice cloning](terms/voice-cloning.md) — Technique that replicates a person’s voice using generative models trained on audio samples.
