{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Glossary","text":"Shared language for AI builders and stewards <p>       Every term in the glossary is citation-backed, audience-aware, and stored as       structured YAML so it can power docs, APIs, and review workflows. Browse by       category, role, or metric to keep product, engineering, and governance teams in sync.     </p> \ud83d\udd0d Explore the interactive search \ud83d\udc65 Role starter packs \ud83e\udded Category explorer"},{"location":"#project-focus","title":"Project focus","text":"<ul> <li>Shared language: harmonizes terminology across technical, product, and   policy stakeholders.</li> <li>Structured delivery: each entry lives in YAML with examples, aliases, and   governance context for downstream reuse.</li> <li>Traceable sources: every definition includes citations, NIST AI RMF tags,   and lifecycle status for auditability.</li> </ul>"},{"location":"#current-features","title":"Current features","text":"<ul> <li>Seed coverage for foundational concepts spanning large language models,   retrieval, MLOps, and governance.</li> <li>Automated validation that enforces schema rules, definition length limits,   and audience-specific explanations before publication.</li> <li>Generated JSON datasets (<code>build/glossary.json</code>, <code>build/search-index.json</code>) and   Markdown documentation (<code>site/docs/terms/</code>) derived from the same YAML source.</li> <li>Categorized navigation so visitors can browse by LLM internals, retrieval,   optimization, operations, or governance topics.</li> <li>Role starter packs and a guided search experience so product, engineering,   policy, legal, security, and communications teams can find relevant terms fast.</li> </ul>"},{"location":"#popular-categories","title":"Popular categoriesLLM CoreRetrieval &amp; RAGGovernance &amp; RiskOptimization &amp; Efficiency","text":"<p>Attention, decoding, prompting, and the building blocks behind language models.</p> Browse terms \u2192 <p>Grounding models with hybrid search, chunking, reranking, and retrieval pipelines.</p> Browse terms \u2192 <p>Responsible AI practices, documentation, privacy, and safety mitigation.</p> Browse terms \u2192 <p>Quantization, LoRA, distillation, and performance tuning for deployment.</p> Browse terms \u2192"},{"location":"#roadmap-highlights","title":"Roadmap highlights","text":"<ul> <li>Grow the corpus to 50+ prioritized LLM, infrastructure, and risk terms.</li> <li>Publish the static site for public access and contributor discovery.</li> <li>Expose the lightweight API to support integrations and chat assistants.</li> </ul> <p>Contributors can review the Contribution guide to learn how to add or refine glossary entries.</p>"},{"location":"categories/","title":"Category Explorer","text":""},{"location":"categories/#category-explorer","title":"Category Explorer","text":"<p>Browse terms grouped by focus area. Categories align with navigation and search filters.</p>"},{"location":"categories/#foundations","title":"Foundations","text":"<p>Core machine learning concepts that underpin modern AI systems.</p> <ul> <li>bias-variance tradeoff \u2014 Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk</li> <li>clip \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Communications &amp; Enablement</li> <li>confusion matrix \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk</li> <li>cross-validation \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>diffusion model \u2014 Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Communications &amp; Enablement</li> <li>f1 score \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>generative ai \u2014 Communications &amp; Enablement, Policy &amp; Risk, Product &amp; Program Managers</li> <li>overfitting \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>precision \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk</li> <li>recall \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk</li> <li>roc auc \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>voice cloning \u2014 Product &amp; Program Managers, Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust</li> </ul>"},{"location":"categories/#llm-core","title":"LLM Core","text":"<p>Mechanics of transformers, prompting, decoding, and language model internals.</p> <ul> <li>attention \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>beam search \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>context window \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>decoding \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>greedy decoding \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>hallucination \u2014 Communications &amp; Enablement, Data Science &amp; Research, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>kv cache \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>log probability \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>prompt engineering \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>repetition penalty \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>system prompt \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>temperature \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>token \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>top-k sampling \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>top-p sampling \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> </ul>"},{"location":"categories/#retrieval-rag","title":"Retrieval &amp; RAG","text":"<p>Tools for grounding models with external knowledge and search infrastructure.</p> <ul> <li>chunking \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>embedding \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>reranking \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>retrieval \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>retrieval-augmented generation \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>vector store \u2014 Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers</li> </ul>"},{"location":"categories/#agents-tooling","title":"Agents &amp; Tooling","text":"<p>Agent patterns, tool invocation, and orchestration strategies.</p> <ul> <li>agentic ai \u2014 Engineering &amp; Platform, Product &amp; Program Managers</li> <li>tool use \u2014 Engineering &amp; Platform, Product &amp; Program Managers</li> </ul>"},{"location":"categories/#optimization-efficiency","title":"Optimization &amp; Efficiency","text":"<p>Techniques for scaling inference and training effectively.</p> <ul> <li>fine-tuning \u2014 Data Science &amp; Research, Engineering &amp; Platform</li> <li>knowledge distillation \u2014 Data Science &amp; Research, Engineering &amp; Platform</li> <li>low-rank adaptation \u2014 Data Science &amp; Research, Engineering &amp; Platform</li> <li>quantization \u2014 Data Science &amp; Research, Engineering &amp; Platform</li> </ul>"},{"location":"categories/#operations-monitoring","title":"Operations &amp; Monitoring","text":"<p>Operational playbooks for running AI in production.</p> <ul> <li>ai incident response \u2014 Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>evaluation \u2014 Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>ml observability \u2014 Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust</li> <li>ml ops \u2014 Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust</li> <li>model drift \u2014 Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>synthetic data evaluation \u2014 Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers</li> </ul>"},{"location":"categories/#governance-risk","title":"Governance &amp; Risk","text":"<p>Policies, controls, and assessments that ensure responsible AI.</p> <ul> <li>algorithmic bias \u2014 Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Communications &amp; Enablement, Security &amp; Trust</li> <li>alignment \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>content moderation \u2014 Policy &amp; Risk, Communications &amp; Enablement, Product &amp; Program Managers, Security &amp; Trust, Engineering &amp; Platform</li> <li>data minimization \u2014 Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>data retention \u2014 Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust, Product &amp; Program Managers</li> <li>differential privacy \u2014 Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust</li> <li>fairness metrics \u2014 Policy &amp; Risk, Legal &amp; Compliance, Engineering &amp; Platform, Product &amp; Program Managers</li> <li>guardrails \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>model card \u2014 Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Engineering &amp; Platform, Communications &amp; Enablement</li> <li>model governance \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>model interpretability \u2014 Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers</li> <li>privacy \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>privacy impact assessment \u2014 Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>red teaming \u2014 Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> <li>responsible ai \u2014 Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers</li> <li>safety evaluation \u2014 Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers, Communications &amp; Enablement</li> <li>synthetic data \u2014 Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for helping expand the glossary. Follow these steps to get started:</p> <ol> <li>Fork the repository and create a feature branch.</li> <li>Install dependencies with <code>pip install -r requirements.txt</code>.</li> <li>Create or update a YAML file under <code>data/terms/</code>, covering categories, roles, citations, and examples.</li> <li>Run <code>make validate</code> to ensure schema and lint checks pass.</li> <li>Run <code>make build</code> followed by <code>cd site &amp;&amp; mkdocs build --strict</code> to confirm the docs compile cleanly.</li> <li>Open a pull request using the provided template.</li> </ol> <p>See the root-level <code>CONTRIBUTING.md</code> for detailed guidance, including formatting rules, review checklist, and citation requirements.</p>"},{"location":"data-model/","title":"Data Model","text":"<p>All glossary entries follow the schema defined in <code>schema/term.schema.json</code>. Each YAML file under <code>data/terms/</code> captures a single concept and includes metadata for cross-team alignment.</p> <pre><code>term: \"example\"\naliases:\n  - \"synonym\"\ncategories:\n  - \"Category name\"\nroles:\n  - \"product\"\npart_of_speech: \"noun\"\nshort_def: \"\u226440 word summary.\"\nlong_def: &gt;-\n  120\u2013200 word explanation that provides history, context, and operational\n  considerations.\naudiences:\n  exec: \"Business-ready framing.\"\n  engineer: \"Technical framing.\"\nexamples:\n  do:\n    - \"What good practice looks like.\"\n  dont:\n    - \"What to avoid.\"\ngovernance:\n  nist_rmf_tags:\n    - \"tag\"\n  risk_notes: \"Key compliance considerations.\"\nrelationships:\n  broader:\n    - \"parent term\"\n  narrower:\n    - \"child term\"\n  related:\n    - \"peer term\"\ncitations:\n  - source: \"Credible reference\"\n    url: \"https://example.com\"\nlicense: \"CC BY-SA 4.0\"\nstatus: \"draft\"\nlast_reviewed: \"2024-01-01\"\n</code></pre>"},{"location":"data-model/#validation-rules","title":"Validation rules","text":"<ul> <li><code>short_def</code> must be 40 words or fewer.</li> <li><code>long_def</code> must fall between 80 and 220 words.</li> <li>Executive and engineering variants are mandatory.</li> <li>Provide at least one example in each of the <code>do</code> and <code>dont</code> lists.</li> <li>Assign at least one category so entries can be grouped in navigation and search.</li> <li>Tag entries with at least one role (<code>product</code>, <code>engineering</code>, <code>data_science</code>, <code>policy</code>,   <code>legal</code>, <code>security</code>, or <code>communications</code>) for role-based guidance.</li> <li>Cite sources with URLs to reputable glossaries, standards, or academic   references.</li> </ul> <p>Run <code>make validate</code> locally to confirm entries match both the JSON Schema and custom rules.</p>"},{"location":"governance-dashboard/","title":"Governance Dashboard","text":"<p>Updated: 2024-11-04</p> <p>This page consolidates governance-oriented glossary content so compliance, policy, legal, and risk teams can quickly assess coverage, discover gaps, and coordinate actions.</p>"},{"location":"governance-dashboard/#nist-rmf-coverage-by-tag","title":"NIST RMF coverage by tag","text":"NIST tag Representative terms Action Accuracy hallucination, precision Ensure evaluation suites report precision/recall alongside hallucination incidents. Fairness algorithmic bias, fairness metrics Align fairness thresholds with policy requirements and document in model cards. Transparency model interpretability, model card Capture explanation tooling and documentation updates before launch. Privacy data minimization, privacy impact assessment, data retention Validate retention schedules and PIA sign-offs for new features. Risk Management content moderation, voice cloning Review incident response playbooks and escalation paths for emerging risks. Documentation model card, governance Keep model cards and governance logs up to date with each release."},{"location":"governance-dashboard/#quick-tasks-for-governance-partners","title":"Quick tasks for governance partners","text":"<ol> <li>Review the Role Starter Pack for Policy &amp; Risk and ensure open tasks are assigned.</li> <li>For each upcoming launch, record fairness, privacy, and interpretability decisions in the appropriate glossary term relationships (e.g., link mitigation plans under \u201crelated\u201d).</li> <li>Schedule quarterly audits of the Category Explorer to confirm coverage is current and identify new terms to add.</li> <li>Use the interactive search filtered by <code>status=draft</code> or <code>role=policy</code> to prioritize review queues.</li> </ol>"},{"location":"governance-dashboard/#useful-resources","title":"Useful resources","text":"<ul> <li>Prompt Engineering Playbook</li> <li>Category Explorer \u2013 Governance &amp; Risk</li> <li>Role Starter Packs \u2013 Legal &amp; Compliance</li> </ul>"},{"location":"prompting/","title":"Prompt Engineering Playbook","text":"<p>Last updated: 2024-11-04</p> <p>The glossary captures a full stack of prompt-related terminology. Use this page as a jumping-off point for crafting reliable instructions and understanding the knobs that affect behavior.</p> <p>Quick links</p> <ul> <li>Role starter pack: Product &amp; Program Managers</li> <li>Role starter pack: Engineering &amp; Platform</li> <li>Search prompts &amp; decoding terms</li> <li>Search governance touchpoints</li> </ul>"},{"location":"prompting/#core-concepts","title":"Core concepts","text":"<ul> <li>Prompt engineering \u2014 overall workflow for iterating and testing prompts.</li> <li>System prompt \u2014 immutable guardrail at the top of every conversation.</li> <li>Context window \u2014 token budget that constrains prompt size and retrieved context.</li> <li>Token \u2014 smallest unit models consume, essential for cost and context planning.</li> </ul>"},{"location":"prompting/#controlling-outputs","title":"Controlling outputs","text":"<ul> <li>Temperature \u2014 adjusts randomness in sampling; lower for consistency, higher for ideation.</li> <li>Top-k sampling \u2014 limits sampling to the top k candidates per step.</li> <li>Top-p sampling \u2014 chooses from the smallest probability mass that sums to p.</li> <li>Repetition penalty \u2014 discourages loops or repeated phrases.</li> <li>Beam search \u2014 deterministic multi-path decoding for structured responses.</li> </ul>"},{"location":"prompting/#grounding-and-retrieval-aids","title":"Grounding and retrieval aids","text":"<ul> <li>Retrieval-augmented generation \u2014 combine prompts with retrieved context.</li> <li>Chunking and vector stores \u2014 structure knowledge bases for precise context windows.</li> <li>Reranking \u2014 surface the best supporting passages before they enter the prompt.</li> </ul>"},{"location":"prompting/#safety-and-governance-touchpoints","title":"Safety and governance touchpoints","text":"<ul> <li>Guardrails \u2014 policy-aligned controls before or after generation.</li> <li>Safety evaluation \u2014 ensure prompt changes don\u2019t undo previous approvals.</li> <li>Model card &amp; content moderation \u2014 document and monitor prompt behavior in production.</li> </ul>"},{"location":"prompting/#quick-checklist-before-launch","title":"Quick checklist before launch","text":"<ol> <li>Version system prompts in source control and log every change.</li> <li>Benchmark prompts across precision, recall, hallucination, and fairness metrics.</li> <li>Validate context window usage with representative journeys (long, multilingual, regulated).</li> <li>Document decoding settings (temperature, top-k/top-p, repetition penalties) in model cards.</li> <li>Run red-teaming and safety evaluations when prompts or grounding data change.</li> </ol> <p>For deeper exploration, use the interactive search with the <code>LLM Core</code> category or filter by your team\u2019s role.</p>"},{"location":"roles/","title":"Role Starter Packs","text":""},{"location":"roles/#role-starter-packs","title":"Role Starter Packs","text":"<p>Guidance for common stakeholder groups. Each pack includes actionable steps and key focus areas so teams can operationalize insights immediately.</p>"},{"location":"roles/#product-program-managers","title":"Product &amp; Program Managers","text":"<p>Focus on user outcomes, feature scope, and launch readiness.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#focus-areas","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (22 terms)</li> <li>LLM Core (16 terms)</li> <li>Foundations (11 terms)</li> <li>Operations &amp; Monitoring (11 terms)</li> <li>Retrieval &amp; RAG (7 terms)</li> <li>Agents &amp; Tooling (2 terms)</li> </ul>"},{"location":"roles/#recommended-terms","title":"Recommended terms","text":"<ul> <li>agentic ai \u2014 Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.</li> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>attention \u2014 Technique enabling models to weight input tokens differently when producing each output.</li> <li>beam search \u2014 Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</li> <li>chunking \u2014 Splitting source documents into manageable pieces before indexing or feeding them to models.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>context window \u2014 Maximum number of tokens a model can consider at once during prompting or inference.</li> <li>cross-validation \u2014 Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>decoding \u2014 Algorithms that turn model probability distributions into output tokens during generation.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>embedding \u2014 Dense numerical representation that captures semantic meaning of text, images, or other data.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>f1 score \u2014 Harmonic mean of precision and recall, balancing false positives and false negatives.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>generative ai \u2014 Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</li> <li>greedy decoding \u2014 Strategy that selects the highest-probability token at each step, producing deterministic outputs.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>kv cache \u2014 Stored attention keys and values reused across decoding steps to speed sequential generation.</li> <li>log probability \u2014 Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>overfitting \u2014 When a model memorizes training data patterns so closely that it performs poorly on new samples.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>prompt engineering \u2014 Crafting and testing prompts to steer model behavior toward desired outcomes.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>repetition penalty \u2014 Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</li> <li>reranking \u2014 Step that refines retrieval results using a more precise but slower scoring model.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>retrieval-augmented generation \u2014 Workflow that grounds a generative model with retrieved context before producing output.</li> <li>roc auc \u2014 Metric summarizing binary classifier performance by measuring area under the ROC curve.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>system prompt \u2014 Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</li> <li>temperature \u2014 Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</li> <li>token \u2014 Smallest unit of text a model processes after tokenization, such as a word fragment or character.</li> <li>tool use \u2014 Pattern where a model selects external tools or functions to handle parts of a task.</li> <li>top-k sampling \u2014 Decoding method that samples from the k most probable next tokens to balance diversity and control.</li> <li>top-p sampling \u2014 Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</li> <li>vector store \u2014 Database optimized to store embeddings and execute similarity search over vectors.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"roles/#engineering-platform","title":"Engineering &amp; Platform","text":"<p>Own model integration, infra, and technical debt.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#focus-areas_1","title":"Focus areas","text":"<ul> <li>LLM Core (16 terms)</li> <li>Governance &amp; Risk (13 terms)</li> <li>Operations &amp; Monitoring (13 terms)</li> <li>Foundations (10 terms)</li> <li>Retrieval &amp; RAG (7 terms)</li> <li>Optimization &amp; Efficiency (4 terms)</li> <li>Agents &amp; Tooling (2 terms)</li> </ul>"},{"location":"roles/#recommended-terms_1","title":"Recommended terms","text":"<ul> <li>agentic ai \u2014 Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.</li> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>attention \u2014 Technique enabling models to weight input tokens differently when producing each output.</li> <li>beam search \u2014 Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</li> <li>bias-variance tradeoff \u2014 Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</li> <li>chunking \u2014 Splitting source documents into manageable pieces before indexing or feeding them to models.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>context window \u2014 Maximum number of tokens a model can consider at once during prompting or inference.</li> <li>cross-validation \u2014 Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</li> <li>decoding \u2014 Algorithms that turn model probability distributions into output tokens during generation.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>embedding \u2014 Dense numerical representation that captures semantic meaning of text, images, or other data.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>f1 score \u2014 Harmonic mean of precision and recall, balancing false positives and false negatives.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>fine-tuning \u2014 Additional training that adapts a pretrained model to a specific task or domain.</li> <li>greedy decoding \u2014 Strategy that selects the highest-probability token at each step, producing deterministic outputs.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>knowledge distillation \u2014 Technique that trains a smaller student model to mimic a larger teacher model\u2019s behavior.</li> <li>kv cache \u2014 Stored attention keys and values reused across decoding steps to speed sequential generation.</li> <li>log probability \u2014 Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</li> <li>low-rank adaptation \u2014 Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.</li> <li>ml observability \u2014 Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</li> <li>ml ops \u2014 Operational discipline that manages ML models from experimentation through deployment and monitoring.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>overfitting \u2014 When a model memorizes training data patterns so closely that it performs poorly on new samples.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>prompt engineering \u2014 Crafting and testing prompts to steer model behavior toward desired outcomes.</li> <li>quantization \u2014 Technique that compresses model weights into lower-precision formats to shrink size and speed inference.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>repetition penalty \u2014 Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</li> <li>reranking \u2014 Step that refines retrieval results using a more precise but slower scoring model.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>retrieval-augmented generation \u2014 Workflow that grounds a generative model with retrieved context before producing output.</li> <li>roc auc \u2014 Metric summarizing binary classifier performance by measuring area under the ROC curve.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>system prompt \u2014 Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</li> <li>temperature \u2014 Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</li> <li>token \u2014 Smallest unit of text a model processes after tokenization, such as a word fragment or character.</li> <li>tool use \u2014 Pattern where a model selects external tools or functions to handle parts of a task.</li> <li>top-k sampling \u2014 Decoding method that samples from the k most probable next tokens to balance diversity and control.</li> <li>top-p sampling \u2014 Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</li> <li>vector store \u2014 Database optimized to store embeddings and execute similarity search over vectors.</li> </ul>"},{"location":"roles/#data-science-research","title":"Data Science &amp; Research","text":"<p>Drive experimentation, measurement, and model improvement.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#focus-areas_2","title":"Focus areas","text":"<ul> <li>LLM Core (16 terms)</li> <li>Foundations (10 terms)</li> <li>Retrieval &amp; RAG (7 terms)</li> <li>Optimization &amp; Efficiency (4 terms)</li> <li>Governance &amp; Risk (3 terms)</li> <li>Operations &amp; Monitoring (2 terms)</li> </ul>"},{"location":"roles/#recommended-terms_2","title":"Recommended terms","text":"<ul> <li>attention \u2014 Technique enabling models to weight input tokens differently when producing each output.</li> <li>beam search \u2014 Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</li> <li>bias-variance tradeoff \u2014 Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</li> <li>chunking \u2014 Splitting source documents into manageable pieces before indexing or feeding them to models.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>context window \u2014 Maximum number of tokens a model can consider at once during prompting or inference.</li> <li>cross-validation \u2014 Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</li> <li>decoding \u2014 Algorithms that turn model probability distributions into output tokens during generation.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>embedding \u2014 Dense numerical representation that captures semantic meaning of text, images, or other data.</li> <li>f1 score \u2014 Harmonic mean of precision and recall, balancing false positives and false negatives.</li> <li>fine-tuning \u2014 Additional training that adapts a pretrained model to a specific task or domain.</li> <li>greedy decoding \u2014 Strategy that selects the highest-probability token at each step, producing deterministic outputs.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>knowledge distillation \u2014 Technique that trains a smaller student model to mimic a larger teacher model\u2019s behavior.</li> <li>kv cache \u2014 Stored attention keys and values reused across decoding steps to speed sequential generation.</li> <li>log probability \u2014 Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</li> <li>low-rank adaptation \u2014 Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.</li> <li>overfitting \u2014 When a model memorizes training data patterns so closely that it performs poorly on new samples.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>prompt engineering \u2014 Crafting and testing prompts to steer model behavior toward desired outcomes.</li> <li>quantization \u2014 Technique that compresses model weights into lower-precision formats to shrink size and speed inference.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>repetition penalty \u2014 Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</li> <li>reranking \u2014 Step that refines retrieval results using a more precise but slower scoring model.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>retrieval-augmented generation \u2014 Workflow that grounds a generative model with retrieved context before producing output.</li> <li>roc auc \u2014 Metric summarizing binary classifier performance by measuring area under the ROC curve.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>system prompt \u2014 Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</li> <li>temperature \u2014 Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</li> <li>token \u2014 Smallest unit of text a model processes after tokenization, such as a word fragment or character.</li> <li>top-k sampling \u2014 Decoding method that samples from the k most probable next tokens to balance diversity and control.</li> <li>top-p sampling \u2014 Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</li> <li>vector store \u2014 Database optimized to store embeddings and execute similarity search over vectors.</li> </ul>"},{"location":"roles/#policy-risk","title":"Policy &amp; Risk","text":"<p>Ensure responsible AI controls align with governance frameworks.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#focus-areas_3","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (23 terms)</li> <li>Operations &amp; Monitoring (13 terms)</li> <li>Foundations (6 terms)</li> <li>LLM Core (1 term)</li> </ul>"},{"location":"roles/#recommended-terms_3","title":"Recommended terms","text":"<ul> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>bias-variance tradeoff \u2014 Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>generative ai \u2014 Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>ml observability \u2014 Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</li> <li>ml ops \u2014 Operational discipline that manages ML models from experimentation through deployment and monitoring.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"roles/#legal-compliance","title":"Legal &amp; Compliance","text":"<p>Evaluate regulatory exposure, contracts, and IP concerns.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#focus-areas_4","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (19 terms)</li> <li>Operations &amp; Monitoring (7 terms)</li> <li>Foundations (1 term)</li> <li>LLM Core (1 term)</li> </ul>"},{"location":"roles/#recommended-terms_4","title":"Recommended terms","text":"<ul> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"roles/#security-trust","title":"Security &amp; Trust","text":"<p>Safeguard data, access, and abuse prevention.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#focus-areas_5","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (12 terms)</li> <li>Operations &amp; Monitoring (8 terms)</li> <li>Foundations (1 term)</li> </ul>"},{"location":"roles/#recommended-terms_5","title":"Recommended terms","text":"<ul> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>ml observability \u2014 Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</li> <li>ml ops \u2014 Operational discipline that manages ML models from experimentation through deployment and monitoring.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"roles/#communications-enablement","title":"Communications &amp; Enablement","text":"<p>Craft messaging, disclosure, and stakeholder education.</p> <p>Action plan - Bookmark the Glossary Search filtered to this role and review the top 5 unfamiliar terms. - Schedule a sync with partner roles listed under each term to clarify ownership and open questions. - Capture insights in your runbook or onboarding guide so future teammates ramp faster.</p>"},{"location":"roles/#focus-areas_6","title":"Focus areas","text":"<ul> <li>Governance &amp; Risk (15 terms)</li> <li>Operations &amp; Monitoring (7 terms)</li> <li>Foundations (4 terms)</li> <li>LLM Core (2 terms)</li> <li>Retrieval &amp; RAG (1 term)</li> </ul>"},{"location":"roles/#recommended-terms_6","title":"Recommended terms","text":"<ul> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>generative ai \u2014 Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"search/","title":"Glossary Search","text":"<p>Use the interactive search below to scan terms, aliases, categories, and statuses. Results update as you type.</p> Quick filters Product Engineering Policy Governance &amp; Risk LLM Core Search Category All categories Status All statuses Role All roles"},{"location":"terms/","title":"Overview","text":""},{"location":"terms/#glossary-terms","title":"Glossary Terms","text":"<p>Total entries: 62</p> <ul> <li>agentic ai \u2014 Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.</li> <li>ai incident response \u2014 Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</li> <li>algorithmic bias \u2014 Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</li> <li>alignment \u2014 Making sure AI systems optimize for human values, policies, and intended outcomes.</li> <li>attention \u2014 Technique enabling models to weight input tokens differently when producing each output.</li> <li>beam search \u2014 Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</li> <li>bias-variance tradeoff \u2014 Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</li> <li>chunking \u2014 Splitting source documents into manageable pieces before indexing or feeding them to models.</li> <li>clip \u2014 Multimodal model that embeds images and text into a shared space using contrastive learning.</li> <li>confusion matrix \u2014 Table that summarizes true/false positives and negatives to diagnose classification performance.</li> <li>content moderation \u2014 Workflows and tools that review, filter, and act on user-generated content to enforce policy.</li> <li>context window \u2014 Maximum number of tokens a model can consider at once during prompting or inference.</li> <li>cross-validation \u2014 Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</li> <li>data minimization \u2014 Principle of collecting and retaining only the data necessary for a defined purpose.</li> <li>data retention \u2014 Policies defining how long data is stored, where it lives, and how it is deleted.</li> <li>decoding \u2014 Algorithms that turn model probability distributions into output tokens during generation.</li> <li>differential privacy \u2014 Mathematical framework that limits how much any single record influences published data or model outputs.</li> <li>diffusion model \u2014 Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</li> <li>embedding \u2014 Dense numerical representation that captures semantic meaning of text, images, or other data.</li> <li>evaluation \u2014 Systematic measurement of model performance, safety, and reliability using defined tests.</li> <li>f1 score \u2014 Harmonic mean of precision and recall, balancing false positives and false negatives.</li> <li>fairness metrics \u2014 Quantitative measures that evaluate whether model performance is equitable across groups.</li> <li>fine-tuning \u2014 Additional training that adapts a pretrained model to a specific task or domain.</li> <li>generative ai \u2014 Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</li> <li>greedy decoding \u2014 Strategy that selects the highest-probability token at each step, producing deterministic outputs.</li> <li>guardrails \u2014 Controls that constrain model behavior to comply with safety, legal, or brand requirements.</li> <li>hallucination \u2014 When an AI model presents fabricated or unsupported information as fact.</li> <li>knowledge distillation \u2014 Technique that trains a smaller student model to mimic a larger teacher model\u2019s behavior.</li> <li>kv cache \u2014 Stored attention keys and values reused across decoding steps to speed sequential generation.</li> <li>log probability \u2014 Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</li> <li>low-rank adaptation \u2014 Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.</li> <li>ml observability \u2014 Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</li> <li>ml ops \u2014 Operational discipline that manages ML models from experimentation through deployment and monitoring.</li> <li>model card \u2014 Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</li> <li>model drift \u2014 Gradual mismatch between model assumptions and real-world data that degrades performance over time.</li> <li>model governance \u2014 Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</li> <li>model interpretability \u2014 Ability to explain how a model arrives at its predictions in ways stakeholders understand.</li> <li>overfitting \u2014 When a model memorizes training data patterns so closely that it performs poorly on new samples.</li> <li>precision \u2014 Share of predicted positives that are actually correct for a given classifier.</li> <li>privacy \u2014 Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</li> <li>privacy impact assessment \u2014 Structured review that evaluates how a system collects, uses, and safeguards personal data.</li> <li>prompt engineering \u2014 Crafting and testing prompts to steer model behavior toward desired outcomes.</li> <li>quantization \u2014 Technique that compresses model weights into lower-precision formats to shrink size and speed inference.</li> <li>recall \u2014 Share of actual positives a model successfully identifies.</li> <li>red teaming \u2014 Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</li> <li>repetition penalty \u2014 Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</li> <li>reranking \u2014 Step that refines retrieval results using a more precise but slower scoring model.</li> <li>responsible ai \u2014 Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</li> <li>retrieval \u2014 Process of selecting relevant documents or vectors from a corpus in response to a query.</li> <li>retrieval-augmented generation \u2014 Workflow that grounds a generative model with retrieved context before producing output.</li> <li>roc auc \u2014 Metric summarizing binary classifier performance by measuring area under the ROC curve.</li> <li>safety evaluation \u2014 Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</li> <li>synthetic data \u2014 Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</li> <li>synthetic data evaluation \u2014 Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</li> <li>system prompt \u2014 Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</li> <li>temperature \u2014 Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</li> <li>token \u2014 Smallest unit of text a model processes after tokenization, such as a word fragment or character.</li> <li>tool use \u2014 Pattern where a model selects external tools or functions to handle parts of a task.</li> <li>top-k sampling \u2014 Decoding method that samples from the k most probable next tokens to balance diversity and control.</li> <li>top-p sampling \u2014 Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</li> <li>vector store \u2014 Database optimized to store embeddings and execute similarity search over vectors.</li> <li>voice cloning \u2014 Technique that replicates a person\u2019s voice using generative models trained on audio samples.</li> </ul>"},{"location":"terms/agentic-ai/","title":"agentic ai","text":""},{"location":"terms/agentic-ai/#agentic-ai","title":"agentic ai","text":"<p>Aliases: ai agents, autonomous agent Categories: Agents &amp; Tooling Roles: Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/agentic-ai/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/agentic-ai/#short-definition","title":"Short definition","text":"<p>Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.</p>"},{"location":"terms/agentic-ai/#long-definition","title":"Long definition","text":"<p>Agentic AI describes architectures where models make decisions about what actions to take next, often using planning loops, tool calls, and memory to pursue a goal. Unlike single-shot prompting, agentic systems break work into steps, call APIs, and reflect on intermediate results before proceeding. They power use cases like research assistants, workflow automation, and incident triage bots. Engineers combine language models with planners, vector memories, and policy checks to maintain control. Product teams set guardrails on autonomy levels, defining when humans approve steps or review logs. Governance stakeholders focus on accountability, ensuring agent actions are auditable, reversible, and aligned with policy. Because agentic AI increases the surface area for safety incidents or costly loops, monitoring and termination criteria are essential. Investing in agent design principles helps organizations harness automation without losing visibility or violating compliance commitments.</p>"},{"location":"terms/agentic-ai/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Agentic AI strings tasks together so the assistant can take initiative instead of waiting for every instruction.</li> <li>Engineer: Orchestrate planning, tool invocation, and memory components around an LLM to execute multi-step objectives.</li> </ul>"},{"location":"terms/agentic-ai/#examples","title":"Examples","text":"<p>Do - Define termination conditions and escalation triggers before enabling autonomous execution.</p> <p>Don't - Allow agents unfettered access to production systems without audit trails or throttling.</p>"},{"location":"terms/agentic-ai/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, risk_management</li> <li>Risk notes: Autonomous loops can drift from intent or amplify harmful behaviors if oversight is weak.</li> </ul>"},{"location":"terms/agentic-ai/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative ai</li> <li>Narrower: tool use, retrieval-augmented generation</li> <li>Related: guardrails, system prompt, incident response</li> </ul>"},{"location":"terms/agentic-ai/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Hugging Face Glossary</li> <li>UK POST AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/agentic-ai.yml</code></p>"},{"location":"terms/ai-incident-response/","title":"ai incident response","text":""},{"location":"terms/ai-incident-response/#ai-incident-response","title":"ai incident response","text":"<p>Aliases: model incident response, ai escalation Categories: Operations &amp; Monitoring, Governance &amp; Risk Roles: Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/ai-incident-response/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/ai-incident-response/#short-definition","title":"Short definition","text":"<p>Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.</p>"},{"location":"terms/ai-incident-response/#long-definition","title":"Long definition","text":"<p>AI incident response adapts traditional incident management to the unique risks of machine learning systems. It defines how teams detect unusual behavior, declare incidents, assemble cross-functional responders, communicate with stakeholders, and deploy mitigations or rollbacks. Triggers include safety violations, hallucinations with material impact, security breaches, or regulatory inquiries. Effective programs maintain runbooks that cover data isolation, prompt freezes, feature flags, and guardrail adjustments. Product, engineering, legal, and communications partners collaborate to assess severity, user impact, and reporting obligations. Governance frameworks expect documented incident response procedures with clear owners and timelines, particularly for high-risk deployments. After-action reviews capture learnings that feed back into evaluation suites, prompts, and monitoring. Without a disciplined incident response plan, organizations risk delayed containment, regulatory penalties, and erosion of user trust.</p>"},{"location":"terms/ai-incident-response/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: AI incident response is the playbook that keeps harm contained and stakeholders informed when something goes wrong.</li> <li>Engineer: Detect anomalies, page the on-call rotation, freeze risky components, and coordinate fixes across data, model, and infra teams.</li> </ul>"},{"location":"terms/ai-incident-response/#examples","title":"Examples","text":"<p>Do - Run quarterly tabletop exercises to rehearse AI incident response workflows.</p> <p>Don't - Silence alerts or skip postmortems once an incident is closed.</p>"},{"location":"terms/ai-incident-response/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Lack of documented response plans prolongs harmful behavior and undermines regulatory reporting.</li> </ul>"},{"location":"terms/ai-incident-response/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: red teaming, guardrails, ml observability</li> </ul>"},{"location":"terms/ai-incident-response/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/ai-incident-response.yml</code></p>"},{"location":"terms/algorithmic-bias/","title":"algorithmic bias","text":""},{"location":"terms/algorithmic-bias/#algorithmic-bias","title":"algorithmic bias","text":"<p>Aliases: systemic bias, ai bias Categories: Governance &amp; Risk Roles: Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Communications &amp; Enablement, Security &amp; Trust Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p> <p>Put it into practice</p> <p>Run the Governance Dashboard checklist before launch.</p>"},{"location":"terms/algorithmic-bias/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/algorithmic-bias/#short-definition","title":"Short definition","text":"<p>Systematic unfairness in model outputs that disadvantages certain groups or outcomes.</p>"},{"location":"terms/algorithmic-bias/#long-definition","title":"Long definition","text":"<p>Algorithmic bias arises when models produce systematically skewed results that disadvantage individuals or groups, often reflecting historical inequities in training data, feature selection, or objective functions. Bias can manifest through discriminatory false positives, unequal error rates, or exclusionary recommendations. Organizations must examine the full pipeline\u2014data collection, labeling, modeling, evaluation, and deployment\u2014to identify root causes. Product teams collaborate with policy, legal, and communications partners to define fairness objectives, while engineers implement bias detection metrics, reweighting, and post-processing adjustments. Governance frameworks (NIST AI RMF, EU AI Act) require documentation of bias assessments, stakeholder engagement, and remediation plans. Failing to manage algorithmic bias can create legal liability, reputational damage, and harm to marginalized communities. Continuous monitoring, diverse evaluation cohorts, and community feedback loops are essential for sustained mitigation.</p>"},{"location":"terms/algorithmic-bias/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Algorithmic bias is when the AI treats groups unfairly, risking customer trust and regulatory violations.</li> <li>Engineer: Quantify and mitigate disparities across subpopulations using metrics like equalized odds, demographic parity, and subgroup ROC analysis.</li> </ul>"},{"location":"terms/algorithmic-bias/#examples","title":"Examples","text":"<p>Do - Include fairness metrics in evaluation pipelines and report them alongside accuracy. - Engage impacted stakeholders when designing mitigation strategies.</p> <p>Don't - Launch features without testing performance across sensitive attributes. - Assume bias is solved after one mitigation; monitor continuously.</p>"},{"location":"terms/algorithmic-bias/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: fairness, transparency</li> <li>Risk notes: Unmitigated bias can violate anti-discrimination laws, trigger regulatory action, and erode brand trust.</li> </ul>"},{"location":"terms/algorithmic-bias/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai</li> <li>Related: safety evaluation, red teaming, privacy impact assessment</li> </ul>"},{"location":"terms/algorithmic-bias/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/algorithmic-bias.yml</code></p>"},{"location":"terms/alignment/","title":"alignment","text":""},{"location":"terms/alignment/#alignment","title":"alignment","text":"<p>Aliases: AI alignment, value alignment Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/alignment/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/alignment/#short-definition","title":"Short definition","text":"<p>Making sure AI systems optimize for human values, policies, and intended outcomes.</p>"},{"location":"terms/alignment/#long-definition","title":"Long definition","text":"<p>Alignment is the multidisciplinary effort to design AI systems whose goals, behaviors, and outputs remain consistent with human intent and societal norms. It spans technical research\u2014such as reward modeling, constitutional AI, interpretability, and adversarial training\u2014and organizational governance, including policy frameworks, oversight committees, and escalation paths. Alignment work acknowledges that models learn from imperfect data and may pursue proxy objectives that conflict with human priorities. Product leaders use alignment roadmaps to decide which features require human-in-the-loop review, while engineers translate alignment goals into metrics, eval harnesses, and guardrails. Regulators and standards bodies, including NIST and ISO, emphasize alignment as part of trustworthy AI, requiring documentation of assumptions, residual risks, and impact mitigation strategies. Sustainable alignment programs treat it as an ongoing lifecycle activity rather than a one-time tuning exercise.</p>"},{"location":"terms/alignment/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Alignment is how we make sure the AI keeps serving our mission and values as it evolves.</li> <li>Engineer: Research and governance toolkit ensuring loss functions, feedback loops, and guardrails drive behavior toward intended objectives.</li> </ul>"},{"location":"terms/alignment/#examples","title":"Examples","text":"<p>Do - Document alignment hypotheses and track eval metrics tied to specific risk scenarios.</p> <p>Don't - Assume alignment is solved after one fine-tuning pass without continuous monitoring.</p>"},{"location":"terms/alignment/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, transparency, validity</li> <li>Risk notes: Weak alignment programs allow models to pursue proxy goals that conflict with legal or ethical obligations.</li> </ul>"},{"location":"terms/alignment/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible AI</li> <li>Narrower: constitutional AI, reinforcement learning from human feedback</li> <li>Related: guardrails, evaluation, red teaming</li> </ul>"},{"location":"terms/alignment/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Stanford HAI Brief Definitions</li> <li>UK POST AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/alignment.yml</code></p>"},{"location":"terms/attention/","title":"attention","text":""},{"location":"terms/attention/#attention","title":"attention","text":"<p>Aliases: attention mechanism, self-attention Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/attention/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/attention/#short-definition","title":"Short definition","text":"<p>Technique enabling models to weight input tokens differently when producing each output.</p>"},{"location":"terms/attention/#long-definition","title":"Long definition","text":"<p>Attention assigns dynamic importance scores to tokens so a model can focus on the most relevant parts of the sequence when generating or interpreting outputs. In transformer architectures, self-attention computes query, key, and value projections that interact through scaled dot products, allowing every token to attend to every other token in the same layer. Multi-head attention repeats this operation across parallel subspaces, capturing nuanced relationships such as syntax, long-range dependencies, and positional context. The mechanism replaced recurrent networks for many language and vision tasks by enabling parallel processing and rich contextual reasoning. Engineers diagnose quality issues by inspecting attention patterns, tuning head counts, or constraining context windows to manage memory. Governance teams monitor attention configurations because they influence explainability\u2014saliency maps and attribution methods often rely on attention weights to justify model decisions in regulated settings.</p>"},{"location":"terms/attention/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Attention is how the model decides which words or pixels matter most before answering.</li> <li>Engineer: QKV projections with softmax-normalized weights that let each token aggregate information from the entire sequence.</li> </ul>"},{"location":"terms/attention/#examples","title":"Examples","text":"<p>Do - Profile attention head usage to identify redundant heads before applying pruning or distillation.</p> <p>Don't - Assume longer context windows automatically improve answers without verifying attention saturation and memory usage.</p>"},{"location":"terms/attention/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, robustness</li> <li>Risk notes: Opaque attention patterns can hinder explainability obligations in regulated workflows.</li> </ul>"},{"location":"terms/attention/#relationships","title":"Relationships","text":"<ul> <li>Broader: transformer</li> <li>Narrower: cross-attention, multi-head attention</li> <li>Related: context window, kv cache, token</li> </ul>"},{"location":"terms/attention/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/attention.yml</code></p>"},{"location":"terms/beam-search/","title":"beam search","text":""},{"location":"terms/beam-search/#beam-search","title":"beam search","text":"<p>Aliases: beam decoding, multi-path decoding Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-02)</p> <p>Put it into practice</p> <p>Pair with the Prompt Engineering Playbook when crafting deterministic flows.</p>"},{"location":"terms/beam-search/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/beam-search/#short-definition","title":"Short definition","text":"<p>Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.</p>"},{"location":"terms/beam-search/#long-definition","title":"Long definition","text":"<p>Beam search expands several candidate continuations in parallel, keeping only the highest-scoring sequences at each step according to their cumulative log probabilities. By exploring multiple beams instead of a single greedy path, the method can uncover higher-quality completions and reduce the chance of getting stuck in locally optimal\u2014but globally poor\u2014answers. Teams tune beam width and length penalties to balance diversity against compute cost, because wider beams demand more memory and latency. Product workflows that require structured or citation-heavy responses often pair beam search with reranking or guardrails to ensure final selections remain compliant. Engineers also log intermediate beams to debug why a response was chosen and to audit near-miss alternatives. Governance stakeholders review beam configurations in high-stakes deployments to confirm that deterministic choices align with evaluation baselines and do not inadvertently suppress required disclosures.</p>"},{"location":"terms/beam-search/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Beam search is a quality knob\u2014explore a few strong answer paths before picking the best one.</li> <li>Engineer: Maintain N top-scoring sequences using cumulative log probabilities; apply length penalties and reranking before selecting the final candidate.</li> </ul>"},{"location":"terms/beam-search/#examples","title":"Examples","text":"<p>Do - Monitor latency impact when increasing beam width beyond 4 to ensure SLAs still hold. - Analyze discarded beams during incident reviews to understand alternative outputs.</p> <p>Don't - Ship beam search without configuring length penalties, which can bias toward verbose responses. - Use the same beam width across all locales without measuring cost and accuracy trade-offs.</p>"},{"location":"terms/beam-search/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: Misconfigured beams can surface repetitive or off-policy content; document parameters and align them with evaluation coverage.</li> </ul>"},{"location":"terms/beam-search/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: greedy decoding, top-k sampling, log probability</li> </ul>"},{"location":"terms/beam-search/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/beam-search.yml</code></p>"},{"location":"terms/bias-variance-tradeoff/","title":"bias-variance tradeoff","text":""},{"location":"terms/bias-variance-tradeoff/#bias-variance-tradeoff","title":"bias-variance tradeoff","text":"<p>Aliases: bias variance trade-off, generalization tradeoff Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/bias-variance-tradeoff/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/bias-variance-tradeoff/#short-definition","title":"Short definition","text":"<p>Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.</p>"},{"location":"terms/bias-variance-tradeoff/#long-definition","title":"Long definition","text":"<p>The bias-variance tradeoff describes how model complexity influences generalization. High-bias models make strong simplifying assumptions, often underfitting by missing real structure in the data. Low-bias models capture more nuance but can exhibit high variance, reacting strongly to noise and overfitting. Practitioners seek a sweet spot where both error sources are minimized. Diagnostics include validation curves that plot training and test error against model complexity, or Monte Carlo simulations that estimate variance across resampled datasets. Techniques such as regularization, ensemble learning, and cross-validation help navigate the tradeoff. Governance teams consider this tradeoff when assessing reliability: models tuned solely for accuracy may become unstable in production, while overly conservative models can entrench bias and miss meaningful signals. Documenting the rationale behind chosen complexity levels supports compliance and future audits.</p>"},{"location":"terms/bias-variance-tradeoff/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: The bias-variance tradeoff explains why simplifying too much misses insight, but over-optimizing creates fragile, noisy behavior.</li> <li>Engineer: Decompose generalization error into bias and variance terms; use validation diagnostics, regularization, and ensembles to reach the lowest combined error.</li> </ul>"},{"location":"terms/bias-variance-tradeoff/#examples","title":"Examples","text":"<p>Do - Plot learning curves to identify whether adding capacity improves validation performance. - Use k-fold cross-validation to estimate variance before promoting a model.</p> <p>Don't - Rely solely on training metrics when evaluating model quality. - Select the most complex architecture without evidence it improves validation outcomes.</p>"},{"location":"terms/bias-variance-tradeoff/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Ignoring the tradeoff leads to brittle models that either underperform or fail compliance evaluations in the field.</li> </ul>"},{"location":"terms/bias-variance-tradeoff/#relationships","title":"Relationships","text":"<ul> <li>Broader: model training</li> <li>Related: overfitting, cross-validation, regularization</li> </ul>"},{"location":"terms/bias-variance-tradeoff/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>UK POST AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/bias-variance-tradeoff.yml</code></p>"},{"location":"terms/chunking/","title":"chunking","text":""},{"location":"terms/chunking/#chunking","title":"chunking","text":"<p>Aliases: document chunking, segmentation Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/chunking/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/chunking/#short-definition","title":"Short definition","text":"<p>Splitting source documents into manageable pieces before indexing or feeding them to models.</p>"},{"location":"terms/chunking/#long-definition","title":"Long definition","text":"<p>Chunking divides large documents, transcripts, or code repositories into smaller segments that fit within a model\u2019s context window and maintain topical coherence. Effective chunking balances granularity: slices must be large enough to preserve meaning yet small enough to avoid wasting tokens on irrelevant text. Techniques include fixed-length windows, semantic segmentation based on headings, and overlap strategies that keep shared context between adjacent chunks. In retrieval-augmented generation systems, chunking quality directly influences whether relevant passages appear in the prompt and whether citations map to recognizable sections. Engineers document chunking parameters alongside embedding models so they can reproduce indexes and evaluate recall. Governance teams assess chunking for privacy and intellectual property concerns, ensuring sensitive data is not inadvertently duplicated or exposed beyond its intended audience when recombining chunks.</p>"},{"location":"terms/chunking/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Chunking is how we slice big documents so the AI can reason over them without losing the thread.</li> <li>Engineer: Segmentation pipeline defining window size, overlap, and heuristics that prepare content for embedding and retrieval.</li> </ul>"},{"location":"terms/chunking/#examples","title":"Examples","text":"<p>Do - Tune chunk size and overlap per document type and validate with retrieval relevance tests.</p> <p>Don't - Use a single chunking strategy for code, policy docs, and conversations without benchmarking.</p>"},{"location":"terms/chunking/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, privacy</li> <li>Risk notes: Poor chunking can duplicate sensitive data or break context needed for accurate, compliant answers.</li> </ul>"},{"location":"terms/chunking/#relationships","title":"Relationships","text":"<ul> <li>Broader: data preprocessing</li> <li>Narrower: semantic chunking</li> <li>Related: retrieval, vector store, reranking</li> </ul>"},{"location":"terms/chunking/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/chunking.yml</code></p>"},{"location":"terms/clip/","title":"clip","text":""},{"location":"terms/clip/#clip","title":"clip","text":"<p>Aliases: contrastive language-image pretraining, clip model Categories: Foundations, Retrieval &amp; RAG Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Communications &amp; Enablement Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/clip/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/clip/#short-definition","title":"Short definition","text":"<p>Multimodal model that embeds images and text into a shared space using contrastive learning.</p>"},{"location":"terms/clip/#long-definition","title":"Long definition","text":"<p>CLIP (Contrastive Language-Image Pretraining) jointly trains an image encoder and a text encoder so that semantically related images and captions map to nearby vectors. The approach uses large-scale image-text pairs scraped from the web and optimizes a contrastive loss that pushes matching pairs together while separating non-matching ones. Once trained, CLIP can perform zero-shot classification, retrieval, and multimodal search by comparing similarity between embeddings. Product teams leverage CLIP to improve content moderation, recommendation, and creative tooling without task-specific labels. Engineers integrate CLIP embeddings into vector stores or downstream fine-tuning pipelines, paying attention to bias, licensing, and safety constraints inherited from web-scale training data. Communications and policy teams monitor CLIP use because it can expose cultural biases and sensitive associations unless mitigated through filtering and evaluation.</p>"},{"location":"terms/clip/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: CLIP understands images and text together, letting you search or classify visuals using natural language.</li> <li>Engineer: Encode images and text with separate transformers trained via contrastive loss; use cosine similarity for retrieval, zero-shot classification, or RAG pipelines.</li> </ul>"},{"location":"terms/clip/#examples","title":"Examples","text":"<p>Do - Audit embeddings for demographic bias before deploying search or moderation features. - Cache CLIP embeddings and align them with domain-specific prompts to improve precision.</p> <p>Don't - Assume CLIP is license-clean; review dataset provenance and usage restrictions. - Ignore safety filters when exposing CLIP-powered features to end users.</p>"},{"location":"terms/clip/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, transparency</li> <li>Risk notes: CLIP inherits web-scale bias and copyright concerns; record how outputs are filtered and evaluated.</li> </ul>"},{"location":"terms/clip/#relationships","title":"Relationships","text":"<ul> <li>Broader: embedding</li> <li>Related: retrieval, synthetic data, guardrails</li> </ul>"},{"location":"terms/clip/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/clip.yml</code></p>"},{"location":"terms/confusion-matrix/","title":"confusion matrix","text":""},{"location":"terms/confusion-matrix/#confusion-matrix","title":"confusion matrix","text":"<p>Aliases: contingency table, error matrix Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/confusion-matrix/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/confusion-matrix/#short-definition","title":"Short definition","text":"<p>Table that summarizes true/false positives and negatives to diagnose classification performance.</p>"},{"location":"terms/confusion-matrix/#long-definition","title":"Long definition","text":"<p>A confusion matrix breaks down classification results into true positives, true negatives, false positives, and false negatives. Visualizing performance in this way helps teams understand error patterns, detect class imbalance issues, and compute derived metrics such as precision, recall, specificity, and accuracy. Product managers use confusion matrices to explain trade-offs to stakeholders, while engineers leverage them to tune thresholds, resampling strategies, or loss functions. Policy and compliance reviewers rely on confusion matrix evidence to confirm that sensitive cohorts are not disproportionately affected. Confusion matrices are most informative when compared across slices\u2014by time, geography, or demographic attributes\u2014to catch drift and fairness concerns.</p>"},{"location":"terms/confusion-matrix/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A confusion matrix shows where the AI is right or wrong so you can see which mistakes matter most.</li> <li>Engineer: Tabulate TP, TN, FP, FN counts; analyze row/column distributions and normalize per class to diagnose bias and guide mitigation.</li> </ul>"},{"location":"terms/confusion-matrix/#examples","title":"Examples","text":"<p>Do - Normalize confusion matrices to compare error rates across classes of different sizes. - Track confusion matrices over time to spot drift and guardrail regressions.</p> <p>Don't - Rely on aggregate accuracy without inspecting confusion patterns. - Hide confusion matrices from stakeholders\u2014they clarify trade-offs.</p>"},{"location":"terms/confusion-matrix/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, validity</li> <li>Risk notes: Ignoring confusion matrix signals can allow harmful error patterns to persist unnoticed.</li> </ul>"},{"location":"terms/confusion-matrix/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: precision, recall, roc auc</li> </ul>"},{"location":"terms/confusion-matrix/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/confusion-matrix.yml</code></p>"},{"location":"terms/content-moderation/","title":"content moderation","text":""},{"location":"terms/content-moderation/#content-moderation","title":"content moderation","text":"<p>Aliases: trust and safety, policy enforcement Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Policy &amp; Risk, Communications &amp; Enablement, Product &amp; Program Managers, Security &amp; Trust, Engineering &amp; Platform Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p> <p>Put it into practice</p> <p>Reference the Governance Dashboard for monitoring obligations.</p>"},{"location":"terms/content-moderation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/content-moderation/#short-definition","title":"Short definition","text":"<p>Workflows and tools that review, filter, and act on user-generated content to enforce policy.</p>"},{"location":"terms/content-moderation/#long-definition","title":"Long definition","text":"<p>Content moderation combines automation, human review, and escalation procedures to detect policy violations such as hate speech, harassment, misinformation, or disallowed imagery. AI systems often provide the first layer of moderation by classifying or scoring content for human queues, requiring careful tuning of precision/recall trade-offs. Policy teams define enforcement rules, while communications and legal stakeholders handle appeals and transparency reports. Engineering teams maintain moderation pipelines, logging, and guardrails; security teams ensure abuse detection remains resilient. Effective moderation programs rely on measurement, red-teaming, and incident response to adapt to adversarial users and evolving regulations.</p>"},{"location":"terms/content-moderation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Content moderation protects users and the brand by keeping AI outputs and user posts within policy.</li> <li>Engineer: Blend classifiers, heuristic filters, and human review; monitor performance, appeals, and adversarial attempts.</li> </ul>"},{"location":"terms/content-moderation/#examples","title":"Examples","text":"<p>Do - Audit moderation models for bias against protected groups. - Publish user-facing guidelines and escalation paths.</p> <p>Don't - Rely solely on automation without human oversight for edge cases. - Ignore feedback loops when policies change.</p>"},{"location":"terms/content-moderation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Weak moderation exposes users to harm, invites regulatory fines, and erodes trust.</li> </ul>"},{"location":"terms/content-moderation/#relationships","title":"Relationships","text":"<ul> <li>Broader: guardrails</li> <li>Related: safety evaluation, incident response, algorithmic bias</li> </ul>"},{"location":"terms/content-moderation/#citations","title":"Citations","text":"<ul> <li>UK POST AI Glossary</li> <li>NIST AI RMF Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/content-moderation.yml</code></p>"},{"location":"terms/context-window/","title":"context window","text":""},{"location":"terms/context-window/#context-window","title":"context window","text":"<p>Aliases: context length, sequence length limit Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/context-window/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/context-window/#short-definition","title":"Short definition","text":"<p>Maximum number of tokens a model can consider at once during prompting or inference.</p>"},{"location":"terms/context-window/#long-definition","title":"Long definition","text":"<p>The context window defines how many tokens a model can ingest in a single forward pass, combining both prompt input and generated output. Because transformer attention scales quadratically with sequence length, vendors ship models with fixed limits that balance cost, latency, and accuracy. Exceeding the window forces truncation or sliding-window strategies that can drop critical instructions. Product teams map context size to user scenarios\u2014for example, how long a document can be summarized or how many chat messages remain in memory. Engineers manage context budgets through prompt compression, retrieval chunking, and caching mechanisms like recurrent attention or stateful decoding. Governance stakeholders examine context policies to ensure sensitive data is not unintentionally persisted across conversations or logged beyond retention periods, especially when long windows capture personally identifiable information.</p>"},{"location":"terms/context-window/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A context window is the attention span of the model\u2014go past it and instructions fall off.</li> <li>Engineer: Token budget for input plus output per request; drives attention memory use and dictates truncation strategies.</li> </ul>"},{"location":"terms/context-window/#examples","title":"Examples","text":"<p>Do - Document context requirements by user journey so prompts stay within safe token limits.</p> <p>Don't - Assume extending the context window eliminates the need for retrieval or prompt optimization.</p>"},{"location":"terms/context-window/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, privacy</li> <li>Risk notes: Oversized contexts can capture unnecessary personal data and complicate retention or deletion obligations.</li> </ul>"},{"location":"terms/context-window/#relationships","title":"Relationships","text":"<ul> <li>Broader: prompt engineering</li> <li>Narrower: long-context models</li> <li>Related: token, attention, kv cache</li> </ul>"},{"location":"terms/context-window/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/context-window.yml</code></p>"},{"location":"terms/cross-validation/","title":"cross-validation","text":""},{"location":"terms/cross-validation/#cross-validation","title":"cross-validation","text":"<p>Aliases: k-fold validation, cv Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/cross-validation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/cross-validation/#short-definition","title":"Short definition","text":"<p>Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.</p>"},{"location":"terms/cross-validation/#long-definition","title":"Long definition","text":"<p>Cross-validation repeatedly partitions a labeled dataset into complementary subsets to assess how a model generalizes. In k-fold cross-validation, the dataset is divided into k folds; the model trains on k-1 folds and validates on the remaining fold, cycling until every fold has served as the validation set. Aggregating results reduces variance compared with a single train-test split and surfaces instability caused by small datasets or class imbalance. Variants such as stratified, time-series, and leave-one-out cross-validation address specific domains. Product managers rely on cross-validation when comparing model candidates, while engineers use fold-level diagnostics to catch overfitting and data leakage before production. Governance teams view cross-validation artifacts as evidence that evaluation processes are robust and reproducible, particularly for regulated scenarios where a single split could mask risk.</p>"},{"location":"terms/cross-validation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Cross-validation is the rehearsal that shows how a model behaves across different slices before real customers see it.</li> <li>Engineer: Partition data into k folds, train/validate across permutations, and average metrics to estimate generalization; track per-fold variance for risk analysis.</li> </ul>"},{"location":"terms/cross-validation/#examples","title":"Examples","text":"<p>Do - Use stratified folds when label imbalance could skew results. - Store per-fold metrics and random seeds for reproducibility.</p> <p>Don't - Leak validation data by reusing preprocessing steps fitted on the full dataset. - Rely on a single train/test split for high-stakes decisions.</p>"},{"location":"terms/cross-validation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Skipping cross-validation invites optimistic bias and limits evidence required for audits or legal reviews.</li> </ul>"},{"location":"terms/cross-validation/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: overfitting, bias-variance tradeoff, model drift</li> </ul>"},{"location":"terms/cross-validation/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/cross-validation.yml</code></p>"},{"location":"terms/data-minimization/","title":"data minimization","text":""},{"location":"terms/data-minimization/#data-minimization","title":"data minimization","text":"<p>Aliases: data minimisation, minimal data collection Categories: Governance &amp; Risk Roles: Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/data-minimization/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/data-minimization/#short-definition","title":"Short definition","text":"<p>Principle of collecting and retaining only the data necessary for a defined purpose.</p>"},{"location":"terms/data-minimization/#long-definition","title":"Long definition","text":"<p>Data minimization limits the amount and duration of personal data collected, processed, or stored. Regulations such as GDPR and CCPA mandate this principle so organizations cannot hoard information \u201cjust in case.\u201d In AI projects, minimization applies to training datasets, prompt logs, telemetry, and derived embeddings. Product teams collaborate with legal and security to define clear purposes, expiration timelines, and deletion workflows. Engineers implement technical controls such as field-level encryption, redaction, and retention policies. Adoption of data minimization reduces breach impact, compliance risk, and model bias stemming from unnecessary sensitive attributes. Documenting minimization choices is also a prerequisite for privacy impact assessments and external audits.</p>"},{"location":"terms/data-minimization/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Data minimization keeps us from collecting more personal information than we need, reducing risk.</li> <li>Engineer: Implement least-data pipelines, redact sensitive fields, and enforce retention limits that map to documented business purposes.</li> </ul>"},{"location":"terms/data-minimization/#examples","title":"Examples","text":"<p>Do - Align prompts and telemetry retention with published privacy notices. - Automate deletion of training data that no longer supports the model\u2019s purpose.</p> <p>Don't - Copy entire databases into model pipelines without reviewing necessity. - Keep user data after opt-outs or contract end dates.</p>"},{"location":"terms/data-minimization/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management</li> <li>Risk notes: Excess data increases breach exposure, drives regulatory penalties, and can introduce bias into models.</li> </ul>"},{"location":"terms/data-minimization/#relationships","title":"Relationships","text":"<ul> <li>Broader: privacy</li> <li>Related: privacy impact assessment, differential privacy, incident response</li> </ul>"},{"location":"terms/data-minimization/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/data-minimization.yml</code></p>"},{"location":"terms/data-retention/","title":"data retention","text":""},{"location":"terms/data-retention/#data-retention","title":"data retention","text":"<p>Aliases: retention policy, data lifecycle Categories: Governance &amp; Risk Roles: Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-04)</p> <p>Put it into practice</p> <p>Map retention updates to the Governance Dashboard.</p>"},{"location":"terms/data-retention/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/data-retention/#short-definition","title":"Short definition","text":"<p>Policies defining how long data is stored, where it lives, and how it is deleted.</p>"},{"location":"terms/data-retention/#long-definition","title":"Long definition","text":"<p>Data retention policies specify how long different data types are stored, why they are kept, where they reside, and how they are securely deleted. For AI systems, retention applies to raw inputs, training datasets, prompt logs, embeddings, and evaluation artifacts. Legal and policy teams determine retention limits based on regulations and contracts, while engineering and security implement technical controls such as automated deletion, encryption, and retention metadata. Product managers need visibility into retention rules to communicate with customers and plan features that rely on historical data. Documentation of retention schedules is often required for privacy impact assessments and audits.</p>"},{"location":"terms/data-retention/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Data retention makes sure we only keep the information we need, for as long as we\u2019re allowed.</li> <li>Engineer: Implement retention tags, automated deletion jobs, and secure archival; track compliance with privacy and contractual obligations.</li> </ul>"},{"location":"terms/data-retention/#examples","title":"Examples","text":"<p>Do - Document retention periods for each dataset used in training and inference. - Verify deletion workflows during audits and incident response drills.</p> <p>Don't - Keep prompt logs indefinitely without legal approval. - Mix data from different retention schedules in the same storage bucket.</p>"},{"location":"terms/data-retention/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management</li> <li>Risk notes: Ignoring retention requirements can lead to regulatory fines, litigation, and security incidents.</li> </ul>"},{"location":"terms/data-retention/#relationships","title":"Relationships","text":"<ul> <li>Broader: privacy</li> <li>Related: data minimization, privacy impact assessment, incident response</li> </ul>"},{"location":"terms/data-retention/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/data-retention.yml</code></p>"},{"location":"terms/decoding/","title":"decoding","text":""},{"location":"terms/decoding/#decoding","title":"decoding","text":"<p>Aliases: text decoding, generation decoding Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/decoding/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/decoding/#short-definition","title":"Short definition","text":"<p>Algorithms that turn model probability distributions into output tokens during generation.</p>"},{"location":"terms/decoding/#long-definition","title":"Long definition","text":"<p>Decoding encompasses the strategies used to convert a model\u2019s token probabilities into concrete outputs. Common methods include greedy decoding, beam search, top-k sampling, and nucleus (top-p) sampling, each trading off determinism, diversity, latency, and risk. Selection of a decoding algorithm affects user experience, safety posture, and evaluation results far more than many teams realize. Product managers configure decoding policies by use case\u2014for example, deterministic responses for support scenarios and higher-variance sampling for ideation tools. Engineers implement controls such as temperature scaling, repetition penalties, and logprob thresholds to manage failure modes like hallucinations or loops. Governance programs capture decoding settings in change logs because updates can materially alter risk assessments. Understanding decoding is foundational to building reliable guardrails, interpreting evaluation results, and communicating the behavior of generative systems to stakeholders.</p>"},{"location":"terms/decoding/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Decoding is the decision logic that turns the model\u2019s probabilities into the words customers see.</li> <li>Engineer: Apply algorithms like greedy, top-k, or top-p to sample from the softmax distribution while enforcing constraints and penalties.</li> </ul>"},{"location":"terms/decoding/#examples","title":"Examples","text":"<p>Do - Document decoding parameters alongside each release note for regulated experiences.</p> <p>Don't - Mix decoding strategies across channels without coordinating evaluation coverage.</p>"},{"location":"terms/decoding/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, robustness</li> <li>Risk notes: Untracked decoding changes can invalidate safety testing and confuse incident investigations.</li> </ul>"},{"location":"terms/decoding/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative ai</li> <li>Narrower: greedy decoding, top-k sampling, top-p sampling</li> <li>Related: temperature, log probability, guardrails</li> </ul>"},{"location":"terms/decoding/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/decoding.yml</code></p>"},{"location":"terms/differential-privacy/","title":"differential privacy","text":""},{"location":"terms/differential-privacy/#differential-privacy","title":"differential privacy","text":"<p>Aliases: DP, epsilon-differential privacy Categories: Governance &amp; Risk Roles: Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-02)</p>"},{"location":"terms/differential-privacy/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/differential-privacy/#short-definition","title":"Short definition","text":"<p>Mathematical framework that limits how much any single record influences published data or model outputs.</p>"},{"location":"terms/differential-privacy/#long-definition","title":"Long definition","text":"<p>Differential privacy protects individuals in a dataset by adding calibrated noise to statistics or training procedures so the presence or absence of any one person becomes indistinguishable. The framework is governed by privacy budgets\u2014epsilon and delta\u2014which quantify acceptable leakage. In AI systems, teams apply differential privacy when releasing analytics, training embeddings, or sharing evaluation datasets. Engineers integrate mechanisms like DP-SGD, Laplace noise, or randomized response, tracking accumulated budgets across queries. Legal and policy partners evaluate whether privacy guarantees meet regulatory requirements, especially when models ingest sensitive or regulated data. Security teams monitor for side-channel attacks that could combine multiple noisy outputs to infer personal information. Differential privacy is not a silver bullet; product and research groups balance utility loss against risk mitigation, document assumptions, and communicate residual exposure to stakeholders.</p>"},{"location":"terms/differential-privacy/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Differential privacy lets you learn from user data while keeping any single person unidentifiable.</li> <li>Engineer: Bound information leakage by injecting calibrated noise; manage cumulative epsilon/delta budgets across analytics or training steps.</li> </ul>"},{"location":"terms/differential-privacy/#examples","title":"Examples","text":"<p>Do - Track privacy budgets in dashboards so analysts know when to stop issuing queries. - Explain residual risk and utility trade-offs in launch documentation.</p> <p>Don't - Assume a single noisy release protects against repeated queries without monitoring budget depletion. - Mix differentially private and non-private data exports without clear labeling.</p>"},{"location":"terms/differential-privacy/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management</li> <li>Risk notes: Incorrect epsilon or budget accounting can create a false sense of protection and trigger regulatory exposure.</li> </ul>"},{"location":"terms/differential-privacy/#relationships","title":"Relationships","text":"<ul> <li>Broader: privacy</li> <li>Narrower: differentially private SGD</li> <li>Related: synthetic data, guardrails, model governance</li> </ul>"},{"location":"terms/differential-privacy/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/differential-privacy.yml</code></p>"},{"location":"terms/diffusion-model/","title":"diffusion model","text":""},{"location":"terms/diffusion-model/#diffusion-model","title":"diffusion model","text":"<p>Aliases: denoising diffusion model, score-based model Categories: Foundations, LLM Core Roles: Engineering &amp; Platform, Data Science &amp; Research, Product &amp; Program Managers, Communications &amp; Enablement Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/diffusion-model/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/diffusion-model/#short-definition","title":"Short definition","text":"<p>Generative model that iteratively denoises random noise to synthesize images, audio, or other data.</p>"},{"location":"terms/diffusion-model/#long-definition","title":"Long definition","text":"<p>Diffusion models generate content by reversing a noising process. During training, clean data samples are progressively corrupted with Gaussian noise. The model learns to predict the noise at each step, effectively mapping from noisy inputs back to structure. During inference, the reverse process starts with pure noise and iteratively denoises toward a coherent sample over dozens or hundreds of timesteps. Diffusion models deliver state-of-the-art image and audio synthesis quality, control, and diversity compared with GANs or VAEs, but they can be computationally intensive. Product teams use guidance techniques, safety filters, and prompt engineering to align outputs with brand expectations. Engineers manage sampler choice, scheduler parameters, and hardware acceleration to meet latency targets. Governance stakeholders evaluate diffusion workflows for intellectual property, misinformation, and safety risks, since the models can produce realistic but fabricated content.</p>"},{"location":"terms/diffusion-model/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Diffusion models create images or audio by gradually refining random noise into something recognizable.</li> <li>Engineer: Train with forward noising and reverse denoising steps; deploy with schedulers (DDIM, Euler) and classifier-free guidance to control quality and speed.</li> </ul>"},{"location":"terms/diffusion-model/#examples","title":"Examples","text":"<p>Do - Log prompt, seed, and sampler metadata to reproduce outputs for audits. - Apply content moderation and watermarking to manage safety and attribution.</p> <p>Don't - Assume diffusion outputs are free from copyright or bias concerns. - Ignore the compute cost of small timestep adjustments on production workloads.</p>"},{"location":"terms/diffusion-model/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, transparency</li> <li>Risk notes: Hyper-realistic outputs raise IP, misinformation, and safety challenges that must be documented and mitigated.</li> </ul>"},{"location":"terms/diffusion-model/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative ai</li> <li>Related: synthetic data, guardrails, safety evaluation</li> </ul>"},{"location":"terms/diffusion-model/#citations","title":"Citations","text":"<ul> <li>Wikipedia AI Glossary</li> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/diffusion-model.yml</code></p>"},{"location":"terms/embedding/","title":"embedding","text":""},{"location":"terms/embedding/#embedding","title":"embedding","text":"<p>Aliases: vector embedding, representation vector Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/embedding/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/embedding/#short-definition","title":"Short definition","text":"<p>Dense numerical representation that captures semantic meaning of text, images, or other data.</p>"},{"location":"terms/embedding/#long-definition","title":"Long definition","text":"<p>Embeddings map pieces of information\u2014such as words, sentences, images, or audio\u2014into dense vectors so similar concepts cluster in vector space. Modern foundation models learn embeddings during pretraining, optimizing them to capture contextual meaning, syntactic roles, and domain signals that make downstream tasks like retrieval, classification, and recommendation more effective. When teams build retrieval-augmented generation systems, they compute embeddings for documents and queries, then compare them with similarity metrics like cosine distance or dot product. The quality of those vectors influences whether the right context is retrieved, how well clustering and anomaly detection work, and how fast nearest-neighbor searches execute. Governance programs scrutinize embedding pipelines because biased or outdated vectors can encode harmful associations and propagate them into products. Maintaining embeddings thus requires dataset curation, versioning, and monitoring to detect drift as language or policies change.</p>"},{"location":"terms/embedding/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Embeddings are a way to translate concepts into numbers so the system can find and compare ideas efficiently.</li> <li>Engineer: Model-generated vectors where distance metrics approximate semantic similarity; used for retrieval, clustering, ranking, and transfer learning.</li> </ul>"},{"location":"terms/embedding/#examples","title":"Examples","text":"<p>Do - Version embedding models and store associated tokenizer metadata to support reproducibility.</p> <p>Don't - Mix embeddings from different models without recalculating similarity thresholds or rescoring.</p>"},{"location":"terms/embedding/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, accountability</li> <li>Risk notes: Embedding drift or biased training data can surface discriminatory outputs when used in search or ranking.</li> </ul>"},{"location":"terms/embedding/#relationships","title":"Relationships","text":"<ul> <li>Broader: representation learning</li> <li>Narrower: sentence embedding, image embedding</li> <li>Related: retrieval, vector store, chunking</li> </ul>"},{"location":"terms/embedding/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/embedding.yml</code></p>"},{"location":"terms/evaluation/","title":"evaluation","text":""},{"location":"terms/evaluation/#evaluation","title":"evaluation","text":"<p>Aliases: model evaluation, AI evaluation Categories: Operations &amp; Monitoring, Governance &amp; Risk Roles: Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/evaluation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/evaluation/#short-definition","title":"Short definition","text":"<p>Systematic measurement of model performance, safety, and reliability using defined tests.</p>"},{"location":"terms/evaluation/#long-definition","title":"Long definition","text":"<p>Evaluation is the disciplined practice of testing AI systems against quantitative and qualitative criteria before and after deployment. It extends beyond accuracy metrics to encompass robustness, bias detection, factual correctness, latency, and safety stress tests such as red teaming or jailbreak attempts. Teams build eval suites that blend automated metrics\u2014like BLEU, accuracy@k, or toxicity scores\u2014with human review checklists tailored to critical user journeys. Continuous evaluation supports regression detection when prompts, datasets, or infrastructure change. Governance frameworks treat evaluations as audit artifacts: they document assumptions, thresholds, and sign-offs required before promoting models to production. Mature programs integrate evaluation pipelines into CI/CD, enabling reproducibility and traceability. Without rigorous evaluation, organizations cannot credibly claim their models meet compliance obligations or user expectations.</p>"},{"location":"terms/evaluation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Evaluation is our quality gate\u2014it proves the AI delivers safe, reliable outcomes before we launch.</li> <li>Engineer: Automated and human-in-the-loop test harnesses measuring task metrics, robustness, bias, and safety across model releases.</li> </ul>"},{"location":"terms/evaluation/#examples","title":"Examples","text":"<p>Do - Run targeted red-team scenarios alongside quantitative metrics before shipping new prompts or fine-tuned models.</p> <p>Don't - Rely on a single aggregate score without examining subgroup performance or qualitative feedback.</p>"},{"location":"terms/evaluation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, accountability</li> <li>Risk notes: Skipping or weakening evaluations increases the likelihood of undetected harmful behaviors in production.</li> </ul>"},{"location":"terms/evaluation/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Narrower: safety evaluation, capability evaluation</li> <li>Related: guardrails, alignment, red teaming</li> </ul>"},{"location":"terms/evaluation/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/evaluation.yml</code></p>"},{"location":"terms/f1-score/","title":"f1 score","text":""},{"location":"terms/f1-score/#f1-score","title":"f1 score","text":"<p>Aliases: f-score, harmonic mean of precision and recall Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/f1-score/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/f1-score/#short-definition","title":"Short definition","text":"<p>Harmonic mean of precision and recall, balancing false positives and false negatives.</p>"},{"location":"terms/f1-score/#long-definition","title":"Long definition","text":"<p>The F1 score combines precision and recall into a single metric by calculating their harmonic mean. It is especially useful when positive class distribution is imbalanced and teams need a balanced view of false positives and false negatives. An F1 score close to 1.0 indicates strong performance on both metrics, while low values signal trade-offs that require attention. Product managers use F1 to compare model variants quickly, and engineers track the score across cohorts to detect regressions after retraining or threshold changes. Because F1 weights precision and recall equally, decision-makers should confirm that assumption matches business priorities; otherwise metrics like F-beta or cost-based evaluation may be more appropriate.</p>"},{"location":"terms/f1-score/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: F1 score summarizes how well the AI avoids both false alarms and missed detections.</li> <li>Engineer: 2 * (precision * recall) / (precision + recall); monitor per-class F1 and consider F-beta when recall or precision matters more.</li> </ul>"},{"location":"terms/f1-score/#examples","title":"Examples","text":"<p>Do - Report F1 alongside precision, recall, and confusion matrices for context. - Track F1 trends after model updates to catch regressions early.</p> <p>Don't - Rely on F1 when business costs heavily favor precision or recall. - Compare F1 across datasets without consistent class distributions.</p>"},{"location":"terms/f1-score/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Misinterpreting F1 can hide risk trade-offs; document why the weight between precision and recall fits the use case.</li> </ul>"},{"location":"terms/f1-score/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: precision, recall, confusion matrix</li> </ul>"},{"location":"terms/f1-score/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/f1-score.yml</code></p>"},{"location":"terms/fairness-metrics/","title":"fairness metrics","text":""},{"location":"terms/fairness-metrics/#fairness-metrics","title":"fairness metrics","text":"<p>Aliases: fairness measures, fairness evaluation Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Policy &amp; Risk, Legal &amp; Compliance, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-04)</p> <p>Put it into practice</p> <p>Coordinate with the Role Starter Packs for governance actions.</p>"},{"location":"terms/fairness-metrics/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/fairness-metrics/#short-definition","title":"Short definition","text":"<p>Quantitative measures that evaluate whether model performance is equitable across groups.</p>"},{"location":"terms/fairness-metrics/#long-definition","title":"Long definition","text":"<p>Fairness metrics quantify disparities between demographic or contextual groups. Common measures include demographic parity, equalized odds, equal opportunity, predictive parity, and subgroup-specific precision/recall. Teams apply these metrics during model evaluation, post-deployment monitoring, and incident response to detect fairness regressions. Choosing the right metric depends on regulatory requirements and domain priorities; not all metrics can be optimized simultaneously. Policy and legal stakeholders define fairness thresholds, while engineers build pipelines that compute metrics for each release and in production dashboards. Findings feed into model cards, governance reviews, and mitigation plans.</p>"},{"location":"terms/fairness-metrics/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Fairness metrics tell us whether different groups get comparable outcomes from the AI.</li> <li>Engineer: Compute metrics such as equalized odds, statistical parity difference, or subgroup ROC; monitor trends across releases and production.</li> </ul>"},{"location":"terms/fairness-metrics/#examples","title":"Examples","text":"<p>Do - Align fairness thresholds with business and regulatory requirements. - Track fairness metrics alongside traditional accuracy metrics in CI/CD.</p> <p>Don't - Rely on a single fairness metric without understanding trade-offs. - Evaluate overall fairness without subgroup data or feedback from impacted communities.</p>"},{"location":"terms/fairness-metrics/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: fairness, transparency</li> <li>Risk notes: Failure to monitor fairness metrics can lead to discrimination, legal liability, and loss of trust.</li> </ul>"},{"location":"terms/fairness-metrics/#relationships","title":"Relationships","text":"<ul> <li>Broader: algorithmic bias</li> <li>Related: model interpretability, evaluation, model card</li> </ul>"},{"location":"terms/fairness-metrics/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Google ML Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/fairness-metrics.yml</code></p>"},{"location":"terms/fine-tuning/","title":"fine-tuning","text":""},{"location":"terms/fine-tuning/#fine-tuning","title":"fine-tuning","text":"<p>Aliases: model adaptation, supervised fine-tuning Categories: Optimization &amp; Efficiency Roles: Data Science &amp; Research, Engineering &amp; Platform Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/fine-tuning/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/fine-tuning/#short-definition","title":"Short definition","text":"<p>Additional training that adapts a pretrained model to a specific task or domain.</p>"},{"location":"terms/fine-tuning/#long-definition","title":"Long definition","text":"<p>Fine-tuning continues training from a pretrained checkpoint using a curated dataset that reflects the target task, tone, or policy. By adjusting weights on top of broad foundation knowledge, teams achieve better accuracy and alignment than prompt engineering alone. Approaches include supervised fine-tuning, reinforcement learning from human feedback, parameter-efficient methods like LoRA, and combinations with synthetic data generation. Product leaders plan fine-tuning roadmaps to differentiate experiences or enforce brand voice, while engineers manage hyperparameters, data balancing, and evaluation suites to prevent catastrophic forgetting. Governance stakeholders scrutinize fine-tuning inputs for licensing, privacy, and bias risks, requiring documentation of provenance and review sign-offs. Because fine-tuned models can drift from base guarantees, organizations version checkpoints, run regression tests, and maintain rollback plans to satisfy compliance obligations and operational reliability.</p>"},{"location":"terms/fine-tuning/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Fine-tuning teaches a general model to speak in the organization\u2019s voice and handle domain-specific tasks.</li> <li>Engineer: Continue training a pretrained model on labeled or preference data, tracking hyperparameters, evals, and release packaging.</li> </ul>"},{"location":"terms/fine-tuning/#examples","title":"Examples","text":"<p>Do - Store data lineage and evaluation results for every fine-tuned checkpoint before deployment.</p> <p>Don't - Blend proprietary and open datasets without clarifying licenses and usage rights.</p>"},{"location":"terms/fine-tuning/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, validity</li> <li>Risk notes: Uncontrolled fine-tuning can override safety mitigations or introduce licensed data without traceability.</li> </ul>"},{"location":"terms/fine-tuning/#relationships","title":"Relationships","text":"<ul> <li>Broader: model training</li> <li>Narrower: low-rank adaptation, reinforcement learning from human feedback</li> <li>Related: knowledge distillation, evaluation, alignment</li> </ul>"},{"location":"terms/fine-tuning/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/fine-tuning.yml</code></p>"},{"location":"terms/generative-ai/","title":"generative ai","text":""},{"location":"terms/generative-ai/#generative-ai","title":"generative ai","text":"<p>Aliases: genai, generative artificial intelligence Categories: Foundations Roles: Communications &amp; Enablement, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/generative-ai/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/generative-ai/#short-definition","title":"Short definition","text":"<p>Family of models that produce new content\u2014text, images, code\u2014rather than only making predictions.</p>"},{"location":"terms/generative-ai/#long-definition","title":"Long definition","text":"<p>Generative AI refers to systems that create novel outputs such as text, images, audio, or code by learning patterns from large training corpora. Foundation models like GPT, diffusion models, and multimodal architectures power these experiences by predicting the next token or pixel based on context. Organizations adopt generative AI for drafting, summarization, design, simulation, and synthetic data generation. The technology amplifies creativity and productivity but introduces unique risks, including hallucinations, intellectual property exposure, and safety concerns. Product leaders evaluate generative AI opportunities alongside guardrails, retrieval augmentation, and review workflows to keep outputs trustworthy. Engineers maintain the pipelines that tokenize data, manage prompts, deploy models, and monitor usage at scale. Governance teams coordinate policies covering responsible use, content moderation, privacy, and transparency. Understanding generative AI provides the context for more specialized concepts in this glossary, from attention mechanisms to red teaming.</p>"},{"location":"terms/generative-ai/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Generative AI is the class of systems that draft content on demand, automating creative and analytical tasks.</li> <li>Engineer: Models that learn probability distributions over complex data and sample from them to produce new artifacts.</li> </ul>"},{"location":"terms/generative-ai/#examples","title":"Examples","text":"<p>Do - Pair generative AI deployments with clear disclosure and feedback channels for users.</p> <p>Don't - Launch generative features without documenting data sources, evaluation, and risk mitigations.</p>"},{"location":"terms/generative-ai/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, risk_management</li> <li>Risk notes: Uncontrolled generative systems can leak sensitive data, create misleading content, or breach intellectual property.</li> </ul>"},{"location":"terms/generative-ai/#relationships","title":"Relationships","text":"<ul> <li>Broader: artificial intelligence</li> <li>Narrower: retrieval-augmented generation, hallucination, decoding</li> <li>Related: attention, prompt engineering, guardrails</li> </ul>"},{"location":"terms/generative-ai/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/generative-ai.yml</code></p>"},{"location":"terms/greedy-decoding/","title":"greedy decoding","text":""},{"location":"terms/greedy-decoding/#greedy-decoding","title":"greedy decoding","text":"<p>Aliases: argmax decoding Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/greedy-decoding/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/greedy-decoding/#short-definition","title":"Short definition","text":"<p>Strategy that selects the highest-probability token at each step, producing deterministic outputs.</p>"},{"location":"terms/greedy-decoding/#long-definition","title":"Long definition","text":"<p>Greedy decoding generates text by repeatedly choosing the token with the maximum probability, skipping any sampling. The approach is deterministic, fast, and easy to reason about, which makes it attractive for scenarios that demand consistency or auditability. However, it can lead to repetitive phrasing, premature endings, or failure to explore alternative but valid continuations. Product teams deploy greedy decoding for transactional tasks such as structured responses, deterministic workflows, or template filling, where creativity is less important than reliability. Engineers monitor for mode collapse and may add techniques like repetition penalties or suffix constraints to mitigate degenerate loops. Governance teams value greedy decoding because it simplifies compliance reviews and reproducibility: the same prompt always yields the same answer. Nonetheless, the lack of variation can hide blind spots if evaluations rely solely on argmax outputs, so organizations often complement greedy decoding tests with sampling-based stress cases.</p>"},{"location":"terms/greedy-decoding/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Greedy decoding delivers the same answer every time, prioritizing consistency over creativity.</li> <li>Engineer: Iteratively pick the argmax token from the softmax distribution, allowing deterministic, low-latency generation.</li> </ul>"},{"location":"terms/greedy-decoding/#examples","title":"Examples","text":"<p>Do - Use greedy decoding for policy disclosures where text must match approved language.</p> <p>Don't - Rely on greedy decoding alone when testing for harmful edge cases that require sampling diversity.</p>"},{"location":"terms/greedy-decoding/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, accountability</li> <li>Risk notes: Deterministic decoding simplifies audits but can mask untested behaviors that appear only with sampling.</li> </ul>"},{"location":"terms/greedy-decoding/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: top-k sampling, top-p sampling, temperature</li> </ul>"},{"location":"terms/greedy-decoding/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/greedy-decoding.yml</code></p>"},{"location":"terms/guardrails/","title":"guardrails","text":""},{"location":"terms/guardrails/#guardrails","title":"guardrails","text":"<p>Aliases: safety guardrails, policy guardrails Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/guardrails/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/guardrails/#short-definition","title":"Short definition","text":"<p>Controls that constrain model behavior to comply with safety, legal, or brand requirements.</p>"},{"location":"terms/guardrails/#long-definition","title":"Long definition","text":"<p>Guardrails combine policy, technical, and operational measures designed to keep AI systems within acceptable behavior. They can include pre- and post-processing filters, policy-informed prompts, classifier ensembles, or moderation APIs that block disallowed content before it reaches end users. Effective guardrail programs coordinate with legal and risk teams to encode organizational standards and regulatory obligations. Engineers integrate guardrails into the request pipeline, monitor their performance, and log interventions for audit trails. Product managers review guardrail coverage to understand trade-offs between user experience and safety friction. Governance stakeholders treat guardrails as living controls that require change management, testing, and documentation to demonstrate due diligence. When guardrails fail or drift, the resulting incidents can expose organizations to legal liability, reputational damage, or regulatory penalties.</p>"},{"location":"terms/guardrails/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Guardrails are the checks that keep the AI from saying or doing things that would put the company at risk.</li> <li>Engineer: Policy-aligned filters, prompts, and classifiers embedded in the inference stack to block or reshape unsafe outputs.</li> </ul>"},{"location":"terms/guardrails/#examples","title":"Examples","text":"<p>Do - Test guardrail coverage with red-team prompts whenever the base model or system prompt changes.</p> <p>Don't - Rely on a single moderation classifier without monitoring precision and recall across scenarios.</p>"},{"location":"terms/guardrails/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, privacy, robustness</li> <li>Risk notes: Outdated guardrails can miss harmful outputs or inadvertently censor legitimate content, creating compliance gaps.</li> </ul>"},{"location":"terms/guardrails/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible AI</li> <li>Narrower: output filtering, safety prompt</li> <li>Related: system prompt, temperature, red teaming</li> </ul>"},{"location":"terms/guardrails/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/guardrails.yml</code></p>"},{"location":"terms/hallucination/","title":"hallucination","text":""},{"location":"terms/hallucination/#hallucination","title":"hallucination","text":"<p>Aliases: AI hallucination, confabulation Categories: LLM Core, Governance &amp; Risk Roles: Communications &amp; Enablement, Data Science &amp; Research, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-09-18)</p>"},{"location":"terms/hallucination/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/hallucination/#short-definition","title":"Short definition","text":"<p>When an AI model presents fabricated or unsupported information as fact.</p>"},{"location":"terms/hallucination/#long-definition","title":"Long definition","text":"<p>Hallucination describes the tendency of generative models to deliver content that sounds plausible but is either factually incorrect, logically inconsistent, or entirely invented. The phenomenon stems from the probabilistic way large language models predict the next token based on training data patterns rather than grounded knowledge of the world. It can occur when prompts lack sufficient context, when the model has not seen relevant examples during training, or when decoding strategies over-index on fluency instead of accuracy. Product teams experience hallucination as broken user trust, while engineers may notice it during evaluation as high lexical overlap paired with low factual precision. Mitigations range from retrieval augmentation and prompt constraints to post-generation fact checking, human review, and model fine-tuning on verified corpora. Organizations must treat hallucination as both a quality and a risk management issue, particularly in regulated or safety-critical workflows.</p>"},{"location":"terms/hallucination/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Signals that the model is making things up, which erodes user trust and can trigger compliance issues.</li> <li>Engineer: Indicates the model sampled a high-probability sequence lacking factual grounding; investigate context, decoding, and eval signals.</li> </ul>"},{"location":"terms/hallucination/#examples","title":"Examples","text":"<p>Do - Log hallucination incidents and route high-severity cases to human review for remediation. - Use retrieval augmentation or tool grounding to supply verifiable context before generation.</p> <p>Don't - Deploy long-form responses without monitoring factual accuracy or adding disclaimers. - Assume higher model size alone will eliminate hallucination without evaluation improvements.</p>"},{"location":"terms/hallucination/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accuracy, transparency, validity</li> <li>Risk notes: Unaddressed hallucinations can produce misleading outputs that violate accuracy commitments and create legal exposure.</li> </ul>"},{"location":"terms/hallucination/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative AI</li> <li>Narrower: factual hallucination, formal hallucination</li> <li>Related: retrieval-augmented generation, guardrails, evaluation</li> </ul>"},{"location":"terms/hallucination/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/hallucination.yml</code></p>"},{"location":"terms/knowledge-distillation/","title":"knowledge distillation","text":""},{"location":"terms/knowledge-distillation/#knowledge-distillation","title":"knowledge distillation","text":"<p>Aliases: distillation, teacher-student training Categories: Optimization &amp; Efficiency Roles: Data Science &amp; Research, Engineering &amp; Platform Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/knowledge-distillation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/knowledge-distillation/#short-definition","title":"Short definition","text":"<p>Technique that trains a smaller student model to mimic a larger teacher model\u2019s behavior.</p>"},{"location":"terms/knowledge-distillation/#long-definition","title":"Long definition","text":"<p>Knowledge distillation transfers capabilities from a high-capacity teacher model to a more efficient student model by training the student on the teacher\u2019s softened output distributions or generated examples. The approach preserves much of the teacher\u2019s accuracy while reducing parameter count, latency, and cost\u2014making it popular for edge deployments and inference scaling. Distillation complements other efficiency strategies such as quantization or pruning, enabling organizations to meet budget constraints without abandoning model quality. Engineers configure temperature, loss weighting, and dataset selection to ensure the student captures critical behaviors, sometimes blending hard labels with soft targets. Governance teams review distillation pipelines to confirm the teacher and student share licensing compatibility and that synthetic data generation respects privacy obligations. Because distillation can copy undesirable biases along with strengths, evaluations must confirm that risk mitigations remain effective in the distilled model.</p>"},{"location":"terms/knowledge-distillation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Distillation keeps model quality high while shrinking the model to run faster and cheaper.</li> <li>Engineer: Train a student network using teacher logits or generated traces so it approximates the teacher\u2019s function with fewer parameters.</li> </ul>"},{"location":"terms/knowledge-distillation/#examples","title":"Examples","text":"<p>Do - Document which teacher checkpoints and datasets produced each distilled release.</p> <p>Don't - Distill sensitive capabilities without reassessing safety performance on the student model.</p>"},{"location":"terms/knowledge-distillation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: efficiency, validity</li> <li>Risk notes: Students inherit teacher biases; missing evaluations can hide regressions introduced during compression.</li> </ul>"},{"location":"terms/knowledge-distillation/#relationships","title":"Relationships","text":"<ul> <li>Broader: model optimization</li> <li>Related: quantization, low-rank adaptation, evaluation</li> </ul>"},{"location":"terms/knowledge-distillation/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/knowledge-distillation.yml</code></p>"},{"location":"terms/kv-cache/","title":"kv cache","text":""},{"location":"terms/kv-cache/#kv-cache","title":"kv cache","text":"<p>Aliases: key-value cache, attention cache Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/kv-cache/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/kv-cache/#short-definition","title":"Short definition","text":"<p>Stored attention keys and values reused across decoding steps to speed sequential generation.</p>"},{"location":"terms/kv-cache/#long-definition","title":"Long definition","text":"<p>The KV cache holds intermediate key and value tensors produced during transformer attention so they can be reused on subsequent tokens without recomputing earlier layers. During autoregressive decoding, each new token only needs to attend to prior states; caching those states significantly reduces latency and compute, especially for long prompts or streaming responses. Production systems manage KV cache sizes carefully because they grow with context length, consuming memory on GPUs and influencing batch throughput. Engineers optimize cache eviction policies, quantization, or paged memory formats to balance cost and responsiveness. Governance and reliability teams monitor KV cache behavior to ensure no residual data persists longer than intended, particularly when serving multi-tenant workloads where prompts may contain sensitive information. Documenting cache configuration is part of operational playbooks for diagnosing performance regressions.</p>"},{"location":"terms/kv-cache/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: The KV cache is the reuse trick that keeps responses snappy even when conversations get long.</li> <li>Engineer: Per-layer tensors of attention keys/values stored between decoding steps to avoid recomputing full sequence context.</li> </ul>"},{"location":"terms/kv-cache/#examples","title":"Examples","text":"<p>Do - Audit GPU memory usage with and without KV caching to plan capacity for long-context workloads.</p> <p>Don't - Share KV cache contents across tenants without isolation controls.</p>"},{"location":"terms/kv-cache/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: efficiency, privacy</li> <li>Risk notes: Improper cache management can leak residual user data or trigger out-of-memory failures.</li> </ul>"},{"location":"terms/kv-cache/#relationships","title":"Relationships","text":"<ul> <li>Broader: inference optimization</li> <li>Narrower: paged attention</li> <li>Related: attention, context window, quantization</li> </ul>"},{"location":"terms/kv-cache/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/kv-cache.yml</code></p>"},{"location":"terms/log-probability/","title":"log probability","text":""},{"location":"terms/log-probability/#log-probability","title":"log probability","text":"<p>Aliases: logprob, token log probability Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/log-probability/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/log-probability/#short-definition","title":"Short definition","text":"<p>Logarithm of a token\u2019s probability, used to inspect model confidence and guide decoding tweaks.</p>"},{"location":"terms/log-probability/#long-definition","title":"Long definition","text":"<p>Log probability represents the natural logarithm of a token\u2019s predicted probability during generation. Because probabilities multiply across sequence lengths, working in log space avoids numerical underflow and makes it easier to compare relative likelihoods. Tooling that surfaces token-level logprobs helps practitioners debug why a model chose specific words, identify low-confidence spans for post-processing, and implement rejection sampling. Product teams use logprob thresholds to trigger fallbacks, escalate to humans, or annotate responses with confidence indicators. Engineers rely on logprobs to enforce constraints such as toxicity caps, apply repetition penalties, or calibrate beam search scores. Governance stakeholders treat logprob telemetry as part of audit trails, correlating low-confidence regions with incidents or hallucinations. Capturing and analyzing log probabilities thus supports transparency requirements and informs mitigation strategies when behavior drifts over time.</p>"},{"location":"terms/log-probability/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Log probabilities surface how confident the model was about each word, which can trigger reviews when confidence drops.</li> <li>Engineer: Natural-log scores from the softmax output that accumulate across tokens; useful for diagnostics, penalties, and rejection sampling.</li> </ul>"},{"location":"terms/log-probability/#examples","title":"Examples","text":"<p>Do - Flag completions where average logprob falls below a threshold and route them for human review.</p> <p>Don't - Expose raw logprob values to end users without context or calibration.</p>"},{"location":"terms/log-probability/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, validity</li> <li>Risk notes: Missing logprob telemetry makes it harder to audit why unsafe or incorrect outputs were produced.</li> </ul>"},{"location":"terms/log-probability/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: temperature, top-k sampling, evaluation</li> </ul>"},{"location":"terms/log-probability/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/log-probability.yml</code></p>"},{"location":"terms/low-rank-adaptation/","title":"low-rank adaptation","text":""},{"location":"terms/low-rank-adaptation/#low-rank-adaptation","title":"low-rank adaptation","text":"<p>Aliases: LoRA, low rank fine-tuning Categories: Optimization &amp; Efficiency Roles: Data Science &amp; Research, Engineering &amp; Platform Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/low-rank-adaptation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/low-rank-adaptation/#short-definition","title":"Short definition","text":"<p>Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.</p>"},{"location":"terms/low-rank-adaptation/#long-definition","title":"Long definition","text":"<p>Low-rank adaptation (LoRA) fine-tunes large language models by learning compact update matrices rather than adjusting full weight tensors. The method freezes the original model parameters and trains additive low-rank factors that capture task-specific shifts, reducing memory usage and compute costs. LoRA adapters can be merged into the base model for deployment or stored separately to toggle behaviors per tenant. This approach enables organizations to customize models using modest hardware, accelerate experimentation, and share adapters without distributing full proprietary checkpoints. Engineers manage rank choices, scaling factors, and target layers to balance quality with efficiency. Governance teams evaluate LoRA artifacts like traditional model versions, reviewing data provenance, licensing, and security implications. Because adapters can encode sensitive capabilities, access control and documentation remain essential\u2014particularly when multiple teams contribute adapters to a shared serving stack.</p>"},{"location":"terms/low-rank-adaptation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: LoRA lets teams customize giant models cheaply by training small plug-ins instead of retraining everything.</li> <li>Engineer: Freeze base weights, insert trainable low-rank matrices into attention or feed-forward layers, and fine-tune with minimal VRAM.</li> </ul>"},{"location":"terms/low-rank-adaptation/#examples","title":"Examples","text":"<p>Do - Track which datasets and objectives produced each LoRA adapter before sharing it across teams.</p> <p>Don't - Merge third-party adapters into production models without license verification.</p>"},{"location":"terms/low-rank-adaptation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: efficiency, accountability</li> <li>Risk notes: Unvetted adapters can override safety tuning or introduce licensed data without traceability.</li> </ul>"},{"location":"terms/low-rank-adaptation/#relationships","title":"Relationships","text":"<ul> <li>Broader: fine-tuning</li> <li>Related: quantization, distillation, guardrails</li> </ul>"},{"location":"terms/low-rank-adaptation/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/low-rank-adaptation.yml</code></p>"},{"location":"terms/ml-observability/","title":"ml observability","text":""},{"location":"terms/ml-observability/#ml-observability","title":"ml observability","text":"<p>Aliases: model observability, ai observability Categories: Operations &amp; Monitoring Roles: Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/ml-observability/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/ml-observability/#short-definition","title":"Short definition","text":"<p>Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.</p>"},{"location":"terms/ml-observability/#long-definition","title":"Long definition","text":"<p>ML observability applies observability principles to machine learning systems, collecting signals that describe data quality, model behavior, infrastructure health, and user outcomes. The discipline unifies telemetry such as latency, throughput, drift metrics, guardrail triggers, and human feedback so teams can diagnose issues quickly. Robust observability stacks ingest logs from preprocessing, inference, retrieval, and post-processing stages, correlating them with experiment metadata and deployment versions. Product owners reference observability dashboards to understand adoption and satisfaction, while engineers rely on them to root-cause regressions and capacity incidents. Governance programs require observable pipelines to demonstrate compliance with monitoring expectations in frameworks like the NIST AI RMF. Without observability, organizations struggle to detect bias, hallucinations, or safety incidents before they impact users. Investing in standardized logging, alerting, and runbooks enables proactive triage and continuous improvement.</p>"},{"location":"terms/ml-observability/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: ML observability provides the dashboards and alerts that show whether AI systems remain healthy and trustworthy.</li> <li>Engineer: Collect metrics, logs, and traces across data, model, and infra layers; stitch them to deployments for debugging and compliance.</li> </ul>"},{"location":"terms/ml-observability/#examples","title":"Examples","text":"<p>Do - Correlate drift alerts with retrieval metrics to pinpoint whether failures stem from data or model changes.</p> <p>Don't - Disable guardrail logging due to cost; missing records breaks incident response and compliance.</p>"},{"location":"terms/ml-observability/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: monitoring, accountability</li> <li>Risk notes: Insufficient observability hides safety incidents and undermines regulatory reporting obligations.</li> </ul>"},{"location":"terms/ml-observability/#relationships","title":"Relationships","text":"<ul> <li>Broader: ml ops</li> <li>Related: model drift, guardrails, evaluation</li> </ul>"},{"location":"terms/ml-observability/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>UK POST AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/ml-observability.yml</code></p>"},{"location":"terms/ml-ops/","title":"ml ops","text":""},{"location":"terms/ml-ops/#ml-ops","title":"ml ops","text":"<p>Aliases: mlops, machine learning operations Categories: Operations &amp; Monitoring Roles: Engineering &amp; Platform, Policy &amp; Risk, Security &amp; Trust Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/ml-ops/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/ml-ops/#short-definition","title":"Short definition","text":"<p>Operational discipline that manages ML models from experimentation through deployment and monitoring.</p>"},{"location":"terms/ml-ops/#long-definition","title":"Long definition","text":"<p>ML Ops adapts DevOps and data engineering practices to the lifecycle of machine learning systems. It encompasses experiment tracking, feature management, deployment automation, monitoring, incident response, and governance. Successful ML Ops programs unify teams across data science, engineering, and compliance so models move from prototype to production without manual heroics. Tooling includes model registries, CI/CD pipelines, automated evaluation gates, and observability platforms that watch for drift, data quality issues, and guardrail violations. Product organizations rely on ML Ops maturity to ship updates safely and respond quickly to incidents. Governance frameworks such as the NIST AI RMF expect documented ML Ops processes to demonstrate accountability, transparency, and continuous monitoring. Without disciplined ML Ops, models stagnate, degrade in accuracy, or fall out of compliance as the environment evolves.</p>"},{"location":"terms/ml-ops/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: ML Ops is the operating system that keeps production models reliable, compliant, and cost-efficient.</li> <li>Engineer: Integrate version control, CI/CD, registries, approvals, and monitoring to manage ML artifacts end to end.</li> </ul>"},{"location":"terms/ml-ops/#examples","title":"Examples","text":"<p>Do - Automate promotion from staging to production only after evaluation and guardrail checks pass.</p> <p>Don't - Deploy model updates outside the pipeline, bypassing logging and rollback controls.</p>"},{"location":"terms/ml-ops/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, monitoring</li> <li>Risk notes: Weak ML Ops processes increase the odds of untracked changes, bias reintroductions, and compliance failures.</li> </ul>"},{"location":"terms/ml-ops/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Narrower: ml observability, model drift</li> <li>Related: fine-tuning, evaluation, guardrails</li> </ul>"},{"location":"terms/ml-ops/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/ml-ops.yml</code></p>"},{"location":"terms/model-card/","title":"model card","text":""},{"location":"terms/model-card/#model-card","title":"model card","text":"<p>Aliases: model documentation, model datasheet Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers, Engineering &amp; Platform, Communications &amp; Enablement Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p> <p>Put it into practice</p> <p>Document changes using the Governance &amp; Risk section.</p>"},{"location":"terms/model-card/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/model-card/#short-definition","title":"Short definition","text":"<p>Standardized documentation describing a model\u2019s purpose, data, performance, and limitations.</p>"},{"location":"terms/model-card/#long-definition","title":"Long definition","text":"<p>A model card summarizes key information about an AI system, including intended use, training data sources, evaluation results (overall and across subgroups), known limitations, safety mitigations, and contact points for escalation. Model cards support transparency, reproducibility, and compliance by giving stakeholders a shared reference that travels with each release. Product managers and engineers co-author model cards during the development lifecycle, updating them after retraining, prompt changes, or mitigation work. Policy, legal, and communications teams rely on model cards to inform disclosures, customer messaging, and regulatory filings. Mature organizations treat model cards as living documents stored in version control, with automated checks verifying that new releases update relevant sections.</p>"},{"location":"terms/model-card/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A model card is the product brief that explains what the AI is for, how it was built, and where it might misfire.</li> <li>Engineer: Document data lineage, evaluation metrics, ethical considerations, and change history; keep cards versioned alongside code.</li> </ul>"},{"location":"terms/model-card/#examples","title":"Examples","text":"<p>Do - Include subgroup metrics and fairness assessments in every model card. - Link to evaluation artifacts, incident history, and responsible AI approvals.</p> <p>Don't - Publish a model without a card\u2014reviewers and regulators will ask for it. - Let model cards go stale; update them whenever prompts or data shift.</p>"},{"location":"terms/model-card/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: documentation, accountability</li> <li>Risk notes: Missing or outdated model cards hinder audits and can delay approvals or trigger compliance findings.</li> </ul>"},{"location":"terms/model-card/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: responsible ai, privacy impact assessment, evaluation</li> </ul>"},{"location":"terms/model-card/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/model-card.yml</code></p>"},{"location":"terms/model-drift/","title":"model drift","text":""},{"location":"terms/model-drift/#model-drift","title":"model drift","text":"<p>Aliases: distribution drift, concept drift Categories: Operations &amp; Monitoring, Governance &amp; Risk Roles: Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/model-drift/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/model-drift/#short-definition","title":"Short definition","text":"<p>Gradual mismatch between model assumptions and real-world data that degrades performance over time.</p>"},{"location":"terms/model-drift/#long-definition","title":"Long definition","text":"<p>Model drift arises when the data encountered in production no longer matches the distribution seen during training or evaluation. Changes in user behavior, regulations, adversarial inputs, or upstream systems can all erode accuracy and trustworthiness. Drift appears in multiple forms: data drift alters input characteristics, concept drift changes the meaning of target labels, and prior drift moves latent relationships within embeddings. Operations teams monitor dashboards for early signals using statistical tests, canary evaluations, and feedback loops. Product managers plan remediation playbooks that include prompt updates, retrieval refreshes, or new fine-tuning cycles. Governance frameworks require drift detection as part of continuous monitoring obligations, ensuring sensitive use cases receive timely reviews. Documenting drift incidents, response timelines, and residual risk assessments supports compliance and informs future model lifecycle decisions.</p>"},{"location":"terms/model-drift/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Model drift is the slow decay that happens when the world changes but the AI stays static.</li> <li>Engineer: Shift between training and production distributions; measured via statistical monitoring and addressed with retraining or prompt updates.</li> </ul>"},{"location":"terms/model-drift/#examples","title":"Examples","text":"<p>Do - Set drift thresholds and automated alerts tied to evaluation reruns for critical workflows.</p> <p>Don't - Ignore user feedback that signals degraded relevance or bias until after incidents occur.</p>"},{"location":"terms/model-drift/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: monitoring, validity</li> <li>Risk notes: Undetected drift can lead to policy violations, misaligned outputs, and regulatory non-compliance.</li> </ul>"},{"location":"terms/model-drift/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: evaluation, guardrails, observability</li> </ul>"},{"location":"terms/model-drift/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/model-drift.yml</code></p>"},{"location":"terms/model-governance/","title":"model governance","text":""},{"location":"terms/model-governance/#model-governance","title":"model governance","text":"<p>Aliases: ai governance, ml governance Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/model-governance/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/model-governance/#short-definition","title":"Short definition","text":"<p>Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.</p>"},{"location":"terms/model-governance/#long-definition","title":"Long definition","text":"<p>Model governance coordinates legal, risk, engineering, and product roles to oversee how AI systems are developed, deployed, and maintained. Core activities include documenting intended use, validating data sources, approving releases, monitoring for drift, and managing incidents. Governance frameworks\u2014such as the NIST AI RMF or ISO/IEC standards\u2014expect organizations to maintain traceability for models, prompts, datasets, and evaluation evidence. Committees or designated owners review changes, enforce segregation of duties, and ensure audits can reconstruct decisions. Product teams rely on governance guardrails to align with policy, while engineers integrate governance checkpoints into ML Ops pipelines. Without governance, AI initiatives accumulate technical debt, expose companies to regulatory penalties, and erode user trust. Mature governance balances innovation with accountability, enabling responsible scaling of AI capabilities.</p>"},{"location":"terms/model-governance/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Model governance keeps AI programs aligned with policy, compliance, and stakeholder expectations as they scale.</li> <li>Engineer: Define ownership, documentation, approvals, and monitoring requirements so every model change is auditable.</li> </ul>"},{"location":"terms/model-governance/#examples","title":"Examples","text":"<p>Do - Record decision logs and reviewers for each production model release.</p> <p>Don't - Deploy models without documenting purpose, controls, and evaluation results.</p>"},{"location":"terms/model-governance/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, risk_management</li> <li>Risk notes: Absent governance leads to untracked risks, inconsistent controls, and regulatory exposure.</li> </ul>"},{"location":"terms/model-governance/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai</li> <li>Narrower: ml ops, model drift, ai incident response</li> <li>Related: evaluation, alignment, guardrails</li> </ul>"},{"location":"terms/model-governance/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/model-governance.yml</code></p>"},{"location":"terms/model-interpretability/","title":"model interpretability","text":""},{"location":"terms/model-interpretability/#model-interpretability","title":"model interpretability","text":"<p>Aliases: interpretability, explainability Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Engineering &amp; Platform, Policy &amp; Risk, Legal &amp; Compliance, Product &amp; Program Managers Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-04)</p> <p>Put it into practice</p> <p>Sync with the Governance Dashboard to capture explanation plans.</p>"},{"location":"terms/model-interpretability/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/model-interpretability/#short-definition","title":"Short definition","text":"<p>Ability to explain how a model arrives at its predictions in ways stakeholders understand.</p>"},{"location":"terms/model-interpretability/#long-definition","title":"Long definition","text":"<p>Model interpretability encompasses methods and practices that reveal why an AI system produced a particular output. Techniques range from local explanations (SHAP, LIME, token attribution) to global summaries (feature importance, surrogate models) and inherently interpretable architectures. Interpretability supports debugging, fairness audits, regulatory compliance, and customer trust. Engineering teams integrate explanation tooling into evaluation pipelines, while policy and legal stakeholders determine the level of transparency required for different products or jurisdictions. Model interpretability should be paired with documentation, human review, and responsible communication to avoid overstating confidence or exposing sensitive features.</p>"},{"location":"terms/model-interpretability/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Interpretability lets us open the black box so customers, regulators, and teams know why decisions were made.</li> <li>Engineer: Use techniques like SHAP, integrated gradients, or counterfactuals to attribute predictions; log results for audits and debugging.</li> </ul>"},{"location":"terms/model-interpretability/#examples","title":"Examples","text":"<p>Do - Provide dashboards that show top contributing features for high-risk decisions. - Validate explanations with subject-matter experts to ensure they make sense.</p> <p>Don't - Offer explanations that contradict model behavior or hide uncertainty. - Release interpretability tooling without access controls when sensitive features are involved.</p>"},{"location":"terms/model-interpretability/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, accountability</li> <li>Risk notes: Lack of interpretability undermines legal defensibility and trust; inaccurate explanations can mislead stakeholders.</li> </ul>"},{"location":"terms/model-interpretability/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai</li> <li>Related: model card, algorithmic bias, evaluation</li> </ul>"},{"location":"terms/model-interpretability/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/model-interpretability.yml</code></p>"},{"location":"terms/overfitting/","title":"overfitting","text":""},{"location":"terms/overfitting/#overfitting","title":"overfitting","text":"<p>Aliases: model overfitting, overtraining Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/overfitting/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/overfitting/#short-definition","title":"Short definition","text":"<p>When a model memorizes training data patterns so closely that it performs poorly on new samples.</p>"},{"location":"terms/overfitting/#long-definition","title":"Long definition","text":"<p>Overfitting occurs when a model adapts too precisely to idiosyncrasies and noise within the training data, sacrificing its ability to generalize to unseen examples. The model appears highly accurate during training but fails when exposed to validation or production inputs, leading to misleading metrics and degraded user experience. Overly complex architectures, insufficient regularization, and limited or unrepresentative datasets increase the risk. Teams detect overfitting by monitoring gaps between training and validation performance, examining learning curves, and evaluating on hold-out or cross-validation splits. Mitigation tactics include collecting more diverse data, applying regularization (dropout, weight decay), simplifying architectures, performing early stopping, or augmenting inputs. Product managers rely on overfitting diagnostics when planning rollouts, while engineers baseline mitigations before deploying models into regulated environments where failures can carry compliance impacts.</p>"},{"location":"terms/overfitting/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Overfitting is why a model can ace practice problems but stumble in front of real customers.</li> <li>Engineer: Large generalization gap between training and validation metrics caused by excessive capacity or insufficient regularization; diagnose via held-out evaluations and learning curves.</li> </ul>"},{"location":"terms/overfitting/#examples","title":"Examples","text":"<p>Do - Track validation performance for every experiment and stop training when it plateaus or declines. - Collect additional samples from underrepresented user journeys to improve generalization.</p> <p>Don't - Ship a model based solely on training accuracy without checking external benchmarks. - Ignore signs of drift that indicate the model has effectively overfit to outdated distributions.</p>"},{"location":"terms/overfitting/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, data_quality</li> <li>Risk notes: Overfit models produce inconsistent, biased outputs that can erode user trust and violate performance commitments.</li> </ul>"},{"location":"terms/overfitting/#relationships","title":"Relationships","text":"<ul> <li>Broader: model training</li> <li>Related: cross-validation, bias-variance tradeoff, evaluation</li> </ul>"},{"location":"terms/overfitting/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/overfitting.yml</code></p>"},{"location":"terms/precision/","title":"precision","text":""},{"location":"terms/precision/#precision","title":"precision","text":"<p>Aliases: positive predictive value, ppv Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/precision/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/precision/#short-definition","title":"Short definition","text":"<p>Share of predicted positives that are actually correct for a given classifier.</p>"},{"location":"terms/precision/#long-definition","title":"Long definition","text":"<p>Precision measures how often a model\u2019s positive predictions are right. It is calculated as true positives divided by the sum of true positives and false positives. Precision is critical when false positives carry high costs\u2014such as flagging legitimate transactions as fraud or routing harmless content to safety reviewers. Product managers examine precision alongside recall to balance user trust and coverage, while engineers monitor precision across subgroups to detect drift and bias. Governance teams require precision reporting in regulated workflows where incorrect flags can create legal exposure. Precision is best interpreted in context with other metrics and operational goals, since maximizing it alone may reduce recall and harm users the model fails to detect.</p>"},{"location":"terms/precision/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Precision tells you how many of the alerts the AI raises are actually right.</li> <li>Engineer: TP / (TP + FP); inspect per-class and per-segment precision, and monitor trade-offs with recall and business cost.</li> </ul>"},{"location":"terms/precision/#examples","title":"Examples","text":"<p>Do - Track precision at the thresholds used in production, not just at 0.5. - Compare precision across demographics to uncover unfair false-positive rates.</p> <p>Don't - Optimize precision without checking the drop in recall. - Report a single precision value when different user segments behave differently.</p>"},{"location":"terms/precision/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, accountability</li> <li>Risk notes: Low precision increases false alarms, eroding trust and causing unnecessary interventions.</li> </ul>"},{"location":"terms/precision/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: recall, f1 score, confusion matrix</li> </ul>"},{"location":"terms/precision/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/precision.yml</code></p>"},{"location":"terms/privacy-impact-assessment/","title":"privacy impact assessment","text":""},{"location":"terms/privacy-impact-assessment/#privacy-impact-assessment","title":"privacy impact assessment","text":"<p>Aliases: pia, data protection impact assessment Categories: Governance &amp; Risk Roles: Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/privacy-impact-assessment/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/privacy-impact-assessment/#short-definition","title":"Short definition","text":"<p>Structured review that evaluates how a system collects, uses, and safeguards personal data.</p>"},{"location":"terms/privacy-impact-assessment/#long-definition","title":"Long definition","text":"<p>A privacy impact assessment (PIA) identifies and mitigates privacy risks before launching or materially changing a product, dataset, or model. The process documents data sources, lawful bases, retention policies, third-party sharing, and safeguards such as encryption or differential privacy. PIAs often involve cross-functional stakeholders\u2014legal, security, engineering, product, and compliance\u2014to ensure controls meet regulatory requirements like GDPR, CCPA, or sector-specific rules. For AI systems, PIAs examine training data provenance, prompt logging, telemetry retention, and user disclosure obligations. Findings feed into risk registers, customer communications, and incident response plans. Conducting PIAs early reduces costly redesigns and strengthens trust with regulators, customers, and internal reviewers.</p>"},{"location":"terms/privacy-impact-assessment/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A PIA is the privacy due diligence that keeps AI launches compliant and defensible.</li> <li>Engineer: Provide technical details on data flows, storage, and safeguards so legal can verify privacy controls.</li> </ul>"},{"location":"terms/privacy-impact-assessment/#examples","title":"Examples","text":"<p>Do - Trigger a PIA whenever new personal data sources or model capabilities are introduced. - Track mitigation tasks from the PIA in your product backlog until resolved.</p> <p>Don't - Treat PIAs as paperwork; revisit them after model updates or incidents. - Launch features that collect personal data without completing the assessment.</p>"},{"location":"terms/privacy-impact-assessment/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, documentation</li> <li>Risk notes: Skipping PIAs can violate legal obligations, resulting in fines or mandatory shutdowns.</li> </ul>"},{"location":"terms/privacy-impact-assessment/#relationships","title":"Relationships","text":"<ul> <li>Broader: model governance</li> <li>Related: differential privacy, incident response, guardrails</li> </ul>"},{"location":"terms/privacy-impact-assessment/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/privacy-impact-assessment.yml</code></p>"},{"location":"terms/privacy/","title":"privacy","text":""},{"location":"terms/privacy/#privacy","title":"privacy","text":"<p>Aliases: data privacy, information privacy Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/privacy/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/privacy/#short-definition","title":"Short definition","text":"<p>Principle of limiting data collection, use, and exposure to protect individuals\u2019 information.</p>"},{"location":"terms/privacy/#long-definition","title":"Long definition","text":"<p>Privacy in AI systems focuses on controlling how personal or sensitive data is collected, processed, and retained. Regulations such as GDPR, CCPA, and sector-specific laws establish legal obligations for transparency, consent, minimization, and user rights. In machine learning, privacy risks arise from training data, prompts, logs, and model outputs that may reveal personal information. Teams mitigate these risks through data minimization, access controls, anonymization, synthetic data, and privacy-enhancing technologies like differential privacy. Product leaders coordinate privacy reviews with legal counsel when launching new features, while engineers implement safeguards in data pipelines, storage, and logging. Governance programs track privacy impact assessments, retention schedules, and incident response plans. Maintaining rigorous privacy practices preserves user trust and avoids regulatory penalties, making it a foundational requirement for responsible AI deployments.</p>"},{"location":"terms/privacy/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Privacy keeps user data protected and ensures AI initiatives comply with laws and customer expectations.</li> <li>Engineer: Limit collection, apply technical controls, and document handling of personal data across training and inference workflows.</li> </ul>"},{"location":"terms/privacy/#examples","title":"Examples","text":"<p>Do - Run privacy impact assessments before ingesting new data sources for fine-tuning.</p> <p>Don't - Log raw prompts that contain personal identifiers without redaction or retention limits.</p>"},{"location":"terms/privacy/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, risk_management</li> <li>Risk notes: Weak privacy controls can expose personal data, triggering legal liability and loss of trust.</li> </ul>"},{"location":"terms/privacy/#relationships","title":"Relationships","text":"<ul> <li>Broader: responsible ai</li> <li>Related: guardrails, model governance, incident response</li> </ul>"},{"location":"terms/privacy/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/privacy.yml</code></p>"},{"location":"terms/prompt-engineering/","title":"prompt engineering","text":""},{"location":"terms/prompt-engineering/#prompt-engineering","title":"prompt engineering","text":"<p>Aliases: prompt design, prompt scripting Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p> <p>Put it into practice</p> <p>Dive into the Prompt Engineering Playbook for workflows and checklists.</p>"},{"location":"terms/prompt-engineering/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/prompt-engineering/#short-definition","title":"Short definition","text":"<p>Crafting and testing prompts to steer model behavior toward desired outcomes.</p>"},{"location":"terms/prompt-engineering/#long-definition","title":"Long definition","text":"<p>Prompt engineering involves designing the instructions, examples, and context provided to a language model so it produces reliable, policy-compliant responses. Practitioners experiment with phrasing, ordering, few-shot examples, and formatting cues to influence reasoning steps or output structure. The discipline has evolved from ad-hoc experimentation to a structured workflow that includes prompt libraries, evaluation harnesses, and version control. Prompt engineers collaborate with product, legal, and safety teams to encode business rules, tone, and disallowed behaviors, often in tandem with system prompts and guardrails. They also balance token budgets against performance, ensuring prompts fit within context windows while still providing the necessary guidance. Governance functions expect prompt changes to pass through review queues, since a seemingly small tweak can alter safety posture or introduce bias.</p>"},{"location":"terms/prompt-engineering/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Prompt engineering is how we teach the model to act like our product or policy experts.</li> <li>Engineer: Systematic design of prompt structure, exemplars, and metadata; evaluated via regression suites to manage behavior drift.</li> </ul>"},{"location":"terms/prompt-engineering/#examples","title":"Examples","text":"<p>Do - Version prompts and run automated evals before rolling them out to production flows.</p> <p>Don't - Copy prompts between domains without revisiting legal and safety requirements.</p>"},{"location":"terms/prompt-engineering/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, transparency</li> <li>Risk notes: Unreviewed prompt changes can bypass controls and result in unsafe or misleading outputs.</li> </ul>"},{"location":"terms/prompt-engineering/#relationships","title":"Relationships","text":"<ul> <li>Broader: human-in-the-loop</li> <li>Narrower: few-shot prompting, chain-of-thought prompting</li> <li>Related: system prompt, guardrails, evaluation</li> </ul>"},{"location":"terms/prompt-engineering/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>UK POST AI Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/prompt-engineering.yml</code></p>"},{"location":"terms/quantization/","title":"quantization","text":""},{"location":"terms/quantization/#quantization","title":"quantization","text":"<p>Aliases: model quantization, weight quantization Categories: Optimization &amp; Efficiency Roles: Data Science &amp; Research, Engineering &amp; Platform Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-09-18)</p>"},{"location":"terms/quantization/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> </ul>"},{"location":"terms/quantization/#short-definition","title":"Short definition","text":"<p>Technique that compresses model weights into lower-precision formats to shrink size and speed inference.</p>"},{"location":"terms/quantization/#long-definition","title":"Long definition","text":"<p>Quantization converts neural network parameters and activations from high-precision floating point representations (such as FP32) into lower bit-width formats (such as INT8 or INT4) to reduce memory footprint and accelerate inference. By mapping continuous values into a discrete set, quantization enables models to run on cost-sensitive hardware, deliver faster responses, and consume less energy, which is critical when deploying large language models at scale or on edge devices. Engineers choose between post-training quantization, which calibrates a frozen model on representative data, and quantization-aware training, which simulates low-precision behavior during fine-tuning to preserve accuracy. Careful evaluation is required to understand the trade-offs: aggressive quantization can introduce numerical instability, harm latency determinism, or amplify bias if calibration data under-represents certain groups. Successful programs pair quantization with monitoring, backstops such as higher-precision fallbacks, and documentation that makes these trade-offs explicit to stakeholders.</p>"},{"location":"terms/quantization/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A cost-control lever: shrink model footprints so you can serve more traffic on existing hardware.</li> <li>Engineer: Apply per-tensor or per-channel scaling, choose symmetric/asymmetric schemes, and validate perplexity and latency post-quantization.</li> </ul>"},{"location":"terms/quantization/#examples","title":"Examples","text":"<p>Do - Benchmark accuracy and latency before and after quantization to document the trade-offs. - Use representative calibration datasets that include edge cases and demographic variation.</p> <p>Don't - Quantize safety-critical models without fallback paths or runtime monitoring. - Assume INT4 settings will work across architectures without profiling.</p>"},{"location":"terms/quantization/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: efficiency, robustness, documentation</li> <li>Risk notes: Quantization can reduce accuracy or shift error distribution; record evaluations and obtain stakeholder sign-off.</li> </ul>"},{"location":"terms/quantization/#relationships","title":"Relationships","text":"<ul> <li>Broader: model optimization</li> <li>Narrower: post-training quantization, quantization-aware training</li> <li>Related: compression, distillation, hardware acceleration</li> </ul>"},{"location":"terms/quantization/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/quantization.yml</code></p>"},{"location":"terms/recall/","title":"recall","text":""},{"location":"terms/recall/#recall","title":"recall","text":"<p>Aliases: sensitivity, true positive rate Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers, Policy &amp; Risk Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/recall/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> </ul>"},{"location":"terms/recall/#short-definition","title":"Short definition","text":"<p>Share of actual positives a model successfully identifies.</p>"},{"location":"terms/recall/#long-definition","title":"Long definition","text":"<p>Recall measures how well a model finds all relevant cases by dividing true positives by the sum of true positives and false negatives. High recall is essential when missing a positive case is costly or dangerous, such as failing to flag offensive content or overlooking a critical defect. When teams adjust thresholds, recall often moves in the opposite direction of precision, so decision-makers must weigh acceptable trade-offs. Engineers analyze recall per subgroup to ensure equitable performance, while policy and product teams set recall targets aligned with risk appetite and regulatory requirements. Recall is commonly monitored in tandem with precision, F1 score, and confusion matrices to understand overall effectiveness.</p>"},{"location":"terms/recall/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Recall is how many of the real problems the AI actually catches.</li> <li>Engineer: TP / (TP + FN); evaluate recall across classes and user cohorts, and tune thresholds or sampling to meet coverage goals.</li> </ul>"},{"location":"terms/recall/#examples","title":"Examples","text":"<p>Do - Set minimum recall thresholds for safety-critical detections. - Monitor recall daily to catch regressions caused by data drift.</p> <p>Don't - Maximize recall without reviewing the spike in false positives. - Report aggregate recall without subgroup breakdowns.</p>"},{"location":"terms/recall/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, accountability</li> <li>Risk notes: Poor recall leaves high-risk events undetected, undermining safety promises and compliance obligations.</li> </ul>"},{"location":"terms/recall/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: precision, f1 score, confusion matrix</li> </ul>"},{"location":"terms/recall/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/recall.yml</code></p>"},{"location":"terms/red-teaming/","title":"red teaming","text":""},{"location":"terms/red-teaming/#red-teaming","title":"red teaming","text":"<p>Aliases: ai red teaming, adversarial testing Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Communications &amp; Enablement, Engineering &amp; Platform, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/red-teaming/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/red-teaming/#short-definition","title":"Short definition","text":"<p>Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.</p>"},{"location":"terms/red-teaming/#long-definition","title":"Long definition","text":"<p>Red teaming mobilizes interdisciplinary experts to craft adversarial prompts, scenarios, and data inputs that challenge an AI system\u2019s safeguards. The goal is to uncover failure modes\u2014such as unsafe content, confidential data leaks, or jailbreak exploits\u2014before attackers or end users discover them. Exercises blend automated probing, scripted attack playbooks, and human creativity. Findings feed into remediation plans for guardrails, prompts, training data, or escalation policies. Product leaders schedule recurring red-team cycles for high-risk surfaces, while engineers build tooling to log attempts, reproduce issues, and verify fixes. Governance teams treat red teaming as part of risk management, requiring documentation of scope, participants, severity ratings, and follow-up actions. In many jurisdictions, regulators expect evidence that red teaming has been performed for sensitive deployments, making it a core component of responsible AI programs.</p>"},{"location":"terms/red-teaming/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Red teaming is the pre-launch fire drill that exposes how the AI could fail or be abused.</li> <li>Engineer: Design adversarial prompts and automated probes, capture reproduction artifacts, and track mitigation work in the backlog.</li> </ul>"},{"location":"terms/red-teaming/#examples","title":"Examples","text":"<p>Do - Incorporate marginalized community perspectives when designing red-team scenarios.</p> <p>Don't - Close a red-team finding without documenting remediation owners and timelines.</p>"},{"location":"terms/red-teaming/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Skipping red teaming leaves blind spots that can result in public incidents or regulatory enforcement.</li> </ul>"},{"location":"terms/red-teaming/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: guardrails, alignment, incident response</li> </ul>"},{"location":"terms/red-teaming/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/red-teaming.yml</code></p>"},{"location":"terms/repetition-penalty/","title":"repetition penalty","text":""},{"location":"terms/repetition-penalty/#repetition-penalty","title":"repetition penalty","text":"<p>Aliases: anti-repetition penalty, token penalty Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-02)</p> <p>Put it into practice</p> <p>Use the Prompt Engineering Playbook to balance repetition controls.</p>"},{"location":"terms/repetition-penalty/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/repetition-penalty/#short-definition","title":"Short definition","text":"<p>Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.</p>"},{"location":"terms/repetition-penalty/#long-definition","title":"Long definition","text":"<p>A repetition penalty rescales token probabilities during decoding so words that have already appeared become less likely to repeat. Implementations typically divide or multiply logits by a penalty factor greater than one, discouraging the model from reusing identical phrases while preserving the rest of the distribution. Product teams enable repetition penalties to prevent user-facing chatbots from producing redundant or endless loops, particularly in multilingual or code-heavy contexts where repetition can spike. Engineers tune separate penalties for input prompts versus generated output, and combine them with stop sequences to honour formatting requirements. Governance stakeholders log penalty settings because altering them can invalidate safety and quality evaluations\u2014lowering the penalty risks repetitive harmful content, whereas an excessively high penalty may distort meaning or remove essential disclosures. Monitoring repetition metrics in production helps confirm the chosen value remains effective as models, prompts, or content domains evolve.</p>"},{"location":"terms/repetition-penalty/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Think of the repetition penalty as the guardrail that keeps conversations from getting stuck in loops.</li> <li>Engineer: Scale logits for previously used tokens by a factor (e.g., 1.1\u20131.2) before sampling to discourage repeats without breaking coherence.</li> </ul>"},{"location":"terms/repetition-penalty/#examples","title":"Examples","text":"<p>Do - Track repetition rate metrics alongside hallucination incidents after changing penalty values. - Differentiate penalties for system prompts versus user-visible responses.</p> <p>Don't - Set the penalty so high that critical disclaimers or citations are removed from answers. - Forget to document penalty changes when comparing evaluation runs.</p>"},{"location":"terms/repetition-penalty/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: Aggressive penalties can alter validated answer formats; coordinate with policy and QA before rollout.</li> </ul>"},{"location":"terms/repetition-penalty/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: temperature, top-p sampling, beam search</li> </ul>"},{"location":"terms/repetition-penalty/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/repetition-penalty.yml</code></p>"},{"location":"terms/reranking/","title":"reranking","text":""},{"location":"terms/reranking/#reranking","title":"reranking","text":"<p>Aliases: re-ranking, second-stage ranking Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/reranking/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/reranking/#short-definition","title":"Short definition","text":"<p>Step that refines retrieval results using a more precise but slower scoring model.</p>"},{"location":"terms/reranking/#long-definition","title":"Long definition","text":"<p>Reranking applies a secondary model to an initial set of retrieved documents to reorder them according to task-specific relevance. The first retrieval pass prioritizes speed, often using sparse or dense similarity search to gather dozens of candidates. A reranker\u2014frequently a cross-encoder or lightweight generative model\u2014then evaluates each candidate alongside the query to produce fine-grained scores. This approach improves precision, supports citation quality in RAG, and helps guardrails by pushing irrelevant or risky passages down the list. Engineers tune rerankers by limiting candidate set sizes, caching scores, and measuring latency budgets. Governance considerations include documenting model provenance, ensuring the reranker does not encode discriminatory bias, and verifying that safety filters still apply after reordering. When reranking is well managed, it improves both factual accuracy and user trust without incurring the cost of fully generative evaluation for every document in the corpus.</p>"},{"location":"terms/reranking/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Reranking lets us double-check the top answers so users see the most reliable sources first.</li> <li>Engineer: Second-stage scorer\u2014often a cross-encoder\u2014reorders retrieved candidates to boost precision while respecting latency limits.</li> </ul>"},{"location":"terms/reranking/#examples","title":"Examples","text":"<p>Do - Measure precision@k with and without reranking to justify added latency.</p> <p>Don't - Deploy rerankers trained on outdated data without monitoring for regressions in safety filters.</p>"},{"location":"terms/reranking/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Biased rerankers can systematically down-rank protected viewpoints or surface outdated information.</li> </ul>"},{"location":"terms/reranking/#relationships","title":"Relationships","text":"<ul> <li>Broader: retrieval</li> <li>Narrower: cross-encoder reranking</li> <li>Related: retrieval-augmented generation, chunking, guardrails</li> </ul>"},{"location":"terms/reranking/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/reranking.yml</code></p>"},{"location":"terms/responsible-ai/","title":"responsible ai","text":""},{"location":"terms/responsible-ai/#responsible-ai","title":"responsible ai","text":"<p>Aliases: trustworthy ai, ethical ai Categories: Governance &amp; Risk Roles: Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>concept</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/responsible-ai/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/responsible-ai/#short-definition","title":"Short definition","text":"<p>Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.</p>"},{"location":"terms/responsible-ai/#long-definition","title":"Long definition","text":"<p>Responsible AI encompasses the policies, technical controls, and cultural norms that guide how AI is built and deployed. It integrates principles such as fairness, transparency, accountability, privacy, and security into each phase of the model lifecycle. Organizations operationalize responsible AI through governance committees, risk assessments, red teaming, documentation standards, and inclusive design processes. Engineering teams implement safeguards like guardrails, evaluation suites, and monitoring to enforce these principles. Product and legal leaders translate regulatory requirements and stakeholder expectations into practical guardrails and disclosures. Responsible AI is not a single project but an ongoing discipline that adapts as technology and regulations evolve. By grounding innovations in responsible AI, organizations increase user trust, reduce liability, and create sustainable value.</p>"},{"location":"terms/responsible-ai/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Responsible AI ensures innovation progresses with safeguards that protect people and the business.</li> <li>Engineer: Embed fairness, safety, privacy, and accountability into data, model, and deployment workflows.</li> </ul>"},{"location":"terms/responsible-ai/#examples","title":"Examples","text":"<p>Do - Include responsible AI reviews in the release checklist for every high-impact feature.</p> <p>Don't - Treat responsible AI as a post-launch audit instead of a lifecycle commitment.</p>"},{"location":"terms/responsible-ai/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Ignoring responsible AI principles invites regulatory action, reputational harm, and inequitable outcomes.</li> </ul>"},{"location":"terms/responsible-ai/#relationships","title":"Relationships","text":"<ul> <li>Broader: artificial intelligence</li> <li>Narrower: model governance, alignment, guardrails</li> <li>Related: red teaming, evaluation, privacy</li> </ul>"},{"location":"terms/responsible-ai/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/responsible-ai.yml</code></p>"},{"location":"terms/retrieval-augmented-generation/","title":"retrieval-augmented generation","text":""},{"location":"terms/retrieval-augmented-generation/#retrieval-augmented-generation","title":"retrieval-augmented generation","text":"<p>Aliases: RAG, retrieval augmented generation Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-09-18)</p> <p>Put it into practice</p> <p>Consult the Category Explorer for end-to-end grounding guidance.</p>"},{"location":"terms/retrieval-augmented-generation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/retrieval-augmented-generation/#short-definition","title":"Short definition","text":"<p>Workflow that grounds a generative model with retrieved context before producing output.</p>"},{"location":"terms/retrieval-augmented-generation/#long-definition","title":"Long definition","text":"<p>Retrieval-augmented generation (RAG) combines information retrieval and text generation so that a model answers questions using relevant, up-to-date context instead of only relying on patterns memorized during pretraining. A retrieval component first decomposes the user query and searches a corpus\u2014via sparse, dense, or hybrid methods\u2014to surface the most relevant passages. Those passages are then packaged, often alongside metadata like source citations, and injected into the model prompt or context window prior to decoding. The approach reduces hallucinations, improves factual accuracy, and enables small models to compete with larger ones when grounded knowledge is more important than general fluency. Successful RAG systems invest in document chunking, embedding quality, reranking, and evaluation pipelines that monitor both retrieval recall and answer precision. Governance teams value RAG because it enables transparent sourcing and can encode organizational policies in the retrieval layer, but they also monitor for drift, stale content, and privacy leaks when sensitive documents are indexed.</p>"},{"location":"terms/retrieval-augmented-generation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Treat it as a guardrail: the system fetches trusted documents before the model speaks, so answers stay on-policy.</li> <li>Engineer: Pipeline = query rewriting, retriever (BM25, DPR, hybrid), reranker, and generator with retrieved context appended to the prompt.</li> </ul>"},{"location":"terms/retrieval-augmented-generation/#examples","title":"Examples","text":"<p>Do - Version and monitor the document index so you can trace outputs back to specific sources. - Evaluate retrieval recall and answer accuracy separately to isolate failure modes.</p> <p>Don't - Assume RAG removes the need for model fine-tuning or ongoing evaluation. - Index sensitive records without access controls or data minimization.</p>"},{"location":"terms/retrieval-augmented-generation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, data_quality, documentation</li> <li>Risk notes: Requires controls for stale data, source governance, and privacy when indexing internal corpora.</li> </ul>"},{"location":"terms/retrieval-augmented-generation/#relationships","title":"Relationships","text":"<ul> <li>Broader: grounded generation</li> <li>Narrower: hybrid RAG, agentic RAG</li> <li>Related: retrieval, chunking, reranking, hallucination</li> </ul>"},{"location":"terms/retrieval-augmented-generation/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/retrieval-augmented-generation.yml</code></p>"},{"location":"terms/retrieval/","title":"retrieval","text":""},{"location":"terms/retrieval/#retrieval","title":"retrieval","text":"<p>Aliases: information retrieval, retriever Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/retrieval/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/retrieval/#short-definition","title":"Short definition","text":"<p>Process of selecting relevant documents or vectors from a corpus in response to a query.</p>"},{"location":"terms/retrieval/#long-definition","title":"Long definition","text":"<p>Retrieval refers to the algorithms and infrastructure that locate the most relevant pieces of information when a user submits a query. Classical approaches use lexical signals like TF-IDF and BM25, while modern systems incorporate dense vector similarity, hybrid combinations, and reranking models tailored to the domain. Retrieval quality underpins search, question answering, and retrieval-augmented generation workflows: without high recall and precision, downstream models hallucinate or return stale policy guidance. Engineers design retrievers by choosing chunking strategies, embedding models, index types, and freshness policies. They also monitor latency, relevance metrics, and guardrails such as filtering sensitive content. Governance teams treat retrieval pipelines as data processors, verifying access controls, audit logging, and compliance with data minimization requirements. Strong retrieval discipline enables traceability by linking generated outputs back to cited sources.</p>"},{"location":"terms/retrieval/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Retrieval is the step that fetches trusted facts before the AI speaks, keeping answers grounded.</li> <li>Engineer: Implements query encoding, similarity search, and ranking to surface high-relevance passages for downstream consumers.</li> </ul>"},{"location":"terms/retrieval/#examples","title":"Examples","text":"<p>Do - Track recall@k and precision metrics separately to diagnose retrieval bottlenecks.</p> <p>Don't - Index sensitive documents without permissioning or automated redaction.</p>"},{"location":"terms/retrieval/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, transparency</li> <li>Risk notes: Weak retrieval controls can expose restricted records or feed outdated content into regulated workflows.</li> </ul>"},{"location":"terms/retrieval/#relationships","title":"Relationships","text":"<ul> <li>Broader: search</li> <li>Narrower: dense retrieval, lexical retrieval</li> <li>Related: retrieval-augmented generation, vector store, reranking</li> </ul>"},{"location":"terms/retrieval/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/retrieval.yml</code></p>"},{"location":"terms/roc-auc/","title":"roc auc","text":""},{"location":"terms/roc-auc/#roc-auc","title":"roc auc","text":"<p>Aliases: area under the roc curve, roc area Categories: Foundations Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/roc-auc/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/roc-auc/#short-definition","title":"Short definition","text":"<p>Metric summarizing binary classifier performance by measuring area under the ROC curve.</p>"},{"location":"terms/roc-auc/#long-definition","title":"Long definition","text":"<p>ROC AUC (Receiver Operating Characteristic Area Under the Curve) quantifies how well a binary classifier separates positive and negative classes across all possible decision thresholds. The ROC curve plots the true positive rate against the false positive rate; the area under that curve ranges from 0.5 (random) to 1.0 (perfect). Teams use ROC AUC when class distributions are imbalanced or when threshold selection will happen later in the product lifecycle. Engineers analyze the curve\u2019s shape to understand trade-offs between sensitivity and specificity, while product managers compare ROC AUC alongside business-focused metrics before launches. Governance and legal stakeholders review ROC AUC for fairness and regulatory reporting to ensure classifiers meet minimum performance standards before deployment.</p>"},{"location":"terms/roc-auc/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: ROC AUC shows how reliably a classifier can distinguish good from bad outcomes before you pick a final threshold.</li> <li>Engineer: Integrate true/false positive rates over thresholds; inspect curve segments to diagnose operating points and class imbalance.</li> </ul>"},{"location":"terms/roc-auc/#examples","title":"Examples","text":"<p>Do - Report ROC AUC alongside precision, recall, and confusion matrices for a holistic view. - Evaluate ROC AUC per subgroup to detect disparate impact before rollout.</p> <p>Don't - Rely solely on ROC AUC when decision thresholds are fixed and business costs are asymmetric. - Compare ROC AUC from different datasets without standardizing splits.</p>"},{"location":"terms/roc-auc/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: validity, transparency</li> <li>Risk notes: Ignoring ROC AUC trends can hide degradation that leads to unfair or unsafe decisions in production.</li> </ul>"},{"location":"terms/roc-auc/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: cross-validation, bias-variance tradeoff, algorithmic bias</li> </ul>"},{"location":"terms/roc-auc/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/roc-auc.yml</code></p>"},{"location":"terms/safety-evaluation/","title":"safety evaluation","text":""},{"location":"terms/safety-evaluation/#safety-evaluation","title":"safety evaluation","text":"<p>Aliases: safety testing, safety assessment Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers, Communications &amp; Enablement Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-02)</p>"},{"location":"terms/safety-evaluation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> </ul>"},{"location":"terms/safety-evaluation/#short-definition","title":"Short definition","text":"<p>Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.</p>"},{"location":"terms/safety-evaluation/#long-definition","title":"Long definition","text":"<p>Safety evaluations probe AI systems for dangerous or disallowed behavior, complementing performance metrics with targeted abuse, bias, and compliance tests. Teams blend automated classifiers, curated prompt suites, and expert reviews to measure toxicity, misinformation, self-harm encouragement, and other high-risk outcomes. Results feed into guardrail tuning, incident response plans, and launch gate decisions. Engineering and policy teams collaborate on coverage, ensuring critical user journeys and demographic perspectives are represented. Communications and legal stakeholders review findings to shape disclosures and mitigation commitments. Safety evaluations are continuous: regressions can surface after prompt changes, model updates, or content shifts, so organizations schedule recurring runs and capture evidence for audits. Failing to operationalize safety evaluations at scale exposes products to public incidents, regulatory scrutiny, and erosion of user trust.</p>"},{"location":"terms/safety-evaluation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Safety evaluation is the checkpoint that proves the AI won\u2019t violate our policies or harm users.</li> <li>Engineer: Execute targeted red-team suites, automated toxicity checks, and manual reviews; document thresholds, residual risk, and remediation plans.</li> </ul>"},{"location":"terms/safety-evaluation/#examples","title":"Examples","text":"<p>Do - Store evaluation artifacts alongside release notes for traceability. - Include marginalized community perspectives in the review panel and prompt set.</p> <p>Don't - Assume safety coverage automatically transfers when prompts or models change. - Treat a passing score as permanent\u2014schedule re-tests after meaningful updates.</p>"},{"location":"terms/safety-evaluation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, accountability</li> <li>Risk notes: Skipping or deferring safety evaluations invites policy breaches, public incidents, and enforcement actions.</li> </ul>"},{"location":"terms/safety-evaluation/#relationships","title":"Relationships","text":"<ul> <li>Broader: evaluation</li> <li>Related: red teaming, guardrails, incident response</li> </ul>"},{"location":"terms/safety-evaluation/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/safety-evaluation.yml</code></p>"},{"location":"terms/synthetic-data-evaluation/","title":"synthetic data evaluation","text":""},{"location":"terms/synthetic-data-evaluation/#synthetic-data-evaluation","title":"synthetic data evaluation","text":"<p>Aliases: synthetic data quality assessment, synthetic validation Categories: Operations &amp; Monitoring, Governance &amp; Risk Roles: Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-04)</p>"},{"location":"terms/synthetic-data-evaluation/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/synthetic-data-evaluation/#short-definition","title":"Short definition","text":"<p>Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.</p>"},{"location":"terms/synthetic-data-evaluation/#long-definition","title":"Long definition","text":"<p>Synthetic data evaluation verifies that generated datasets mirror the statistical properties of real data without leaking sensitive information. Teams measure fidelity (distribution similarity), utility (model performance when trained on synthetic data), privacy leakage (distance or matching tests), and bias (subgroup balance). Evaluations often combine automated metrics\u2014such as Wasserstein distance, nearest-neighbor tests, TSTR/TSNT\u2014and human or domain-expert reviews. Product and policy teams review results to decide whether synthetic data is acceptable for training, testing, or sharing. Documentation should capture evaluation methods, thresholds, and mitigation plans when issues arise.</p>"},{"location":"terms/synthetic-data-evaluation/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Synthetic data evaluation proves the generated data is both useful and safe before we rely on it.</li> <li>Engineer: Compute fidelity, utility, privacy, and bias metrics; compare against acceptance thresholds and log findings with dataset versions.</li> </ul>"},{"location":"terms/synthetic-data-evaluation/#examples","title":"Examples","text":"<p>Do - Use real-world benchmarks (TSTR) to confirm models trained on synthetic data generalize. - Validate privacy leakage with membership inference or nearest-neighbor tests.</p> <p>Don't - Assume synthetic data is safe without quantitative evaluation. - Ignore subgroup representation when assessing utility.</p>"},{"location":"terms/synthetic-data-evaluation/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: data_quality, privacy</li> <li>Risk notes: Poor evaluation can result in biased or leaky synthetic datasets reaching production or partners.</li> </ul>"},{"location":"terms/synthetic-data-evaluation/#relationships","title":"Relationships","text":"<ul> <li>Broader: synthetic data</li> <li>Related: differential privacy, fairness metrics, evaluation</li> </ul>"},{"location":"terms/synthetic-data-evaluation/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/synthetic-data-evaluation.yml</code></p>"},{"location":"terms/synthetic-data/","title":"synthetic data","text":""},{"location":"terms/synthetic-data/#synthetic-data","title":"synthetic data","text":"<p>Aliases: generated data, simulated data Categories: Governance &amp; Risk, Operations &amp; Monitoring Roles: Data Science &amp; Research, Engineering &amp; Platform, Policy &amp; Risk, Product &amp; Program Managers, Security &amp; Trust Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-02)</p>"},{"location":"terms/synthetic-data/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/synthetic-data/#short-definition","title":"Short definition","text":"<p>Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.</p>"},{"location":"terms/synthetic-data/#long-definition","title":"Long definition","text":"<p>Synthetic data recreates statistical properties of real datasets without exposing exact records. Teams generate it with generative models, simulations, or rule-based scripts to augment scarce examples, balance demographic representation, or share data across boundaries. When used responsibly, synthetic data accelerates experimentation and protects privacy; when mismanaged, it can amplify biases, leak sensitive patterns, or give a false sense of security. Product and data science teams validate that synthetic datasets preserve signal relevant to their tasks, while policy and security partners evaluate whether the generation process meets governance requirements. Documentation should cover source data lineage, generation methods, and evaluation metrics such as fidelity, utility, and privacy leakage. Synthetic data is most effective when paired with differential privacy, guardrails, and robust monitoring that detects drift between simulated and real-world behavior.</p>"},{"location":"terms/synthetic-data/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Synthetic data lets teams test and train quickly without always touching sensitive customer records.</li> <li>Engineer: Generate data via simulations or generative models; validate fidelity, diversity, and privacy leakage before using it in pipelines.</li> </ul>"},{"location":"terms/synthetic-data/#examples","title":"Examples","text":"<p>Do - Track utility metrics comparing model performance on synthetic versus real validation sets. - Label synthetic datasets clearly so downstream teams understand provenance.</p> <p>Don't - Assume synthetic data automatically removes bias\u2014measure subgroup impacts explicitly. - Share synthetic datasets externally without privacy and legal review.</p>"},{"location":"terms/synthetic-data/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: privacy, data_quality</li> <li>Risk notes: Poorly generated data can encode discriminatory patterns or expose sensitive distributions; pair releases with privacy and bias assessments.</li> </ul>"},{"location":"terms/synthetic-data/#relationships","title":"Relationships","text":"<ul> <li>Broader: data preprocessing</li> <li>Related: differential privacy, model drift, evaluation</li> </ul>"},{"location":"terms/synthetic-data/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Google ML Glossary</li> <li>UK POST AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/synthetic-data.yml</code></p>"},{"location":"terms/system-prompt/","title":"system prompt","text":""},{"location":"terms/system-prompt/#system-prompt","title":"system prompt","text":"<p>Aliases: system instruction, base prompt Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p> <p>Put it into practice</p> <p>Review the Prompt Engineering Playbook before shipping updates.</p>"},{"location":"terms/system-prompt/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/system-prompt/#short-definition","title":"Short definition","text":"<p>Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.</p>"},{"location":"terms/system-prompt/#long-definition","title":"Long definition","text":"<p>A system prompt is the preamble sent to a conversational model before user messages to establish behavior, policies, and capabilities. It can describe persona, writing style, safety instructions, tool usage rules, or escalation paths. Because the system prompt is prepended to every conversation turn, it shapes how the model interprets later inputs and resolves conflicts between user requests and organizational policy. Product teams iterate on system prompts to balance friendliness with compliance, while engineers version and test them like code, running regression suites to detect unexpectedly permissive outputs. Governance reviewers treat system prompts as formal policy artifacts: they require approval, change tracking, and alignment with risk controls such as prohibited content lists. Poorly maintained system prompts can drift, accumulate contradictory clauses, or leak internal policies if exposed, so teams pair them with automated linting and secret scanning.</p>"},{"location":"terms/system-prompt/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Think of the system prompt as the playbook that keeps the assistant on-brand and on-policy before it ever talks to a user.</li> <li>Engineer: Immutable prefix in the prompt stack that defines instructions and tool contracts; version-controlled for audits and evals.</li> </ul>"},{"location":"terms/system-prompt/#examples","title":"Examples","text":"<p>Do - Store system prompts in source control and run evaluations whenever they change.</p> <p>Don't - Embed sensitive credentials or private policies in the system prompt without access controls.</p>"},{"location":"terms/system-prompt/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, accountability</li> <li>Risk notes: Unreviewed system prompts can encode unsafe behaviors or leak confidential policy guidance.</li> </ul>"},{"location":"terms/system-prompt/#relationships","title":"Relationships","text":"<ul> <li>Broader: prompt engineering</li> <li>Narrower: safety prompt</li> <li>Related: guardrails, temperature, tool use</li> </ul>"},{"location":"terms/system-prompt/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>UK POST AI Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/system-prompt.yml</code></p>"},{"location":"terms/temperature/","title":"temperature","text":""},{"location":"terms/temperature/#temperature","title":"temperature","text":"<p>Aliases: sampling temperature, softmax temperature Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p> <p>Put it into practice</p> <p>Experiment with settings using the Prompt Engineering Playbook.</p>"},{"location":"terms/temperature/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/temperature/#short-definition","title":"Short definition","text":"<p>Decoding parameter that controls how random or deterministic a model\u2019s outputs are.</p>"},{"location":"terms/temperature/#long-definition","title":"Long definition","text":"<p>Temperature adjusts the softness of the probability distribution used when sampling tokens during decoding. A low temperature sharpens the distribution so the model consistently chooses the highest-probability token, producing deterministic, conservative responses. Higher temperatures flatten the distribution, increasing the likelihood of picking lower-probability tokens that introduce creativity but also raise the risk of incoherence or policy violations. Product teams tune temperature per use case\u2014customer support bots typically prefer lower settings, while brainstorming assistants tolerate higher ones. Engineers may pair temperature with top-k or nucleus sampling to constrain randomness while preserving diversity. Governance reviewers monitor temperature configurations because overly permissive settings can bypass guardrails validated at default values, leading to unpredictable behavior in production. Documenting temperature choices and evaluating them against safety scenarios is therefore part of responsible deployment.</p>"},{"location":"terms/temperature/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Temperature is the creativity dial\u2014turn it down for consistency, up for variety.</li> <li>Engineer: Softmax scaling factor applied prior to sampling; inversely proportional to distribution sharpness and deterministic behavior.</li> </ul>"},{"location":"terms/temperature/#examples","title":"Examples","text":"<p>Do - Run evaluation suites at multiple temperature settings to quantify factual accuracy trade-offs.</p> <p>Don't - Ship higher temperatures to production without revisiting safety and compliance test results.</p>"},{"location":"terms/temperature/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: High temperatures can invalidate prior safety assessments and increase hallucination incidents.</li> </ul>"},{"location":"terms/temperature/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Narrower: temperature annealing</li> <li>Related: top-k sampling, top-p sampling, hallucination</li> </ul>"},{"location":"terms/temperature/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/temperature.yml</code></p>"},{"location":"terms/token/","title":"token","text":""},{"location":"terms/token/#token","title":"token","text":"<p>Aliases: subword token, tokenized unit Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/token/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/token/#short-definition","title":"Short definition","text":"<p>Smallest unit of text a model processes after tokenization, such as a word fragment or character.</p>"},{"location":"terms/token/#long-definition","title":"Long definition","text":"<p>In language models a token is the discrete unit produced by a tokenizer before feeding data into the network. Depending on the tokenizer, a token might represent a whole word, a meaningful subword chunk, or even individual characters and punctuation. Models operate on token sequences rather than raw text, which makes token boundaries central to context window sizing, cost estimation, and latency planning. Tokens also drive how prompts and completions are billed in commercial APIs, where limits are expressed in tokens instead of characters. Engineers track token counts to avoid truncating prompts or spilling past context limits, while product teams translate token budgets into supported use cases and pricing. Understanding tokenization quirks helps explain why uncommon spellings, multilingual inputs, or code snippets can explode in length compared with natural language, affecting both accuracy and economics. Governance stakeholders rely on the deterministic nature of tokenization when auditing prompts and ensuring reproducibility across deployments.</p>"},{"location":"terms/token/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Think of tokens as the LEGO bricks that determine how long prompts can be and what they cost.</li> <li>Engineer: Tokenizer output units that set sequence length, influence embedding lookup, and bound context windows for inference/training.</li> </ul>"},{"location":"terms/token/#examples","title":"Examples","text":"<p>Do - Estimate prompt costs by counting tokens instead of characters before rolling out a pricing plan.</p> <p>Don't - Assume character length equals token length when enforcing guardrails or cost controls.</p>"},{"location":"terms/token/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: transparency, validity</li> <li>Risk notes: Incorrect token accounting can break context constraints, leading to truncated outputs and compliance failures.</li> </ul>"},{"location":"terms/token/#relationships","title":"Relationships","text":"<ul> <li>Broader: tokenization</li> <li>Related: context window, embedding, prompt engineering</li> </ul>"},{"location":"terms/token/#citations","title":"Citations","text":"<ul> <li>Google ML Glossary</li> <li>Hugging Face Tokenizers</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/token.yml</code></p>"},{"location":"terms/tool-use/","title":"tool use","text":""},{"location":"terms/tool-use/#tool-use","title":"tool use","text":"<p>Aliases: function calling, model tool invocation Categories: Agents &amp; Tooling Roles: Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/tool-use/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/tool-use/#short-definition","title":"Short definition","text":"<p>Pattern where a model selects external tools or functions to handle parts of a task.</p>"},{"location":"terms/tool-use/#long-definition","title":"Long definition","text":"<p>Tool use occurs when a language model defers specific subtasks to external functions, APIs, or agents that it selects based on the conversation. Platforms expose structured schemas describing available tools\u2014such as search, retrieval, math, or workflow actions\u2014and the model predicts when and how to call them. This architecture extends model capabilities beyond text generation, supporting grounded answers, transactions, and integrations with enterprise systems. Engineers design tool contracts, manage authentication, and implement result formatting so outputs slot back into the conversation. Product owners document which tools are enabled per experience and define escalation paths when tools fail. Governance teams treat tool catalogs as controlled assets: they require access reviews, logging, and guardrails to prevent unauthorized actions or data leakage. When implemented responsibly, tool use increases accuracy and reduces hallucinations, but it also introduces new attack surfaces and compliance obligations.</p>"},{"location":"terms/tool-use/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Tool use lets the assistant call trusted services\u2014think search or ticketing\u2014to deliver accurate, actionable answers.</li> <li>Engineer: Expose structured tool specs, let the model emit JSON arguments, execute the function, and feed results back into the dialogue loop.</li> </ul>"},{"location":"terms/tool-use/#examples","title":"Examples","text":"<p>Do - Log every tool invocation with inputs and outputs for auditing and debugging.</p> <p>Don't - Expose production tools without rate limits or abuse monitoring.</p>"},{"location":"terms/tool-use/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, transparency</li> <li>Risk notes: Poorly governed tool catalogs can trigger unauthorized actions or leak sensitive data through function interfaces.</li> </ul>"},{"location":"terms/tool-use/#relationships","title":"Relationships","text":"<ul> <li>Broader: agentic ai</li> <li>Narrower: retrieval tool, calculator tool</li> <li>Related: retrieval-augmented generation, system prompt, guardrails</li> </ul>"},{"location":"terms/tool-use/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>UK POST AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/tool-use.yml</code></p>"},{"location":"terms/top-k-sampling/","title":"top-k sampling","text":""},{"location":"terms/top-k-sampling/#top-k-sampling","title":"top-k sampling","text":"<p>Aliases: k-sampling, truncated sampling Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p> <p>Put it into practice</p> <p>See the Prompt Engineering Playbook for tuning tips.</p>"},{"location":"terms/top-k-sampling/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/top-k-sampling/#short-definition","title":"Short definition","text":"<p>Decoding method that samples from the k most probable next tokens to balance diversity and control.</p>"},{"location":"terms/top-k-sampling/#long-definition","title":"Long definition","text":"<p>Top-k sampling limits the candidate set during generation to the k tokens with the highest probabilities after applying the model\u2019s softmax distribution. By discarding the long tail of unlikely options, the technique keeps outputs coherent while preserving some variability relative to greedy decoding. Product teams use top-k to tune tone and creativity without inviting the unrestricted randomness of pure sampling. Engineers choose k values based on experimentation with validation prompts, often combining the method with temperature scaling or nucleus sampling to fine-tune randomness. Operationally, top-k is simple to implement and deterministic when paired with fixed seeds, which supports reproducibility requirements. Governance reviewers document chosen k values because they affect the likelihood of unsafe or off-policy responses; overly permissive settings can undo safety evaluations performed at lower k thresholds. As part of responsible deployment, teams monitor how k adjustments interact with guardrails, cost, and quality metrics across releases.</p>"},{"location":"terms/top-k-sampling/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A knob that keeps the model creative but still focused on the best few answers.</li> <li>Engineer: Truncate the probability distribution to the highest-probability k tokens before sampling to manage diversity versus determinism.</li> </ul>"},{"location":"terms/top-k-sampling/#examples","title":"Examples","text":"<p>Do - Evaluate customer support flows at k=1, 10, and 40 to understand trade-offs in tone and accuracy.</p> <p>Don't - Increase k in production without rerunning safety and bias evaluations.</p>"},{"location":"terms/top-k-sampling/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: Raising k expands behavioral variance and can expose users to unreviewed content patterns.</li> </ul>"},{"location":"terms/top-k-sampling/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: temperature, top-p sampling, greedy decoding</li> </ul>"},{"location":"terms/top-k-sampling/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Wikipedia AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/top-k-sampling.yml</code></p>"},{"location":"terms/top-p-sampling/","title":"top-p sampling","text":""},{"location":"terms/top-p-sampling/#top-p-sampling","title":"top-p sampling","text":"<p>Aliases: nucleus sampling, p-sampling Categories: LLM Core Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p> <p>Put it into practice</p> <p>See the Prompt Engineering Playbook for tuning tips.</p>"},{"location":"terms/top-p-sampling/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/top-p-sampling/#short-definition","title":"Short definition","text":"<p>Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.</p>"},{"location":"terms/top-p-sampling/#long-definition","title":"Long definition","text":"<p>Top-p sampling, also called nucleus sampling, builds a dynamic shortlist of candidate tokens whose cumulative probability exceeds a threshold p. Instead of fixing the number of options, the method adapts to the shape of the probability distribution: sharply peaked distributions produce small candidate sets, while flatter distributions expand the pool. This flexibility makes top-p useful for preserving coherence in confident contexts while still allowing creative variations when the model is less certain. Engineers tune the p value in tandem with temperature to achieve the desired balance between determinism and variety. Product teams rely on the technique for experiences like storytelling, marketing copy, or brainstorming where monotone responses are undesirable. Governance teams record chosen p values alongside evaluation evidence, recognizing that higher thresholds can surface content not vetted during safety reviews. Monitoring how p interacts with policy classifiers and guardrails is an integral part of ongoing risk management.</p>"},{"location":"terms/top-p-sampling/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: A probability threshold that keeps responses varied without wandering too far off message.</li> <li>Engineer: Accumulate token probabilities until they exceed p, normalize, then sample\u2014yielding adaptive candidate sets per step.</li> </ul>"},{"location":"terms/top-p-sampling/#examples","title":"Examples","text":"<p>Do - Test critical workflows at p values of 0.7, 0.9, and 0.95 to document behavioral differences.</p> <p>Don't - Use high p thresholds for regulated communications without updated compliance sign-off.</p>"},{"location":"terms/top-p-sampling/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: robustness, transparency</li> <li>Risk notes: Large nucleus thresholds can introduce unvetted behaviors and may invalidate safety testing baselines.</li> </ul>"},{"location":"terms/top-p-sampling/#relationships","title":"Relationships","text":"<ul> <li>Broader: decoding</li> <li>Related: temperature, top-k sampling, greedy decoding</li> </ul>"},{"location":"terms/top-p-sampling/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>Stanford HAI Brief Definitions</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/top-p-sampling.yml</code></p>"},{"location":"terms/vector-store/","title":"vector store","text":""},{"location":"terms/vector-store/#vector-store","title":"vector store","text":"<p>Aliases: vector database, embedding index Categories: Retrieval &amp; RAG Roles: Data Science &amp; Research, Engineering &amp; Platform, Product &amp; Program Managers Part of speech: <code>noun_phrase</code> Status: <code>draft</code> (Last reviewed: 2024-11-01)</p>"},{"location":"terms/vector-store/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Data Science &amp; Research: Incorporate the metric or method into evaluation pipelines.</li> <li>Engineering &amp; Platform: Document implementation requirements and operational caveats.</li> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> </ul>"},{"location":"terms/vector-store/#short-definition","title":"Short definition","text":"<p>Database optimized to store embeddings and execute similarity search over vectors.</p>"},{"location":"terms/vector-store/#long-definition","title":"Long definition","text":"<p>A vector store persists embeddings alongside metadata and exposes similarity search primitives such as nearest-neighbor queries, filtering, and hybrid scoring. Unlike relational databases, vector stores optimize for high-dimensional distance computations, using structures like HNSW graphs, IVF lists, or product quantization to balance recall and latency. They are a foundational component of retrieval-augmented generation platforms because they resolve which document chunks should accompany a prompt. Operational teams monitor vector stores for index freshness, replication, and backup coverage, while engineers tune them for sharding strategies, dimensionality alignment with embedding models, and filterable metadata schemas. Governance teams oversee access controls and deletion workflows to guarantee that personal or regulated data can be removed promptly. Properly instrumented vector stores also log which records contributed to an answer, supporting audit trails and user-facing citations.</p>"},{"location":"terms/vector-store/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Think of a vector store as the searchable memory that lets AI find related knowledge quickly.</li> <li>Engineer: Specialized database offering ANN search over embedding vectors with metadata filters and lifecycle management.</li> </ul>"},{"location":"terms/vector-store/#examples","title":"Examples","text":"<p>Do - Align vector dimensionality and distance metrics with the embedding model before bulk ingestion.</p> <p>Don't - Leave vector indexes unversioned, making it impossible to trace which data powered a response.</p>"},{"location":"terms/vector-store/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: accountability, data_quality</li> <li>Risk notes: Lax access controls or deletion policies can expose sensitive embeddings or violate retention requirements.</li> </ul>"},{"location":"terms/vector-store/#relationships","title":"Relationships","text":"<ul> <li>Broader: retrieval infrastructure</li> <li>Narrower: approximate nearest neighbor index</li> <li>Related: embedding, retrieval, chunking</li> </ul>"},{"location":"terms/vector-store/#citations","title":"Citations","text":"<ul> <li>Hugging Face Glossary</li> <li>Google ML Glossary</li> <li>NIST AI RMF Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/vector-store.yml</code></p>"},{"location":"terms/voice-cloning/","title":"voice cloning","text":""},{"location":"terms/voice-cloning/#voice-cloning","title":"voice cloning","text":"<p>Aliases: voice synthesis, speech cloning Categories: Foundations, Governance &amp; Risk Roles: Product &amp; Program Managers, Communications &amp; Enablement, Legal &amp; Compliance, Policy &amp; Risk, Security &amp; Trust Part of speech: <code>process</code> Status: <code>draft</code> (Last reviewed: 2024-11-03)</p>"},{"location":"terms/voice-cloning/#role-takeaways","title":"Role takeaways","text":"<ul> <li>Product &amp; Program Managers: Translate this concept into user impact and rollout plans.</li> <li>Communications &amp; Enablement: Align messaging, FAQs, and enablement materials using this definition.</li> <li>Legal &amp; Compliance: Assess contractual and regulatory obligations tied to this term.</li> <li>Policy &amp; Risk: Map the definition to governance controls and review checklists.</li> <li>Security &amp; Trust: Plan monitoring and abuse prevention scenarios influenced by this term.</li> </ul>"},{"location":"terms/voice-cloning/#short-definition","title":"Short definition","text":"<p>Technique that replicates a person\u2019s voice using generative models trained on audio samples.</p>"},{"location":"terms/voice-cloning/#long-definition","title":"Long definition","text":"<p>Voice cloning systems learn a speaker\u2019s vocal characteristics from recorded samples and synthesize new speech that mimics that voice. Modern approaches use encoder-decoder architectures, diffusion models, or transformer-based TTS pipelines conditioned on speaker embeddings. While voice cloning powers accessibility, localization, and creative tools, it also raises serious risks: impersonation, fraud, misinformation, and consent violations. Product teams must obtain clear user permissions and provide safeguards like watermarks or audible disclosures. Legal and policy teams assess compliance with biometric privacy laws and emerging deepfake regulations. Security groups monitor abuse signals and coordinate rapid takedowns when clones are misused. Transparency, usage logs, and red-team exercises are essential for trustworthy deployment.</p>"},{"location":"terms/voice-cloning/#audience-perspectives","title":"Audience perspectives","text":"<ul> <li>Exec: Voice cloning lets you generate speech that sounds like a real person\u2014but it needs strict guardrails.</li> <li>Engineer: Extract speaker embeddings, condition neural TTS or diffusion decoders, and enforce consent, watermarking, and usage logs.</li> </ul>"},{"location":"terms/voice-cloning/#examples","title":"Examples","text":"<p>Do - Require opt-in consent and verification before training on a person\u2019s voice. - Embed inaudible watermarks and provide detection tools to partners.</p> <p>Don't - Release cloning features without a response plan for malicious use. - Store raw recordings longer than necessary or without encryption.</p>"},{"location":"terms/voice-cloning/#governance","title":"Governance","text":"<ul> <li>NIST RMF tags: risk_management, transparency</li> <li>Risk notes: Misuse of voice cloning can lead to fraud, reputational harm, and regulatory penalties; strict access controls and auditing are mandatory.</li> </ul>"},{"location":"terms/voice-cloning/#relationships","title":"Relationships","text":"<ul> <li>Broader: generative ai</li> <li>Related: synthetic data, content moderation, incident response</li> </ul>"},{"location":"terms/voice-cloning/#citations","title":"Citations","text":"<ul> <li>NIST AI RMF Glossary</li> <li>Wikipedia AI Glossary</li> <li>UK POST AI Glossary</li> </ul> <p>License: CC BY-SA 4.0</p> <p>Source file: <code>data/terms/voice-cloning.yml</code></p>"}]}