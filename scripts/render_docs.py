"""Generate Markdown documentation from glossary YAML entries."""

from __future__ import annotations

import argparse
import sys
from collections import OrderedDict
from pathlib import Path
from typing import Any, Dict, List

CURRENT_DIR = Path(__file__).resolve().parent
REPO_ROOT = CURRENT_DIR.parent
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from glossary_utils import safe_load_path

HEADER_COMMENT = """<!--\n  This file is auto-generated by scripts/render_docs.py. Do not edit manually.\n-->"""
MKDOCS_PATH = REPO_ROOT / "site" / "mkdocs.yml"
NAV_START_MARKER = "# AUTOGENERATED TERMS START"
NAV_END_MARKER = "# AUTOGENERATED TERMS END"
CATEGORY_NAV_ORDER = [
    "Foundations",
    "LLM Core",
    "Retrieval & RAG",
    "Agents & Tooling",
    "Optimization & Efficiency",
    "Operations & Monitoring",
    "Governance & Risk",
]
ROLE_DIRECTORY = OrderedDict(
    (
        ("product", {
            "title": "Product & Program Managers",
            "summary": "Focus on user outcomes, feature scope, and launch readiness.",
        }),
        ("engineering", {
            "title": "Engineering & Platform",
            "summary": "Own model integration, infra, and technical debt.",
        }),
        ("data_science", {
            "title": "Data Science & Research",
            "summary": "Drive experimentation, measurement, and model improvement.",
        }),
        ("policy", {
            "title": "Policy & Risk",
            "summary": "Ensure responsible AI controls align with governance frameworks.",
        }),
        ("legal", {
            "title": "Legal & Compliance",
            "summary": "Evaluate regulatory exposure, contracts, and IP concerns.",
        }),
        ("security", {
            "title": "Security & Trust",
            "summary": "Safeguard data, access, and abuse prevention.",
        }),
        ("communications", {
            "title": "Communications & Enablement",
            "summary": "Craft messaging, disclosure, and stakeholder education.",
        }),
    )
)


def group_terms_by_category(terms: List[Dict[str, Any]]) -> OrderedDict[str, List[Dict[str, Any]]]:
    buckets: OrderedDict[str, List[Dict[str, Any]]] = OrderedDict((cat, []) for cat in CATEGORY_NAV_ORDER)
    extras: OrderedDict[str, List[Dict[str, Any]]] = OrderedDict()
    for term in terms:
        categories = term.get("categories", [])
        if not categories:
            continue
        primary = categories[0]
        if primary in buckets:
            buckets[primary].append(term)
        else:
            extras.setdefault(primary, []).append(term)
    return OrderedDict(list(buckets.items()) + list(extras.items()))


def normalize_term(term: str) -> str:
    slug = term.strip().lower().replace(" ", "-").replace("_", "-")
    while "--" in slug:
        slug = slug.replace("--", "-")
    return slug


def load_terms(data_dir: Path) -> List[Dict[str, Any]]:
    terms: List[Dict[str, Any]] = []
    for path in sorted(data_dir.glob("*.yml")):
        data = safe_load_path(path) or {}
        if not isinstance(data, dict):
            raise ValueError(f"Expected mapping in {path}")
        data["slug"] = normalize_term(str(data.get("term", path.stem)))
        data["_source_file"] = str(path)
        terms.append(data)
    terms.sort(key=lambda entry: entry.get("term", "").lower())
    return terms


def render_term_page(term: Dict[str, Any]) -> str:
    aliases = term.get("aliases") or []
    categories = term.get("categories") or []
    roles = term.get("roles") or []
    governance = term.get("governance") or {}
    relationships = term.get("relationships") or {}
    examples = term.get("examples") or {}
    audiences = term.get("audiences") or {}

    lines: List[str] = [HEADER_COMMENT, "", f"# {term.get('term', '').strip()}", ""]

    metadata_bits: List[str] = []
    if aliases:
        metadata_bits.append(f"**Aliases:** {', '.join(aliases)}")
    if categories:
        metadata_bits.append(f"**Categories:** {', '.join(categories)}")
    if roles:
        role_names = [ROLE_DIRECTORY.get(role, {}).get("title", role) for role in roles]
        metadata_bits.append(f"**Roles:** {', '.join(role_names)}")
    metadata_bits.append(f"**Part of speech:** `{term.get('part_of_speech', '—')}`")
    metadata_bits.append(
        f"**Status:** `{term.get('status', '—')}` (Last reviewed: {term.get('last_reviewed', '—')})"
    )
    lines.append("\n".join(metadata_bits))
    lines.append("")

    short_def = term.get("short_def", "")
    if short_def:
        lines.extend(["## Short definition", short_def.strip(), ""])

    long_def = term.get("long_def", "")
    if long_def:
        lines.extend(["## Long definition", long_def.strip(), ""])

    if audiences:
        exec_view = audiences.get("exec")
        eng_view = audiences.get("engineer")
        lines.append("## Audience perspectives")
        if exec_view:
            lines.append(f"- **Exec:** {exec_view.strip()}")
        if eng_view:
            lines.append(f"- **Engineer:** {eng_view.strip()}")
        lines.append("")

    if examples:
        do_examples = examples.get("do") or []
        dont_examples = examples.get("dont") or []
        lines.append("## Examples")
        if do_examples:
            lines.append("**Do**")
            for item in do_examples:
                lines.append(f"- {item}")
            lines.append("")
        if dont_examples:
            lines.append("**Don't**")
            for item in dont_examples:
                lines.append(f"- {item}")
            lines.append("")

    if governance:
        tags = governance.get("nist_rmf_tags") or []
        risk_notes = governance.get("risk_notes")
        lines.append("## Governance")
        if tags:
            lines.append(f"- **NIST RMF tags:** {', '.join(tags)}")
        if risk_notes:
            lines.append(f"- **Risk notes:** {risk_notes.strip()}")
        lines.append("")

    relationship_sections = []
    for key, label in (("broader", "Broader"), ("narrower", "Narrower"), ("related", "Related")):
        values = relationships.get(key) or []
        if values:
            relationship_sections.append(f"- **{label}:** {', '.join(values)}")
    if relationship_sections:
        lines.append("## Relationships")
        lines.extend(relationship_sections)
        lines.append("")

    citations = term.get("citations") or []
    if citations:
        lines.append("## Citations")
        for citation in citations:
            source = citation.get("source", "Unknown source").strip()
            url = citation.get("url")
            if url:
                lines.append(f"- [{source}]({url})")
            else:
                lines.append(f"- {source}")
        lines.append("")

    license_name = term.get("license")
    if license_name:
        lines.append(f"_License: {license_name}_")
        lines.append("")

    source_file = term.get("_source_file")
    if source_file:
        rel_path = Path(source_file).as_posix()
        lines.append(f"_Source file: `{rel_path}`_")
        lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def render_index_page(terms: List[Dict[str, Any]]) -> str:
    lines: List[str] = [HEADER_COMMENT, "", "# Glossary Terms", ""]
    lines.append(f"Total entries: {len(terms)}")
    lines.append("")
    for term in terms:
        short_def = term.get("short_def", "").strip()
        lines.append(
            f"- [{term.get('term')}](./{term['slug']}.md) — {short_def}" if short_def else f"- [{term.get('term')}](./{term['slug']}.md)"
        )
    lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def render_roles_page(terms: List[Dict[str, Any]]) -> str:
    lines: List[str] = [HEADER_COMMENT, "", "# Role Starter Packs", ""]
    lines.append(
        "Guidance for common stakeholder groups. Each section links to the most relevant terms based on "
        "category and role tags."
    )
    lines.append("")

    terms_by_role: Dict[str, List[Dict[str, Any]]] = {role: [] for role in ROLE_DIRECTORY}
    for term in terms:
        for role in term.get("roles", []):
            if role in terms_by_role:
                terms_by_role[role].append(term)

    for role, meta in ROLE_DIRECTORY.items():
        role_terms = terms_by_role.get(role, [])
        if not role_terms:
            continue
        lines.append(f"## {meta['title']}")
        lines.append(meta["summary"])
        lines.append("")
        lines.append("### Focus areas")
        category_counts: Dict[str, int] = {}
        for term in role_terms:
            for category in term.get("categories", []):
                category_counts[category] = category_counts.get(category, 0) + 1
        for category, count in sorted(category_counts.items(), key=lambda item: (-item[1], item[0])):
            lines.append(f"- {category} ({count} term{'s' if count != 1 else ''})")
        lines.append("")
        lines.append("### Recommended terms")
        role_terms.sort(key=lambda item: item.get("term", "").lower())
        for term in role_terms:
            lines.append(
                f"- [{term['term']}](terms/{term['slug']}.md) — {term.get('short_def', '').strip()}"
            )
        lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def render_categories_page(terms: List[Dict[str, Any]]) -> str:
    lines: List[str] = [HEADER_COMMENT, "", "# Category Explorer", ""]
    lines.append("Browse terms grouped by focus area. Categories align with navigation and search filters.")
    lines.append("")

    grouped = group_terms_by_category(terms)

    for category, bucket in grouped.items():
        if not bucket:
            continue
        lines.append(f"## {category}")
        description = {
            "Foundations": "Core machine learning concepts that underpin modern AI systems.",
            "LLM Core": "Mechanics of transformers, prompting, decoding, and language model internals.",
            "Retrieval & RAG": "Tools for grounding models with external knowledge and search infrastructure.",
            "Agents & Tooling": "Agent patterns, tool invocation, and orchestration strategies.",
            "Optimization & Efficiency": "Techniques for scaling inference and training effectively.",
            "Operations & Monitoring": "Operational playbooks for running AI in production.",
            "Governance & Risk": "Policies, controls, and assessments that ensure responsible AI."
        }.get(category, "")
        if description:
            lines.append(description)
            lines.append("")
        bucket.sort(key=lambda item: item.get("term", "").lower())
        for term in bucket:
            tag = ", ".join([ROLE_DIRECTORY.get(role, {}).get("title", role) for role in term.get("roles", [])])
            meta = f" — {tag}" if tag else ""
            lines.append(f"- [{term['term']}](terms/{term['slug']}.md){meta}")
        lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def write_docs(terms: List[Dict[str, Any]], docs_dir: Path) -> None:
    docs_dir.mkdir(parents=True, exist_ok=True)

    for path in docs_dir.glob("*.md"):
        path.unlink()

    for term in terms:
        destination = docs_dir / f"{term['slug']}.md"
        destination.write_text(render_term_page(term), encoding="utf-8")

    index_path = docs_dir / "index.md"
    index_path.write_text(render_index_page(terms), encoding="utf-8")

    roles_path = docs_dir.parent / "roles.md"
    roles_path.write_text(render_roles_page(terms), encoding="utf-8")

    categories_path = docs_dir.parent / "categories.md"
    categories_path.write_text(render_categories_page(terms), encoding="utf-8")


def update_mkdocs_nav(terms: List[Dict[str, Any]], mkdocs_path: Path = MKDOCS_PATH) -> None:
    if not mkdocs_path.exists():
        return

    content = mkdocs_path.read_text(encoding="utf-8")
    lines = content.splitlines()

    try:
        start_idx = next(i for i, line in enumerate(lines) if NAV_START_MARKER in line)
        end_idx = next(i for i, line in enumerate(lines) if NAV_END_MARKER in line)
    except StopIteration:
        print("Nav markers not found in mkdocs.yml; skipping nav update.")
        return

    if end_idx <= start_idx:
        print("Invalid nav marker positions; skipping nav update.")
        return

    marker_line = lines[start_idx]
    indent = marker_line[: marker_line.index(NAV_START_MARKER)]

    grouped: OrderedDict[str, List[Dict[str, Any]]] = OrderedDict((cat, []) for cat in CATEGORY_NAV_ORDER)
    extras: OrderedDict[str, List[Dict[str, Any]]] = OrderedDict()

    for term in terms:
        categories = term.get("categories") or []
        primary = categories[0] if categories else "Other"
        bucket = grouped.get(primary)
        if bucket is None:
            bucket = extras.setdefault(primary, [])
        bucket.append(term)

    nav_entries: List[str] = []
    for category, bucket in list(grouped.items()) + list(extras.items()):
        if not bucket:
            continue
        bucket.sort(key=lambda item: item.get("term", "").lower())
        nav_entries.append(f"{indent}- {category}:")
        for entry in bucket:
            nav_entries.append(f"{indent}  - {entry['term']}: terms/{entry['slug']}.md")

    updated = lines[: start_idx + 1] + nav_entries + lines[end_idx:]
    mkdocs_path.write_text("\n".join(updated) + "\n", encoding="utf-8")


def main(argv: List[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description="Render Markdown docs from glossary YAML entries")
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=Path("data/terms"),
        help="Directory containing YAML term files",
    )
    parser.add_argument(
        "--docs-dir",
        type=Path,
        default=Path("site/docs/terms"),
        help="Directory where Markdown files should be written",
    )
    args = parser.parse_args(argv)

    terms = load_terms(args.data_dir)
    if not terms:
        print(f"No term files found in {args.data_dir}", file=sys.stderr)
        return 1

    write_docs(terms, args.docs_dir)
    update_mkdocs_nav(terms)
    print(f"Rendered {len(terms)} term page(s) to {args.docs_dir}")
    return 0


if __name__ == "__main__":
    sys.exit(main())
