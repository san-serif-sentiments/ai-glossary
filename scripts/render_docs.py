"""Generate Markdown documentation from glossary YAML entries."""

from __future__ import annotations

import argparse
import sys
from collections import OrderedDict
from pathlib import Path
from typing import Any, Dict, List

CURRENT_DIR = Path(__file__).resolve().parent
REPO_ROOT = CURRENT_DIR.parent
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from glossary_utils import safe_load_path

HEADER_COMMENT = """<!--\n  This file is auto-generated by scripts/render_docs.py. Do not edit manually.\n-->"""
MKDOCS_PATH = REPO_ROOT / "site" / "mkdocs.yml"
NAV_START_MARKER = "# AUTOGENERATED TERMS START"
NAV_END_MARKER = "# AUTOGENERATED TERMS END"
CATEGORY_NAV_ORDER = [
    "Foundations",
    "LLM Core",
    "Retrieval & RAG",
    "Agents & Tooling",
    "Optimization & Efficiency",
    "Operations & Monitoring",
    "Governance & Risk",
]
ROLE_DIRECTORY = OrderedDict(
    (
        ("product", {
            "title": "Product & Program Managers",
            "summary": "Focus on user outcomes, feature scope, and launch readiness.",
        }),
        ("engineering", {
            "title": "Engineering & Platform",
            "summary": "Own model integration, infra, and technical debt.",
        }),
        ("data_science", {
            "title": "Data Science & Research",
            "summary": "Drive experimentation, measurement, and model improvement.",
        }),
        ("policy", {
            "title": "Policy & Risk",
            "summary": "Ensure responsible AI controls align with governance frameworks.",
        }),
        ("legal", {
            "title": "Legal & Compliance",
            "summary": "Evaluate regulatory exposure, contracts, and IP concerns.",
        }),
        ("security", {
            "title": "Security & Trust",
            "summary": "Safeguard data, access, and abuse prevention.",
        }),
        ("communications", {
            "title": "Communications & Enablement",
            "summary": "Craft messaging, disclosure, and stakeholder education.",
        }),
    )
)

ROLE_TAKEAWAYS = {
    "product": "Translate this concept into user impact and rollout plans.",
    "engineering": "Document implementation requirements and operational caveats.",
    "data_science": "Incorporate the metric or method into evaluation pipelines.",
    "policy": "Map the definition to governance controls and review checklists.",
    "legal": "Assess contractual and regulatory obligations tied to this term.",
    "security": "Plan monitoring and abuse prevention scenarios influenced by this term.",
    "communications": "Align messaging, FAQs, and enablement materials using this definition.",
}

ROLE_LEARNING_PATHS = {
    "product": [
        "Skim the Governance & Risk category to learn which terms drive launch checklists.",
        "Bookmark three model or prompt concepts that influence roadmap trade-offs.",
        "Schedule a debrief with policy partners to align on escalation triggers.",
    ],
    "engineering": [
        "Start with LLM Core mechanics to understand knobs that affect reliability.",
        "Review Operations & Monitoring entries and note which metrics to add to dashboards.",
        "Pair with policy leads on governance terms that require instrumentation support.",
    ],
    "data_science": [
        "Refresh foundational metrics (precision, recall, ROC AUC) to ensure evaluation coverage.",
        "Study Optimization & Efficiency techniques to plan future experiments.",
        "Document how governance-aligned metrics will be reported to stakeholders.",
    ],
    "policy": [
        "Read algorithmic governance terms to map glossary content to internal controls.",
        "Identify three technical concepts to discuss with engineering for upcoming reviews.",
        "Draft guidance for disclosure or transparency using relevant glossary examples.",
    ],
    "legal": [
        "Focus on Responsible AI and compliance-related terms to spot regulatory hooks.",
        "Cross-reference privacy-focused entries with current policy language.",
        "Capture open questions for the next risk or contract review cycle.",
    ],
    "security": [
        "Study Operations & Monitoring entries for logging and detection requirements.",
        "Review tool and agent terminology to assess abuse surface areas.",
        "Coordinate with product/legal on incident response and disclosure expectations.",
    ],
    "communications": [
        "Scan definitions tagged for Governance & Risk to prep stakeholder messaging.",
        "Collect relatable examples from the glossary to use in enablement materials.",
        "Draft a narrative that links technical terms to user-facing value and risk mitigations.",
    ],
}

ROLE_PRACTICE_TASKS = {
    "product": [
        "Review the glossary search filtered to product + governance and log two takeaways in your launch checklist.",
        "Pair with engineering to confirm which guardrails or prompts need updates before feature freeze.",
    ],
    "engineering": [
        "Use the search filters (engineering + operations) and capture metrics to wire into observability dashboards.",
        "Document deployment actions in your runbook using examples referenced in the glossary.",
    ],
    "data_science": [
        "Select one evaluation metric and one mitigation technique from the glossary for your next experiment brief.",
        "Record baseline measurements tied to the definitions before shipping changes.",
    ],
    "policy": [
        "Draft a review checklist referencing the top three governance terms surfaced in the backlog.",
        "Map required disclosures for the next launch memo using linked glossary examples.",
    ],
    "legal": [
        "Compare contractual language with glossary definitions for privacy and retention to spot gaps.",
        "Flag terms needing legal guidance through the intake form so questions are tracked.",
    ],
    "security": [
        "Audit incident response and tool-use entries to confirm abuse-prevention controls are documented.",
        "Plan a tabletop exercise using the glossary's scenario examples and log outcomes.",
    ],
    "communications": [
        "Draft an FAQ using glossary language to keep messaging consistent across teams.",
        "Tag enablement tickets with relevant glossary links so stakeholders can self-serve context.",
    ],
}

CALLS_TO_ACTION = {
    "prompt-engineering": "Dive into the [Prompt Engineering Playbook](../prompting.md) for workflows and checklists.",
    "system-prompt": "Review the [Prompt Engineering Playbook](../prompting.md) before shipping updates.",
    "temperature": "Experiment with settings using the [Prompt Engineering Playbook](../prompting.md).",
    "top-k-sampling": "See the [Prompt Engineering Playbook](../prompting.md) for tuning tips.",
    "top-p-sampling": "See the [Prompt Engineering Playbook](../prompting.md) for tuning tips.",
    "repetition-penalty": "Use the [Prompt Engineering Playbook](../prompting.md) to balance repetition controls.",
    "beam-search": "Pair with the [Prompt Engineering Playbook](../prompting.md) when crafting deterministic flows.",
    "retrieval-augmented-generation": "Consult the [Category Explorer](../categories.md#retrieval--rag) for end-to-end grounding guidance.",
    "fairness-metrics": "Coordinate with the [Role Starter Packs](../roles.md#policy--risk) for governance actions.",
    "model-card": "Document changes using the [Governance & Risk section](../categories.md#governance--risk).",
    "algorithmic-bias": "Run the [Governance Dashboard](../governance-dashboard.md) checklist before launch.",
    "model-interpretability": "Sync with the [Governance Dashboard](../governance-dashboard.md) to capture explanation plans.",
    "data-retention": "Map retention updates to the [Governance Dashboard](../governance-dashboard.md).",
    "content-moderation": "Reference the [Governance Dashboard](../governance-dashboard.md) for monitoring obligations.",
}

CATEGORY_APPLY_GUIDANCE = {
    "Governance & Risk": [
        "Map this term to the governance dashboard and record accountable owners in the backlog.",
        "Review current regulatory guidance or internal policy notes linked from the resources page before sign-off.",
    ],
    "LLM Core": [
        "Prototype behaviour changes in a sandbox notebook and capture prompt or decoding settings for others.",
        "Share findings with enablement so downstream teams understand model implications.",
    ],
    "Operations & Monitoring": [
        "Instrument dashboards or alerts that reflect the metrics highlighted in this definition.",
        "Update incident response or on-call runbooks with the glossary's do/don't scenarios.",
    ],
    "Retrieval & RAG": [
        "Validate retrieval quality using the evaluation guidance referenced in this entry.",
        "Ensure knowledge sources named here appear in your data governance inventory.",
    ],
    "Agents & Tooling": [
        "Audit exposed tools against the safeguards described and document approval paths.",
        "Test hand-offs with human reviewers to confirm the safety expectations captured here are met.",
    ],
    "Optimization & Efficiency": [
        "Record before-and-after performance metrics when applying this optimisation technique.",
        "Document trade-offs for product and policy partners using the glossary's language.",
    ],
    "Foundations": [
        "Add this concept to onboarding materials so teammates share a common baseline.",
        "Link supporting research or documentation in your internal wiki for deeper study.",
    ],
}


def group_terms_by_category(terms: List[Dict[str, Any]]) -> OrderedDict[str, List[Dict[str, Any]]]:
    buckets: OrderedDict[str, List[Dict[str, Any]]] = OrderedDict((cat, []) for cat in CATEGORY_NAV_ORDER)
    extras: OrderedDict[str, List[Dict[str, Any]]] = OrderedDict()
    for term in terms:
        categories = term.get("categories", [])
        if not categories:
            continue
        primary = categories[0]
        if primary in buckets:
            buckets[primary].append(term)
        else:
            extras.setdefault(primary, []).append(term)
    return OrderedDict(list(buckets.items()) + list(extras.items()))


def normalize_term(term: str) -> str:
    slug = term.strip().lower().replace(" ", "-").replace("_", "-")
    while "--" in slug:
        slug = slug.replace("--", "-")
    return slug


def load_terms(data_dir: Path) -> List[Dict[str, Any]]:
    terms: List[Dict[str, Any]] = []
    for path in sorted(data_dir.glob("*.yml")):
        data = safe_load_path(path) or {}
        if not isinstance(data, dict):
            raise ValueError(f"Expected mapping in {path}")
        data["slug"] = normalize_term(str(data.get("term", path.stem)))
        data["_source_file"] = str(path)
        terms.append(data)
    terms.sort(key=lambda entry: entry.get("term", "").lower())
    return terms


def render_term_page(term: Dict[str, Any]) -> str:
    aliases = term.get("aliases") or []
    categories = term.get("categories") or []
    roles = term.get("roles") or []
    governance = term.get("governance") or {}
    relationships = term.get("relationships") or {}
    examples = term.get("examples") or {}
    audiences = term.get("audiences") or {}

    lines: List[str] = [HEADER_COMMENT, "", f"# {term.get('term', '').strip()}", ""]

    metadata_bits: List[str] = []
    if aliases:
        metadata_bits.append(f"**Aliases:** {', '.join(aliases)}")
    if categories:
        metadata_bits.append(f"**Categories:** {', '.join(categories)}")
    if roles:
        role_names = [ROLE_DIRECTORY.get(role, {}).get("title", role) for role in roles]
        metadata_bits.append(f"**Roles:** {', '.join(role_names)}")
    metadata_bits.append(f"**Part of speech:** `{term.get('part_of_speech', '—')}`")
    status_value = term.get("status")
    status_display = (status_value or "—").replace("_", " ")
    status_chip = (
        f"<span class=\"status-chip status-{status_value}\">{status_display.title()}</span>"
        if status_value
        else "`—`"
    )
    metadata_bits.append(
        f"**Status:** {status_chip} (Last reviewed: {term.get('last_reviewed', '—')})"
    )
    lines.append("\n".join(metadata_bits))
    lines.append("")

    if term.get("slug") in CALLS_TO_ACTION:
        lines.extend([
            "!!! tip \"Put it into practice\"",
            f"    {CALLS_TO_ACTION[term['slug']]}"
        ])
        lines.append("")

    if roles:
        lines.append("## Role takeaways")
        for role in roles:
            label = ROLE_DIRECTORY.get(role, {}).get("title", role.title())
            guidance = ROLE_TAKEAWAYS.get(role, "Focus on how this affects your workflows.")
            lines.append(f"- **{label}:** {guidance}")
        lines.append("")

    apply_messages: List[str] = []
    for category in categories:
        apply_messages.extend(CATEGORY_APPLY_GUIDANCE.get(category, []))
    if roles:
        apply_messages.append(
            "Share takeaways with the accountable roles listed above so actions land with the right owners."
        )
    if apply_messages:
        ordered: List[str] = []
        for message in apply_messages:
            if message and message not in ordered:
                ordered.append(message)
        if ordered:
            lines.append("## Practice & apply")
            for message in ordered:
                lines.append(f"- {message}")
            lines.append("")

    short_def = term.get("short_def", "")
    if short_def:
        lines.extend(["## Short definition", short_def.strip(), ""])

    long_def = term.get("long_def", "")
    if long_def:
        lines.extend(["## Long definition", long_def.strip(), ""])

    if audiences:
        exec_view = audiences.get("exec")
        eng_view = audiences.get("engineer")
        lines.append("## Audience perspectives")
        if exec_view:
            lines.append(f"- **Exec:** {exec_view.strip()}")
        if eng_view:
            lines.append(f"- **Engineer:** {eng_view.strip()}")
        lines.append("")

    if examples:
        do_examples = examples.get("do") or []
        dont_examples = examples.get("dont") or []
        lines.append("## Examples")
        if do_examples:
            lines.append("**Do**")
            for item in do_examples:
                lines.append(f"- {item}")
            lines.append("")
        if dont_examples:
            lines.append("**Don't**")
            for item in dont_examples:
                lines.append(f"- {item}")
            lines.append("")

    if governance:
        tags = governance.get("nist_rmf_tags") or []
        risk_notes = governance.get("risk_notes")
        lines.append("## Governance")
        if tags:
            lines.append(f"- **NIST RMF tags:** {', '.join(tags)}")
        if risk_notes:
            lines.append(f"- **Risk notes:** {risk_notes.strip()}")
        lines.append("")

    relationship_sections = []
    for key, label in (("broader", "Broader"), ("narrower", "Narrower"), ("related", "Related")):
        values = relationships.get(key) or []
        if values:
            relationship_sections.append(f"- **{label}:** {', '.join(values)}")
    if relationship_sections:
        lines.append("## Relationships")
        lines.extend(relationship_sections)
        lines.append("")

    term_name = term.get("term", "this term")
    lines.extend(
        [
            "!!! info \"Something missing?\"",
            f"    Suggest examples or clarifications via the [term request intake](../term-request.md) and mention '{term_name}'.",
            "",
        ]
    )

    citations = term.get("citations") or []
    if citations:
        lines.append("## Citations")
        for citation in citations:
            source = citation.get("source", "Unknown source").strip()
            url = citation.get("url")
            if url:
                lines.append(f"- [{source}]({url})")
            else:
                lines.append(f"- {source}")
        lines.append("")

    license_name = term.get("license")
    if license_name:
        lines.append(f"_License: {license_name}_")
        lines.append("")

    source_file = term.get("_source_file")
    if source_file:
        rel_path = Path(source_file).as_posix()
        lines.append(f"_Source file: `{rel_path}`_")
        lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def render_index_page(terms: List[Dict[str, Any]]) -> str:
    lines: List[str] = [HEADER_COMMENT, "", "# Glossary Terms", ""]
    lines.append(f"Total entries: {len(terms)}")
    lines.append("")
    for term in terms:
        short_def = term.get("short_def", "").strip()
        lines.append(
            f"- [{term.get('term')}](./{term['slug']}.md) — {short_def}" if short_def else f"- [{term.get('term')}](./{term['slug']}.md)"
        )
    lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def render_roles_page(terms: List[Dict[str, Any]]) -> str:
    lines: List[str] = [HEADER_COMMENT, "", "# Role Starter Packs", ""]
    lines.append(
        "Guidance for common stakeholder groups. Each pack includes actionable steps and key focus areas "
        "so teams can operationalize insights immediately."
    )
    lines.append("")

    terms_by_role: Dict[str, List[Dict[str, Any]]] = {role: [] for role in ROLE_DIRECTORY}
    for term in terms:
        for role in term.get("roles", []):
            if role in terms_by_role:
                terms_by_role[role].append(term)

    for role, meta in ROLE_DIRECTORY.items():
        role_terms = terms_by_role.get(role, [])
        if not role_terms:
            continue
        lines.append(f"## {meta['title']}")
        lines.append(meta["summary"])
        lines.append("")
        lines.append("**Action plan**")
        lines.append("- Bookmark the [Glossary Search](search.md) filtered to this role and review the top 5 unfamiliar terms.")
        lines.append("- Schedule a sync with partner roles listed under each term to clarify ownership and open questions.")
        lines.append("- Capture insights in your runbook or onboarding guide so future teammates ramp faster.")
        lines.append("")

        learning_steps = ROLE_LEARNING_PATHS.get(role) or []
        if learning_steps:
            lines.append("### Guided learning path")
            for idx, step in enumerate(learning_steps, start=1):
                lines.append(f"{idx}. {step}")
            lines.append("")

        practice_tasks = ROLE_PRACTICE_TASKS.get(role) or []
        if practice_tasks:
            lines.append("### Practice checklist")
            for task in practice_tasks:
                lines.append(f"- {task}")
            lines.append("")
        lines.append("### Focus areas")
        category_counts: Dict[str, int] = {}
        for term in role_terms:
            for category in term.get("categories", []):
                category_counts[category] = category_counts.get(category, 0) + 1
        for category, count in sorted(category_counts.items(), key=lambda item: (-item[1], item[0])):
            lines.append(f"- {category} ({count} term{'s' if count != 1 else ''})")
        lines.append("")
        lines.append("### Recommended terms")
        role_terms.sort(key=lambda item: item.get("term", "").lower())
        for term in role_terms:
            lines.append(
                f"- [{term['term']}](terms/{term['slug']}.md) — {term.get('short_def', '').strip()}"
            )
        lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def render_categories_page(terms: List[Dict[str, Any]]) -> str:
    lines: List[str] = [HEADER_COMMENT, "", "# Category Explorer", ""]
    lines.append("Browse terms grouped by focus area. Categories align with navigation and search filters.")
    lines.append("")

    grouped = group_terms_by_category(terms)

    for category, bucket in grouped.items():
        if not bucket:
            continue
        lines.append(f"## {category}")
        description = {
            "Foundations": "Core machine learning concepts that underpin modern AI systems.",
            "LLM Core": "Mechanics of transformers, prompting, decoding, and language model internals.",
            "Retrieval & RAG": "Tools for grounding models with external knowledge and search infrastructure.",
            "Agents & Tooling": "Agent patterns, tool invocation, and orchestration strategies.",
            "Optimization & Efficiency": "Techniques for scaling inference and training effectively.",
            "Operations & Monitoring": "Operational playbooks for running AI in production.",
            "Governance & Risk": "Policies, controls, and assessments that ensure responsible AI."
        }.get(category, "")
        if description:
            lines.append(description)
            lines.append("")
        bucket.sort(key=lambda item: item.get("term", "").lower())
        for term in bucket:
            tag = ", ".join([ROLE_DIRECTORY.get(role, {}).get("title", role) for role in term.get("roles", [])])
            meta = f" — {tag}" if tag else ""
            lines.append(f"- [{term['term']}](terms/{term['slug']}.md){meta}")
        lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def write_docs(terms: List[Dict[str, Any]], docs_dir: Path) -> None:
    docs_dir.mkdir(parents=True, exist_ok=True)

    for path in docs_dir.glob("*.md"):
        path.unlink()

    for term in terms:
        destination = docs_dir / f"{term['slug']}.md"
        destination.write_text(render_term_page(term), encoding="utf-8")

    index_path = docs_dir / "index.md"
    index_path.write_text(render_index_page(terms), encoding="utf-8")

    roles_path = docs_dir.parent / "roles.md"
    roles_path.write_text(render_roles_page(terms), encoding="utf-8")

    categories_path = docs_dir.parent / "categories.md"
    categories_path.write_text(render_categories_page(terms), encoding="utf-8")


def update_mkdocs_nav(terms: List[Dict[str, Any]], mkdocs_path: Path = MKDOCS_PATH) -> None:
    if not mkdocs_path.exists():
        return

    content = mkdocs_path.read_text(encoding="utf-8")
    lines = content.splitlines()

    try:
        start_idx = next(i for i, line in enumerate(lines) if NAV_START_MARKER in line)
        end_idx = next(i for i, line in enumerate(lines) if NAV_END_MARKER in line)
    except StopIteration:
        print("Nav markers not found in mkdocs.yml; skipping nav update.")
        return

    if end_idx <= start_idx:
        print("Invalid nav marker positions; skipping nav update.")
        return

    marker_line = lines[start_idx]
    indent = marker_line[: marker_line.index(NAV_START_MARKER)]

    grouped: OrderedDict[str, List[Dict[str, Any]]] = OrderedDict((cat, []) for cat in CATEGORY_NAV_ORDER)
    extras: OrderedDict[str, List[Dict[str, Any]]] = OrderedDict()

    for term in terms:
        categories = term.get("categories") or []
        primary = categories[0] if categories else "Other"
        bucket = grouped.get(primary)
        if bucket is None:
            bucket = extras.setdefault(primary, [])
        bucket.append(term)

    nav_entries: List[str] = []
    for category, bucket in list(grouped.items()) + list(extras.items()):
        if not bucket:
            continue
        bucket.sort(key=lambda item: item.get("term", "").lower())
        nav_entries.append(f"{indent}- {category}:")
        for entry in bucket:
            nav_entries.append(f"{indent}  - {entry['term']}: terms/{entry['slug']}.md")

    updated = lines[: start_idx + 1] + nav_entries + lines[end_idx:]
    mkdocs_path.write_text("\n".join(updated) + "\n", encoding="utf-8")


def main(argv: List[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description="Render Markdown docs from glossary YAML entries")
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=Path("data/terms"),
        help="Directory containing YAML term files",
    )
    parser.add_argument(
        "--docs-dir",
        type=Path,
        default=Path("site/docs/terms"),
        help="Directory where Markdown files should be written",
    )
    args = parser.parse_args(argv)

    terms = load_terms(args.data_dir)
    if not terms:
        print(f"No term files found in {args.data_dir}", file=sys.stderr)
        return 1

    write_docs(terms, args.docs_dir)
    update_mkdocs_nav(terms)
    print(f"Rendered {len(terms)} term page(s) to {args.docs_dir}")
    return 0


if __name__ == "__main__":
    sys.exit(main())
