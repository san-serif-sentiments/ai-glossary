[
  {
    "term": "agent executor",
    "aliases": [
      "agent run loop",
      "agent controller"
    ],
    "categories": [
      "Agents & Tooling"
    ],
    "roles": [
      "engineering",
      "product"
    ],
    "short_def": "Controller layer that schedules planning, tool calls, and stop conditions so an AI agent completes tasks safely.",
    "nist_rmf_tags": [
      "governance",
      "risk_management",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "agent-executor"
  },
  {
    "term": "agentic ai",
    "aliases": [
      "ai agents",
      "autonomous agent"
    ],
    "categories": [
      "Agents & Tooling"
    ],
    "roles": [
      "engineering",
      "product"
    ],
    "short_def": "Systems that plan, act, and iterate with minimal human prompts by chaining model calls and tools.",
    "nist_rmf_tags": [
      "accountability",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "agentic-ai"
  },
  {
    "term": "ai assurance",
    "aliases": [
      "AI assurance program"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "product",
      "engineering",
      "policy",
      "legal",
      "security"
    ],
    "short_def": "Discipline that produces evidence, controls, and attestations showing an AI system meets agreed safety, compliance, and performance thresholds.",
    "nist_rmf_tags": [
      "accountability",
      "risk_management",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "ai-assurance"
  },
  {
    "term": "ai circuit breaker",
    "aliases": [
      "kill switch",
      "automated shutdown"
    ],
    "categories": [
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "security",
      "policy",
      "product"
    ],
    "short_def": "Automated control that halts model responses or tool access when risk thresholds are exceeded.",
    "nist_rmf_tags": [
      "risk_management",
      "monitoring",
      "security"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "ai-circuit-breaker"
  },
  {
    "term": "ai incident response",
    "aliases": [
      "model incident response",
      "ai escalation"
    ],
    "categories": [
      "Operations & Monitoring",
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "engineering",
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Coordinated workflow for detecting, triaging, and remediating harmful or out-of-policy AI behavior.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "ai-incident-response"
  },
  {
    "term": "algorithmic audit",
    "aliases": [
      "algorithm accountability audit"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "product",
      "engineering",
      "policy",
      "legal",
      "communications"
    ],
    "short_def": "Independent review of an AI systemâ€™s data, design, and outcomes to verify compliance, fairness, and risk controls.",
    "nist_rmf_tags": [
      "accountability",
      "fairness",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "algorithmic-audit"
  },
  {
    "term": "algorithmic bias",
    "aliases": [
      "systemic bias",
      "ai bias"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "legal",
      "product",
      "communications",
      "security"
    ],
    "short_def": "Systematic unfairness in model outputs that disadvantages certain groups or outcomes.",
    "nist_rmf_tags": [
      "fairness",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "algorithmic-bias"
  },
  {
    "term": "algorithmic impact assessment",
    "aliases": [
      "AIA",
      "AI impact assessment"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "legal",
      "product",
      "communications",
      "engineering"
    ],
    "short_def": "Structured review that documents how an AI system may affect people, processes, and compliance obligations.",
    "nist_rmf_tags": [
      "accountability",
      "risk_management",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "algorithmic-impact-assessment"
  },
  {
    "term": "alignment",
    "aliases": [
      "AI alignment",
      "value alignment"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Making sure AI systems optimize for human values, policies, and intended outcomes.",
    "nist_rmf_tags": [
      "accountability",
      "transparency",
      "validity"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "alignment"
  },
  {
    "term": "assurance case",
    "aliases": [
      "safety case",
      "structured assurance argument"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "legal",
      "engineering"
    ],
    "short_def": "Structured argument that proves an AI system meets safety and compliance expectations.",
    "nist_rmf_tags": [
      "governance",
      "accountability",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "assurance-case"
  },
  {
    "term": "attention",
    "aliases": [
      "attention mechanism",
      "self-attention"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Technique enabling models to weight input tokens differently when producing each output.",
    "nist_rmf_tags": [
      "transparency",
      "robustness"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "attention"
  },
  {
    "term": "beam search",
    "aliases": [
      "beam decoding",
      "multi-path decoding"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Deterministic decoding that keeps the top scoring sequences across multiple beams before selecting the final output.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "beam-search"
  },
  {
    "term": "bias-variance tradeoff",
    "aliases": [
      "bias variance trade-off",
      "generalization tradeoff"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "policy"
    ],
    "short_def": "Balance between underfitting and overfitting: low bias increases variance, while high bias lowers variance but misses patterns.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "bias-variance-tradeoff"
  },
  {
    "term": "chain-of-thought prompting",
    "aliases": [
      "cot prompting",
      "step-by-step prompting"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product"
    ],
    "short_def": "Prompting technique that asks models to reason through intermediate steps before giving a final answer.",
    "nist_rmf_tags": [
      "transparency",
      "risk_management",
      "monitoring"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "chain-of-thought-prompting"
  },
  {
    "term": "chunking",
    "aliases": [
      "document chunking",
      "segmentation"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Splitting source documents into manageable pieces before indexing or feeding them to models.",
    "nist_rmf_tags": [
      "data_quality",
      "privacy"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "chunking"
  },
  {
    "term": "clip",
    "aliases": [
      "contrastive language-image pretraining",
      "clip model"
    ],
    "categories": [
      "Foundations",
      "Retrieval & RAG"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product",
      "communications"
    ],
    "short_def": "Multimodal model that embeds images and text into a shared space using contrastive learning.",
    "nist_rmf_tags": [
      "data_quality",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "clip"
  },
  {
    "term": "confusion matrix",
    "aliases": [
      "contingency table",
      "error matrix"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product",
      "policy"
    ],
    "short_def": "Table that summarizes true/false positives and negatives to diagnose classification performance.",
    "nist_rmf_tags": [
      "transparency",
      "validity"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "confusion-matrix"
  },
  {
    "term": "consent management",
    "aliases": [
      "consent governance",
      "user consent tracking"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "legal",
      "policy",
      "product",
      "engineering"
    ],
    "short_def": "Practices that capture, honor, and audit user permissions across AI features.",
    "nist_rmf_tags": [
      "privacy",
      "accountability",
      "governance"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "consent-management"
  },
  {
    "term": "constitutional ai",
    "aliases": [
      "principle-guided alignment",
      "self-critique alignment"
    ],
    "categories": [
      "Governance & Risk",
      "LLM Core"
    ],
    "roles": [
      "product",
      "engineering",
      "policy"
    ],
    "short_def": "Alignment approach where models critique and revise their own outputs against a written set of principles.",
    "nist_rmf_tags": [
      "accountability",
      "validity",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "constitutional-ai"
  },
  {
    "term": "content moderation",
    "aliases": [
      "trust and safety",
      "policy enforcement"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "policy",
      "communications",
      "product",
      "security",
      "engineering"
    ],
    "short_def": "Workflows and tools that review, filter, and act on user-generated content to enforce policy.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "content-moderation"
  },
  {
    "term": "context window",
    "aliases": [
      "context length",
      "sequence length limit"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Maximum number of tokens a model can consider at once during prompting or inference.",
    "nist_rmf_tags": [
      "transparency",
      "privacy"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "context-window"
  },
  {
    "term": "cross-validation",
    "aliases": [
      "k-fold validation",
      "cv"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Evaluation technique that splits data into multiple folds to estimate model performance on unseen samples.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "cross-validation"
  },
  {
    "term": "data lineage",
    "aliases": [
      "lineage tracking",
      "data provenance"
    ],
    "categories": [
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "data_science",
      "policy",
      "legal"
    ],
    "short_def": "Traceable record of how data moves, transforms, and is used across AI systems.",
    "nist_rmf_tags": [
      "accountability",
      "governance",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "data-lineage"
  },
  {
    "term": "data minimization",
    "aliases": [
      "data minimisation",
      "minimal data collection"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Principle of collecting and retaining only the data necessary for a defined purpose.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "data-minimization"
  },
  {
    "term": "data redaction",
    "aliases": [
      "pii redaction",
      "sensitive data masking"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "engineering",
      "policy",
      "legal",
      "security"
    ],
    "short_def": "Removal or masking of sensitive fields before data is stored, shared, or used for model training.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management",
      "security"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "data-redaction"
  },
  {
    "term": "data retention",
    "aliases": [
      "retention policy",
      "data lifecycle"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "legal",
      "policy",
      "security",
      "product"
    ],
    "short_def": "Policies defining how long data is stored, where it lives, and how it is deleted.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "data-retention"
  },
  {
    "term": "dataset card",
    "aliases": [
      "dataset documentation",
      "data sheet"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "data_science",
      "policy",
      "legal",
      "product"
    ],
    "short_def": "Structured documentation describing a datasetâ€™s purpose, composition, risks, and usage constraints.",
    "nist_rmf_tags": [
      "transparency",
      "governance",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "dataset-card"
  },
  {
    "term": "decoding",
    "aliases": [
      "text decoding",
      "generation decoding"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Algorithms that turn model probability distributions into output tokens during generation.",
    "nist_rmf_tags": [
      "transparency",
      "robustness"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "decoding"
  },
  {
    "term": "differential privacy",
    "aliases": [
      "DP",
      "epsilon-differential privacy"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "engineering",
      "legal",
      "policy",
      "security"
    ],
    "short_def": "Mathematical framework that limits how much any single record influences published data or model outputs.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "differential-privacy"
  },
  {
    "term": "diffusion model",
    "aliases": [
      "denoising diffusion model",
      "score-based model"
    ],
    "categories": [
      "Foundations",
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product",
      "communications"
    ],
    "short_def": "Generative model that iteratively denoises random noise to synthesize images, audio, or other data.",
    "nist_rmf_tags": [
      "risk_management",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "diffusion-model"
  },
  {
    "term": "direct preference optimization",
    "aliases": [
      "dpo",
      "preference optimization without rlhf"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product"
    ],
    "short_def": "Alignment technique that fine-tunes models directly on preference data without training a separate reward model.",
    "nist_rmf_tags": [
      "measurement",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "direct-preference-optimization"
  },
  {
    "term": "embedding",
    "aliases": [
      "vector embedding",
      "representation vector"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Dense numerical representation that captures semantic meaning of text, images, or other data.",
    "nist_rmf_tags": [
      "data_quality",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "embedding"
  },
  {
    "term": "escalation policy",
    "aliases": [
      "human escalation policy",
      "handoff policy"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "product",
      "policy",
      "security",
      "engineering"
    ],
    "short_def": "Playbook that defines when and how AI systems route control to human reviewers.",
    "nist_rmf_tags": [
      "governance",
      "monitoring",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "escalation-policy"
  },
  {
    "term": "evaluation harness",
    "aliases": [
      "eval harness",
      "agent evaluation pipeline"
    ],
    "categories": [
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product"
    ],
    "short_def": "Automated pipeline that replays tasks, scores outputs, and reports regressions for AI systems.",
    "nist_rmf_tags": [
      "measurement",
      "monitoring",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "evaluation-harness"
  },
  {
    "term": "evaluation",
    "aliases": [
      "model evaluation",
      "AI evaluation"
    ],
    "categories": [
      "Operations & Monitoring",
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "engineering",
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Systematic measurement of model performance, safety, and reliability using defined tests.",
    "nist_rmf_tags": [
      "validity",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "evaluation"
  },
  {
    "term": "f1 score",
    "aliases": [
      "f-score",
      "harmonic mean of precision and recall"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Harmonic mean of precision and recall, balancing false positives and false negatives.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "f1-score"
  },
  {
    "term": "fairness metrics",
    "aliases": [
      "fairness measures",
      "fairness evaluation"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "policy",
      "legal",
      "engineering",
      "product"
    ],
    "short_def": "Quantitative measures that evaluate whether model performance is equitable across groups.",
    "nist_rmf_tags": [
      "fairness",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "fairness-metrics"
  },
  {
    "term": "fine-tuning",
    "aliases": [
      "model adaptation",
      "supervised fine-tuning"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "data_science",
      "engineering"
    ],
    "short_def": "Additional training that adapts a pretrained model to a specific task or domain.",
    "nist_rmf_tags": [
      "accountability",
      "validity"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "fine-tuning"
  },
  {
    "term": "function calling",
    "aliases": [
      "tool calling",
      "structured output invocation"
    ],
    "categories": [
      "Agents & Tooling",
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "product",
      "data_science"
    ],
    "short_def": "LLM capability that lets prompts invoke predefined functions and return structured arguments.",
    "nist_rmf_tags": [
      "accountability",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "function-calling"
  },
  {
    "term": "generative ai",
    "aliases": [
      "genai",
      "generative artificial intelligence"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "communications",
      "policy",
      "product"
    ],
    "short_def": "Family of models that produce new contentâ€”text, images, codeâ€”rather than only making predictions.",
    "nist_rmf_tags": [
      "transparency",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "generative-ai"
  },
  {
    "term": "greedy decoding",
    "aliases": [
      "argmax decoding"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Strategy that selects the highest-probability token at each step, producing deterministic outputs.",
    "nist_rmf_tags": [
      "validity",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "greedy-decoding"
  },
  {
    "term": "guardrail policy",
    "aliases": [
      "guardrail playbook",
      "safety policy prompt"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "product",
      "security",
      "engineering"
    ],
    "short_def": "Documented rules and prompts that define allowed, blocked, and escalated behaviors for AI systems.",
    "nist_rmf_tags": [
      "governance",
      "risk_management",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "guardrail-policy"
  },
  {
    "term": "guardrails",
    "aliases": [
      "safety guardrails",
      "policy guardrails"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Controls that constrain model behavior to comply with safety, legal, or brand requirements.",
    "nist_rmf_tags": [
      "accountability",
      "privacy",
      "robustness"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "guardrails"
  },
  {
    "term": "hallucination",
    "aliases": [
      "AI hallucination",
      "confabulation"
    ],
    "categories": [
      "LLM Core",
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "data_science",
      "engineering",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "When an AI model presents fabricated or unsupported information as fact.",
    "nist_rmf_tags": [
      "accuracy",
      "transparency",
      "validity"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "hallucination"
  },
  {
    "term": "human handoff",
    "aliases": [
      "agent-to-human handoff",
      "human-in-the-loop handoff"
    ],
    "categories": [
      "Agents & Tooling"
    ],
    "roles": [
      "product",
      "communications",
      "engineering",
      "policy"
    ],
    "short_def": "Moment when an AI workflow transfers control to a human for review or action.",
    "nist_rmf_tags": [
      "governance",
      "monitoring",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "human-handoff"
  },
  {
    "term": "impact mitigation plan",
    "aliases": [
      "mitigation roadmap",
      "risk remediation plan"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "product",
      "legal",
      "engineering"
    ],
    "short_def": "Action plan that tracks risks, mitigations, owners, and timelines for an AI deployment.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability",
      "monitoring"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "impact-mitigation-plan"
  },
  {
    "term": "incident taxonomy",
    "aliases": [
      "incident classification",
      "risk taxonomy"
    ],
    "categories": [
      "Operations & Monitoring"
    ],
    "roles": [
      "policy",
      "security",
      "product",
      "engineering"
    ],
    "short_def": "Standardized categories used to tag, analyze, and report AI incidents consistently.",
    "nist_rmf_tags": [
      "governance",
      "monitoring",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "incident-taxonomy"
  },
  {
    "term": "instruction tuning",
    "aliases": [
      "instruction fine-tuning",
      "supervised fine-tuning"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product",
      "policy"
    ],
    "short_def": "Supervised training that teaches models to follow natural-language instructions using curated examples.",
    "nist_rmf_tags": [
      "risk_management",
      "measurement",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "instruction-tuning"
  },
  {
    "term": "jailbreak prompt",
    "aliases": [
      "prompt jailbreak",
      "guardrail bypass"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "security",
      "product",
      "engineering"
    ],
    "short_def": "Crafted input that persuades a model to ignore safety instructions and produce disallowed responses.",
    "nist_rmf_tags": [
      "security",
      "monitoring",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "jailbreak-prompt"
  },
  {
    "term": "knowledge distillation",
    "aliases": [
      "distillation",
      "teacher-student training"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "data_science",
      "engineering"
    ],
    "short_def": "Technique that trains a smaller student model to mimic a larger teacher modelâ€™s behavior.",
    "nist_rmf_tags": [
      "efficiency",
      "validity"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "knowledge-distillation"
  },
  {
    "term": "kv cache",
    "aliases": [
      "key-value cache",
      "attention cache"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Stored attention keys and values reused across decoding steps to speed sequential generation.",
    "nist_rmf_tags": [
      "efficiency",
      "privacy"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "kv-cache"
  },
  {
    "term": "log probability",
    "aliases": [
      "logprob",
      "token log probability"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Logarithm of a tokenâ€™s probability, used to inspect model confidence and guide decoding tweaks.",
    "nist_rmf_tags": [
      "transparency",
      "validity"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "log-probability"
  },
  {
    "term": "low-rank adaptation",
    "aliases": [
      "LoRA",
      "low rank fine-tuning"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "data_science",
      "engineering"
    ],
    "short_def": "Parameter-efficient fine-tuning that injects low-rank update matrices into transformer weights.",
    "nist_rmf_tags": [
      "efficiency",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "low-rank-adaptation"
  },
  {
    "term": "memory strategy",
    "aliases": [
      "agent memory strategy",
      "memory policy"
    ],
    "categories": [
      "Agents & Tooling"
    ],
    "roles": [
      "engineering",
      "product"
    ],
    "short_def": "Deliberate approach for when an AI agent stores, retrieves, or forgets context across tasks.",
    "nist_rmf_tags": [
      "governance",
      "privacy",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "memory-strategy"
  },
  {
    "term": "mixture of experts",
    "aliases": [
      "moe",
      "expert gating"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product"
    ],
    "short_def": "Neural architecture that routes tokens to specialized submodels to scale capacity efficiently.",
    "nist_rmf_tags": [
      "monitoring",
      "risk_management",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "mixture-of-experts"
  },
  {
    "term": "ml observability",
    "aliases": [
      "model observability",
      "ai observability"
    ],
    "categories": [
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "policy",
      "security"
    ],
    "short_def": "Practices and tooling that surface model health through metrics, traces, and alerts across the lifecycle.",
    "nist_rmf_tags": [
      "monitoring",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "ml-observability"
  },
  {
    "term": "ml ops",
    "aliases": [
      "mlops",
      "machine learning operations"
    ],
    "categories": [
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "policy",
      "security"
    ],
    "short_def": "Operational discipline that manages ML models from experimentation through deployment and monitoring.",
    "nist_rmf_tags": [
      "accountability",
      "monitoring"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "ml-ops"
  },
  {
    "term": "model card",
    "aliases": [
      "model documentation",
      "model datasheet"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "policy",
      "legal",
      "product",
      "engineering",
      "communications"
    ],
    "short_def": "Standardized documentation describing a modelâ€™s purpose, data, performance, and limitations.",
    "nist_rmf_tags": [
      "documentation",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "model-card"
  },
  {
    "term": "model drift",
    "aliases": [
      "distribution drift",
      "concept drift"
    ],
    "categories": [
      "Operations & Monitoring",
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "engineering",
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Gradual mismatch between model assumptions and real-world data that degrades performance over time.",
    "nist_rmf_tags": [
      "monitoring",
      "validity"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "model-drift"
  },
  {
    "term": "model governance",
    "aliases": [
      "ai governance",
      "ml governance"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Policies and processes that manage AI models across risk, compliance, and lifecycle decisions.",
    "nist_rmf_tags": [
      "accountability",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "model-governance"
  },
  {
    "term": "model interpretability",
    "aliases": [
      "interpretability",
      "explainability"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "policy",
      "legal",
      "product"
    ],
    "short_def": "Ability to explain how a model arrives at its predictions in ways stakeholders understand.",
    "nist_rmf_tags": [
      "transparency",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "model-interpretability"
  },
  {
    "term": "overfitting",
    "aliases": [
      "model overfitting",
      "overtraining"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "When a model memorizes training data patterns so closely that it performs poorly on new samples.",
    "nist_rmf_tags": [
      "validity",
      "data_quality"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "overfitting"
  },
  {
    "term": "precision",
    "aliases": [
      "positive predictive value",
      "ppv"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product",
      "policy"
    ],
    "short_def": "Share of predicted positives that are actually correct for a given classifier.",
    "nist_rmf_tags": [
      "validity",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "precision"
  },
  {
    "term": "preference dataset",
    "aliases": [
      "preference data",
      "human feedback dataset"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "policy",
      "product",
      "engineering"
    ],
    "short_def": "Labeled comparisons of model outputs that capture which responses humans prefer.",
    "nist_rmf_tags": [
      "accountability",
      "risk_management",
      "privacy"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "preference-dataset"
  },
  {
    "term": "privacy budget",
    "aliases": [
      "epsilon budget",
      "differential privacy budget"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "data_science",
      "policy",
      "legal",
      "engineering"
    ],
    "short_def": "Quantitative limit on how much privacy loss is allowed when applying differential privacy.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management",
      "governance"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "privacy-budget"
  },
  {
    "term": "privacy impact assessment",
    "aliases": [
      "pia",
      "data protection impact assessment"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Structured review that evaluates how a system collects, uses, and safeguards personal data.",
    "nist_rmf_tags": [
      "privacy",
      "documentation"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "privacy-impact-assessment"
  },
  {
    "term": "privacy",
    "aliases": [
      "data privacy",
      "information privacy"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Principle of limiting data collection, use, and exposure to protect individualsâ€™ information.",
    "nist_rmf_tags": [
      "privacy",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "privacy"
  },
  {
    "term": "prompt engineering",
    "aliases": [
      "prompt design",
      "prompt scripting"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Crafting and testing prompts to steer model behavior toward desired outcomes.",
    "nist_rmf_tags": [
      "accountability",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "prompt-engineering"
  },
  {
    "term": "prompt injection",
    "aliases": [
      "prompt attack",
      "context hijacking"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "security",
      "engineering",
      "product"
    ],
    "short_def": "Attack that inserts malicious instructions into model inputs to override original prompts or policies.",
    "nist_rmf_tags": [
      "security",
      "risk_management",
      "monitoring"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "prompt-injection"
  },
  {
    "term": "quantization",
    "aliases": [
      "model quantization",
      "weight quantization"
    ],
    "categories": [
      "Optimization & Efficiency"
    ],
    "roles": [
      "data_science",
      "engineering"
    ],
    "short_def": "Technique that compresses model weights into lower-precision formats to shrink size and speed inference.",
    "nist_rmf_tags": [
      "efficiency",
      "robustness",
      "documentation"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "quantization"
  },
  {
    "term": "recall",
    "aliases": [
      "sensitivity",
      "true positive rate"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product",
      "policy"
    ],
    "short_def": "Share of actual positives a model successfully identifies.",
    "nist_rmf_tags": [
      "validity",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "recall"
  },
  {
    "term": "red teaming",
    "aliases": [
      "ai red teaming",
      "adversarial testing"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "communications",
      "engineering",
      "legal",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Deliberate stress testing that probes AI systems for harmful, biased, or policy-violating behavior.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "red-teaming"
  },
  {
    "term": "reinforcement learning from human feedback",
    "aliases": [
      "rlhf",
      "preference optimization"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "data_science",
      "product",
      "policy"
    ],
    "short_def": "Training approach that tunes a model using reward signals learned from human preference data.",
    "nist_rmf_tags": [
      "risk_management",
      "monitoring",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "reinforcement-learning-from-human-feedback"
  },
  {
    "term": "repetition penalty",
    "aliases": [
      "anti-repetition penalty",
      "token penalty"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Decoding adjustment that down-weights tokens already generated to reduce loops and repeated phrases.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "repetition-penalty"
  },
  {
    "term": "reranking",
    "aliases": [
      "re-ranking",
      "second-stage ranking"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Step that refines retrieval results using a more precise but slower scoring model.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "reranking"
  },
  {
    "term": "responsible ai",
    "aliases": [
      "trustworthy ai",
      "ethical ai"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "communications",
      "legal",
      "policy",
      "product"
    ],
    "short_def": "Frameworks and practices that ensure AI systems are safe, fair, and aligned with ethical and legal expectations.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "responsible-ai"
  },
  {
    "term": "retrieval-augmented generation",
    "aliases": [
      "RAG",
      "retrieval augmented generation"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Workflow that grounds a generative model with retrieved context before producing output.",
    "nist_rmf_tags": [
      "transparency",
      "data_quality",
      "documentation"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "retrieval-augmented-generation"
  },
  {
    "term": "retrieval",
    "aliases": [
      "information retrieval",
      "retriever"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "communications",
      "data_science",
      "product",
      "security",
      "engineering",
      "policy"
    ],
    "short_def": "Process of selecting relevant documents or vectors from a corpus in response to a query.",
    "nist_rmf_tags": [
      "data_quality",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "retrieval"
  },
  {
    "term": "reward model",
    "aliases": [
      "preference model",
      "policy reward model"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "data_science",
      "policy"
    ],
    "short_def": "Model trained on human preferences that scores AI responses for alignment or quality.",
    "nist_rmf_tags": [
      "measurement",
      "risk_management",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "reward-model"
  },
  {
    "term": "risk register",
    "aliases": [
      "risk log",
      "risk inventory"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "product",
      "legal",
      "engineering"
    ],
    "short_def": "Central list of identified AI risks, their owners, mitigations, and review status.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability",
      "governance"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "risk-register"
  },
  {
    "term": "robust prompting",
    "aliases": [
      "defensive prompting",
      "resilient prompting"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "product",
      "security"
    ],
    "short_def": "Prompt design techniques that harden models against injections, ambiguity, and unsafe outputs.",
    "nist_rmf_tags": [
      "risk_management",
      "safety",
      "monitoring"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "robust-prompting"
  },
  {
    "term": "roc auc",
    "aliases": [
      "area under the roc curve",
      "roc area"
    ],
    "categories": [
      "Foundations"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Metric summarizing binary classifier performance by measuring area under the ROC curve.",
    "nist_rmf_tags": [
      "validity",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "roc-auc"
  },
  {
    "term": "safety classifier",
    "aliases": [
      "safety filter",
      "policy classifier"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "security",
      "policy",
      "product",
      "engineering"
    ],
    "short_def": "Model that detects policy-violating or risky content before or after generation.",
    "nist_rmf_tags": [
      "monitoring",
      "risk_management",
      "security"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "safety-classifier"
  },
  {
    "term": "safety evaluation",
    "aliases": [
      "safety testing",
      "safety assessment"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "policy",
      "product",
      "communications"
    ],
    "short_def": "Testing focused on preventing harmful, abusive, or policy-violating AI behavior before and after launch.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "safety-evaluation"
  },
  {
    "term": "safety review board",
    "aliases": [
      "ai safety council",
      "responsible ai board"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "legal",
      "security",
      "product"
    ],
    "short_def": "Cross-functional committee that approves high-risk AI launches and monitors mitigations.",
    "nist_rmf_tags": [
      "governance",
      "accountability",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "safety-review-board"
  },
  {
    "term": "safety spec",
    "aliases": [
      "safety specification",
      "model safety policy"
    ],
    "categories": [
      "Governance & Risk",
      "LLM Core"
    ],
    "roles": [
      "product",
      "engineering",
      "policy",
      "security",
      "communications"
    ],
    "short_def": "Document that codifies allowed, disallowed, and escalated behaviours for an AI system so teams can enforce safety and policy expectations.",
    "nist_rmf_tags": [
      "risk_management",
      "accountability",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "safety-spec"
  },
  {
    "term": "self-consistency decoding",
    "aliases": [
      "self-consistency",
      "majority-vote reasoning"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "engineering",
      "data_science"
    ],
    "short_def": "Decoding strategy that samples multiple reasoning paths and aggregates the most consistent answer.",
    "nist_rmf_tags": [
      "monitoring",
      "risk_management"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "self-consistency-decoding"
  },
  {
    "term": "self-critique loop",
    "aliases": [
      "self-reflection loop",
      "critique-and-revise"
    ],
    "categories": [
      "Agents & Tooling"
    ],
    "roles": [
      "engineering",
      "product",
      "policy"
    ],
    "short_def": "Pattern where a model reviews its own outputs, critiques them, and produces revisions before responding.",
    "nist_rmf_tags": [
      "monitoring",
      "risk_management",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "self-critique-loop"
  },
  {
    "term": "shadow deployment",
    "aliases": [
      "shadow mode",
      "silent launch"
    ],
    "categories": [
      "Operations & Monitoring"
    ],
    "roles": [
      "engineering",
      "product",
      "policy",
      "security"
    ],
    "short_def": "Deploying an AI system alongside the existing workflow without user impact to collect telemetry.",
    "nist_rmf_tags": [
      "monitoring",
      "risk_management",
      "governance"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "shadow-deployment"
  },
  {
    "term": "synthetic data evaluation",
    "aliases": [
      "synthetic data quality assessment",
      "synthetic validation"
    ],
    "categories": [
      "Operations & Monitoring",
      "Governance & Risk"
    ],
    "roles": [
      "data_science",
      "engineering",
      "policy",
      "product"
    ],
    "short_def": "Process for measuring fidelity, utility, privacy, and bias of synthetic datasets before use.",
    "nist_rmf_tags": [
      "data_quality",
      "privacy"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "synthetic-data-evaluation"
  },
  {
    "term": "synthetic data",
    "aliases": [
      "generated data",
      "simulated data"
    ],
    "categories": [
      "Governance & Risk",
      "Operations & Monitoring"
    ],
    "roles": [
      "data_science",
      "engineering",
      "policy",
      "product",
      "security"
    ],
    "short_def": "Artificially generated dataset used to augment training, testing, or privacy-preserving workflows.",
    "nist_rmf_tags": [
      "privacy",
      "data_quality"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "synthetic-data"
  },
  {
    "term": "system prompt",
    "aliases": [
      "system instruction",
      "base prompt"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Foundational instruction that sets role, tone, and guardrails for an AI assistant before user input.",
    "nist_rmf_tags": [
      "transparency",
      "accountability"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "system-prompt"
  },
  {
    "term": "temperature",
    "aliases": [
      "sampling temperature",
      "softmax temperature"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Decoding parameter that controls how random or deterministic a modelâ€™s outputs are.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "temperature"
  },
  {
    "term": "token",
    "aliases": [
      "subword token",
      "tokenized unit"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Smallest unit of text a model processes after tokenization, such as a word fragment or character.",
    "nist_rmf_tags": [
      "transparency",
      "validity"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "token"
  },
  {
    "term": "tool use",
    "aliases": [
      "function calling",
      "model tool invocation"
    ],
    "categories": [
      "Agents & Tooling"
    ],
    "roles": [
      "engineering",
      "product"
    ],
    "short_def": "Pattern where a model selects external tools or functions to handle parts of a task.",
    "nist_rmf_tags": [
      "accountability",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "tool-use"
  },
  {
    "term": "top-k sampling",
    "aliases": [
      "k-sampling",
      "truncated sampling"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Decoding method that samples from the k most probable next tokens to balance diversity and control.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "top-k-sampling"
  },
  {
    "term": "top-p sampling",
    "aliases": [
      "nucleus sampling",
      "p-sampling"
    ],
    "categories": [
      "LLM Core"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Decoding strategy that samples from the smallest set of tokens whose probabilities sum to p.",
    "nist_rmf_tags": [
      "robustness",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "top-p-sampling"
  },
  {
    "term": "transparency report",
    "aliases": [
      "algorithmic transparency report",
      "safety disclosure report"
    ],
    "categories": [
      "Governance & Risk"
    ],
    "roles": [
      "policy",
      "legal",
      "communications",
      "product"
    ],
    "short_def": "Periodic disclosure that details how an AI system operates, what data it handles, and how risks are mitigated.",
    "nist_rmf_tags": [
      "transparency",
      "accountability",
      "governance"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "transparency-report"
  },
  {
    "term": "vector store",
    "aliases": [
      "vector database",
      "embedding index"
    ],
    "categories": [
      "Retrieval & RAG"
    ],
    "roles": [
      "data_science",
      "engineering",
      "product"
    ],
    "short_def": "Database optimized to store embeddings and execute similarity search over vectors.",
    "nist_rmf_tags": [
      "accountability",
      "data_quality"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "vector-store"
  },
  {
    "term": "voice cloning",
    "aliases": [
      "voice synthesis",
      "speech cloning"
    ],
    "categories": [
      "Foundations",
      "Governance & Risk"
    ],
    "roles": [
      "product",
      "communications",
      "legal",
      "policy",
      "security"
    ],
    "short_def": "Technique that replicates a personâ€™s voice using generative models trained on audio samples.",
    "nist_rmf_tags": [
      "risk_management",
      "transparency"
    ],
    "status": "approved",
    "last_reviewed": "2025-09-29",
    "slug": "voice-cloning"
  }
]