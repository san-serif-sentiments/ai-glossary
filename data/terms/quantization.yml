term: "quantization"
aliases:
  - "model quantization"
  - "weight quantization"
part_of_speech: "process"
short_def: "Technique that compresses model weights into lower-precision formats to shrink size and speed inference."
long_def: >-
  Quantization converts neural network parameters and activations from high-precision floating
  point representations (such as FP32) into lower bit-width formats (such as INT8 or INT4) to
  reduce memory footprint and accelerate inference. By mapping continuous values into a discrete
  set, quantization enables models to run on cost-sensitive hardware, deliver faster responses,
  and consume less energy, which is critical when deploying large language models at scale or on
  edge devices. Engineers choose between post-training quantization, which calibrates a frozen
  model on representative data, and quantization-aware training, which simulates low-precision
  behavior during fine-tuning to preserve accuracy. Careful evaluation is required to understand
  the trade-offs: aggressive quantization can introduce numerical instability, harm latency
  determinism, or amplify bias if calibration data under-represents certain groups. Successful
  programs pair quantization with monitoring, backstops such as higher-precision fallbacks, and
  documentation that makes these trade-offs explicit to stakeholders.
audiences:
  exec: "A cost-control lever: shrink model footprints so you can serve more traffic on existing hardware."
  engineer: "Apply per-tensor or per-channel scaling, choose symmetric/asymmetric schemes, and validate perplexity and latency post-quantization."
examples:
  do:
    - "Benchmark accuracy and latency before and after quantization to document the trade-offs."
    - "Use representative calibration datasets that include edge cases and demographic variation."
  dont:
    - "Quantize safety-critical models without fallback paths or runtime monitoring."
    - "Assume INT4 settings will work across architectures without profiling."
governance:
  nist_rmf_tags:
    - "efficiency"
    - "robustness"
    - "documentation"
  risk_notes: "Quantization can reduce accuracy or shift error distribution; record evaluations and obtain stakeholder sign-off."
relationships:
  broader:
    - "model optimization"
  narrower:
    - "post-training quantization"
    - "quantization-aware training"
  related:
    - "compression"
    - "distillation"
    - "hardware acceleration"
citations:
  - source: "Google ML Glossary"
    url: "https://developers.google.com/machine-learning/glossary"
  - source: "Hugging Face Glossary"
    url: "https://huggingface.co/docs/transformers/en/glossary"
  - source: "Stanford HAI Brief Definitions"
    url: "https://hai.stanford.edu/news/brief-definitions"
license: "CC BY-SA 4.0"
status: "draft"
last_reviewed: "2024-09-18"
