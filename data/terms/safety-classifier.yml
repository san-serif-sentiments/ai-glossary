term: "safety classifier"
aliases:
  - "safety filter"
  - "policy classifier"
categories:
  - "Governance & Risk"
roles:
  - "security"
  - "policy"
  - "product"
  - "engineering"
part_of_speech: "noun_phrase"
short_def: "Model that detects policy-violating or risky content before or after generation."
long_def: >-
  A safety classifier screens prompts and outputs to catch disallowed topics such as self-harm, extremism, or personal
  data. Classifiers can run pre-generation (blocking unsafe inputs), post-generation (filtering responses), or both.
  They complement prompt-based guardrails, providing a measurable signal that policies are enforced. Security and policy
  teams curate labeled datasets, engineering integrates the classifier into pipelines, and product owners tune messaging
  when content is blocked. Mature teams monitor precision/recall, recalibrate thresholds, and pair classifiers with human
  review for high-severity categories. Classifiers require ongoing evaluation to avoid overblocking legitimate use or
  missing novel abuse patterns.
audiences:
  exec: "Treat safety classifiers as a control with metrics, owners, and effectiveness reviews."
  engineer: "Log classifier scores, decisions, and overrides for auditing and incident analysis."
examples:
  do:
    - "Version classifier thresholds and align them with published policy requirements."
    - "Run adversarial tests to catch bypasses introduced by new jailbreaks."
  dont:
    - "Rely solely on classifiers without human review for high-risk categories."
    - "Deploy classifiers trained on outdated policy definitions."
governance:
  nist_rmf_tags:
    - "monitoring"
    - "risk_management"
    - "security"
  risk_notes: "Poorly tuned classifiers erode trust—either by letting harmful content through or by overblocking."
relationships:
  broader:
    - "guardrails"
  related:
    - "jailbreak prompt"
    - "prompt injection"
    - "robust prompting"
citations:
  - source: "OpenAI – New Moderation Tools"
    url: "https://openai.com/blog/new-moderation-tools"
  - source: "Google Cloud – Content Safety Tools"
    url: "https://cloud.google.com/blog/products/ai-machine-learning/responsible-ai-toolkits-content-safety"
license: "CC BY-SA 4.0"
status: "approved"
last_reviewed: "2025-09-29"
