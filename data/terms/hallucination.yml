term: "hallucination"
aliases:
  - "AI hallucination"
  - "confabulation"
categories:
  - "LLM Core"
  - "Governance & Risk"
roles:
  - "communications"
  - "data_science"
  - "engineering"
  - "legal"
  - "policy"
  - "product"
part_of_speech: "noun"
short_def: "When an AI model presents fabricated or unsupported information as fact."
long_def: >-
  Hallucination describes the tendency of generative models to deliver content that sounds
  plausible but is either factually incorrect, logically inconsistent, or entirely invented.
  The phenomenon stems from the probabilistic way large language models predict the next
  token based on training data patterns rather than grounded knowledge of the world. It can
  occur when prompts lack sufficient context, when the model has not seen relevant examples
  during training, or when decoding strategies over-index on fluency instead of accuracy.
  Product teams experience hallucination as broken user trust, while engineers may notice it
  during evaluation as high lexical overlap paired with low factual precision. Mitigations
  range from retrieval augmentation and prompt constraints to post-generation fact checking,
  human review, and model fine-tuning on verified corpora. Organizations must treat
  hallucination as both a quality and a risk management issue, particularly in regulated or
  safety-critical workflows.
audiences:
  exec: "Signals that the model is making things up, which erodes user trust and can trigger compliance issues."
  engineer: "Indicates the model sampled a high-probability sequence lacking factual grounding; investigate context, decoding, and eval signals."
examples:
  do:
    - "Log hallucination incidents and route high-severity cases to human review for remediation."
    - "Use retrieval augmentation or tool grounding to supply verifiable context before generation."
  dont:
    - "Deploy long-form responses without monitoring factual accuracy or adding disclaimers."
    - "Assume higher model size alone will eliminate hallucination without evaluation improvements."
governance:
  nist_rmf_tags:
    - "accuracy"
    - "transparency"
    - "validity"
  risk_notes: "Unaddressed hallucinations can produce misleading outputs that violate accuracy commitments and create legal exposure."
relationships:
  broader:
    - "generative AI"
  narrower:
    - "factual hallucination"
    - "formal hallucination"
  related:
    - "retrieval-augmented generation"
    - "guardrails"
    - "evaluation"
citations:
  - source: "Hugging Face Glossary"
    url: "https://huggingface.co/docs/transformers/en/glossary"
  - source: "NIST AI RMF Glossary"
    url: "https://airc.nist.gov/glossary/"
  - source: "Google ML Glossary"
    url: "https://developers.google.com/machine-learning/glossary"
license: "CC BY-SA 4.0"
status: "draft"
last_reviewed: "2024-09-18"
