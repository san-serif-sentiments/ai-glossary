term: "jailbreak prompt"
aliases:
  - "prompt jailbreak"
  - "guardrail bypass"
categories:
  - "Governance & Risk"
roles:
  - "security"
  - "product"
  - "engineering"
part_of_speech: "noun_phrase"
short_def: "Crafted input that persuades a model to ignore safety instructions and produce disallowed responses."
long_def: >-
  A jailbreak prompt is a deliberate attempt to bypass an AI system's guardrails. Attackers wrap malicious requests in
  roleplay, obfuscation, or multi-step instructions so the model forgets or ignores its policy. Jailbreaks often pair
  with prompt injection to extract secrets, generate harmful content, or trigger unauthorized tool calls. Defenders
  maintain libraries of known jailbreak patterns, run continuous red-teaming, and enforce layered mitigations (robust
  prompts, classifiers, tool restrictions). Product teams communicate refusals transparently, while governance partners
  document residual risks. Without jailbreak monitoring, safety regressions can go unnoticed until customers or
  regulators report incidents.
audiences:
  exec: "Treat jailbreaks as a measurable security risk with owners, metrics, and incident response plans."
  engineer: "Automate regression tests for known jailbreak patterns and update defenses when new variants appear."
examples:
  do:
    - "Maintain a catalog of blocked prompts and feed them into the evaluation harness."
    - "Pair jailbreak detection with human review for high-severity categories."
  dont:
    - "Assume a single policy prompt will withstand evolving jailbreak tactics."
    - "Silently filter outputs without notifying security or policy teams."
governance:
  nist_rmf_tags:
    - "security"
    - "monitoring"
    - "risk_management"
  risk_notes: "Unmitigated jailbreaks can surface banned content, expose data, or erode user trust."
relationships:
  broader:
    - "prompt injection"
  related:
    - "robust prompting"
    - "red teaming"
    - "guardrails"
citations:
  - source: "Anthropic – Prompt Jailbreaks"
    url: "https://www.anthropic.com/index/prompt-jailbreaks"
  - source: "OpenAI – Safety Best Practices"
    url: "https://help.openai.com/en/articles/7039943-safety-best-practices-for-openai-apis"
license: "CC BY-SA 4.0"
status: "approved"
last_reviewed: "2025-09-29"
