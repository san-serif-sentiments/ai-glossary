term: "evaluation"
aliases:
  - "model evaluation"
  - "AI evaluation"
categories:
  - "Operations & Monitoring"
  - "Governance & Risk"
roles:
  - "communications"
  - "engineering"
  - "legal"
  - "policy"
  - "product"
  - "security"
part_of_speech: "process"
short_def: "Systematic measurement of model performance, safety, and reliability using defined tests."
long_def: >-
  Evaluation is the disciplined practice of testing AI systems against quantitative and qualitative criteria before and after deployment. It extends beyond accuracy metrics to encompass robustness, bias detection, factual correctness, latency, and safety stress tests such as red teaming or jailbreak attempts. Teams build eval suites that blend automated metrics—like BLEU, accuracy@k, or toxicity scores—with human review checklists tailored to critical user journeys. Continuous evaluation supports regression detection when prompts, datasets, or infrastructure change. Governance frameworks treat evaluations as audit artifacts: they document assumptions, thresholds, and sign-offs required before promoting models to production. Mature programs integrate evaluation pipelines into CI/CD, enabling reproducibility and traceability. Without rigorous evaluation, organizations cannot credibly claim their models meet compliance obligations or user expectations.
audiences:
  exec: "Evaluation is our quality gate—it proves the AI delivers safe, reliable outcomes before we launch."
  engineer: "Automated and human-in-the-loop test harnesses measuring task metrics, robustness, bias, and safety across model releases."
examples:
  do:
    - "Run targeted red-team scenarios alongside quantitative metrics before shipping new prompts or fine-tuned models."
  dont:
    - "Rely on a single aggregate score without examining subgroup performance or qualitative feedback."
governance:
  nist_rmf_tags:
    - "validity"
    - "accountability"
  risk_notes: "Skipping or weakening evaluations increases the likelihood of undetected harmful behaviors in production."
relationships:
  broader:
    - "model governance"
  narrower:
    - "safety evaluation"
    - "capability evaluation"
  related:
    - "guardrails"
    - "alignment"
    - "red teaming"
citations:
  - source: "NIST AI RMF Glossary"
    url: "https://airc.nist.gov/glossary/"
  - source: "Google ML Glossary"
    url: "https://developers.google.com/machine-learning/glossary"
  - source: "Stanford HAI Brief Definitions"
    url: "https://hai.stanford.edu/news/brief-definitions"
license: "CC BY-SA 4.0"
status: "reviewed"
last_reviewed: "2025-09-28"
